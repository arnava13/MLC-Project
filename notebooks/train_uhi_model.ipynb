{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dee44cc",
   "metadata": {},
   "source": [
    "# Train UHI Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87449c2f",
   "metadata": {},
   "source": [
    "# Train UHI Prediction Model\n",
    "\n",
    "This notebook trains the `UHINet` model using Sentinel-2 mosaics, weather data, LST, and time embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97feb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import lightning as L\n",
    "from box import Box\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "workspace_dir = Path(\"/Users/arnav/MLC-Project\") # Adjust if necessary\n",
    "sys.path.append(str(workspace_dir))\n",
    "\n",
    "from src.model import UHINet\n",
    "# Assume dataloaders are defined in src/dataloader.py (or adjust import)\n",
    "# from src.dataloader import UHIDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e727ff",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35980f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Paths\n",
    "CLAY_CHECKPOINT_DIR = workspace_dir / \"models\" / \"clay_checkpoints\"\n",
    "CLAY_CHECKPOINT_NAME = \"clay-v1.5.ckpt\" # Name of the downloaded checkpoint\n",
    "CLAY_CHECKPOINT_PATH = CLAY_CHECKPOINT_DIR / CLAY_CHECKPOINT_NAME\n",
    "CLAY_METADATA_PATH = workspace_dir / \"src\" / \"models\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "DATA_DIR = workspace_dir / \"data\" # Adjust based on your data structure\n",
    "LOG_DIR = workspace_dir / \"lightning_logs\"\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "CLAY_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model Hyperparameters (Example values, adjust as needed)\n",
    "CLAY_MODEL_SIZE = \"large\"\n",
    "CLAY_BANDS = [\"blue\", \"green\", \"red\", \"nir\"] # Must match bands used in src/model.py\n",
    "CLAY_PLATFORM = \"sentinel-2-l2a\"\n",
    "CLAY_GSD = 10\n",
    "\n",
    "WEATHER_CHANNELS = 3 # Example: Tmax, Tmin, Precip\n",
    "LST_CHANNELS = 1\n",
    "USE_LST = True\n",
    "TIME_EMBED_DIM = 128 # Example\n",
    "\n",
    "GRU_HIDDEN_DIM = 64\n",
    "GRU_KERNEL_SIZE = 3\n",
    "PROJ_CH = 32 # From UHINet init default\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "# Training Hyperparameters (Example values)\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 50\n",
    "ACCELERATOR = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c44cc1",
   "metadata": {},
   "source": [
    "## Download Clay Checkpoint\n",
    "\n",
    "Download the pre-trained Clay checkpoint if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca486df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Clay checkpoint\n",
    "clay_url = \"https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\"\n",
    "\n",
    "if not CLAY_CHECKPOINT_PATH.is_file():\n",
    "    print(f\"Downloading Clay checkpoint to {CLAY_CHECKPOINT_PATH}...\")\n",
    "    # Use os.system for wget within the notebook environment\n",
    "    os.system(f\"wget -q {clay_url} -O {CLAY_CHECKPOINT_PATH}\")\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(f\"Clay checkpoint already exists at {CLAY_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06690edc",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the UHINet model\n",
    "model = UHINet(\n",
    "    clay_checkpoint_path=str(CLAY_CHECKPOINT_PATH),\n",
    "    clay_metadata_path=str(CLAY_METADATA_PATH),\n",
    "    weather_channels=WEATHER_CHANNELS,\n",
    "    time_embed_dim=TIME_EMBED_DIM,\n",
    "    # --- Args with defaults ---\n",
    "    proj_ch=PROJ_CH,\n",
    "    clay_model_size=CLAY_MODEL_SIZE,\n",
    "    clay_bands=CLAY_BANDS,\n",
    "    clay_platform=CLAY_PLATFORM,\n",
    "    clay_gsd=CLAY_GSD,\n",
    "    lst_channels=LST_CHANNELS,\n",
    "    use_lst=USE_LST,\n",
    "    gru_hidden_dim=GRU_HIDDEN_DIM,\n",
    "    gru_kernel_size=GRU_KERNEL_SIZE,\n",
    "    output_channels=OUTPUT_CHANNELS\n",
    ")\n",
    "\n",
    "print(\"UHINet model initialized.\")\n",
    "# print(model) # Optional: print model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e407c5",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Set up the dataloader. This section needs to be implemented based on your specific data structure and `UHIDataModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation (Placeholder) ---\n",
    "\n",
    "# TODO: Implement or import your UHIDataModule\n",
    "# data_module = UHIDataModule(\n",
    "#     data_dir=DATA_DIR,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     num_workers=os.cpu_count(),\n",
    "#     clay_bands=CLAY_BANDS, # Ensure datamodule provides correct bands\n",
    "#     use_lst=USE_LST,\n",
    "#     # ... other datamodule args ...\n",
    "# )\n",
    "\n",
    "# data_module.setup() # Prepare data, download etc.\n",
    "\n",
    "print(\"Data preparation placeholder. Needs implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bc5f0",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Configure the PyTorch Lightning Trainer and start training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e9e1a",
   "metadata": {},
   "source": [
    "# Train UHI Net Model\n",
    "\n",
    "This notebook trains the UHI prediction model (`UHINet`) using pre-processed data.\n",
    "\n",
    "**Steps:**\n",
    "1. Install necessary packages (if not already installed).\n",
    "2. Download the Clay foundation model checkpoint.\n",
    "3. Define configuration parameters (paths, hyperparameters).\n",
    "4. Load the dataset.\n",
    "5. Initialize the `UHINet` model, loss function, and optimizer.\n",
    "6. Run the training and validation loop.\n",
    "7. Save the best model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d70512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Add project root to path to import custom modules\n",
    "project_root = Path('.').resolve().parent # Assumes notebook is in 'notebooks' dir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.ingest.dataloader import CityDataSet\n",
    "# Import the *refactored* UHINet model\n",
    "from src.model import UHINet \n",
    "from src.train.loss import masked_mae_loss, masked_mse_loss\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7105ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Download Clay Checkpoint\n",
    "# Define where to save the checkpoint\n",
    "clay_checkpoint_dir = project_root / \"models\" / \"clay_checkpoints\"\n",
    "clay_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "clay_checkpoint_filename = \"clay-v1.5.ckpt\"\n",
    "clay_checkpoint_path_local = clay_checkpoint_dir / clay_checkpoint_filename\n",
    "clay_checkpoint_url = \"https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\"\n",
    "\n",
    "# Download only if it doesn't exist\n",
    "if not clay_checkpoint_path_local.exists():\n",
    "    print(f\"Downloading Clay checkpoint from {clay_checkpoint_url}...\")\n",
    "    # Use wget through the shell\n",
    "    import subprocess\n",
    "    try:\n",
    "        subprocess.run([\"wget\", \"-q\", \"-O\", str(clay_checkpoint_path_local), clay_checkpoint_url], check=True)\n",
    "        print(f\"Clay checkpoint saved to {clay_checkpoint_path_local}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'wget' command not found. Please install wget or download the file manually.\")\n",
    "        # Optionally provide manual download instructions\n",
    "        # print(f\"Manual download: {clay_checkpoint_url}\")\n",
    "        # clay_checkpoint_path_local = None # Indicate download failed\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        # clay_checkpoint_path_local = None # Indicate download failed\n",
    "else:\n",
    "    print(f\"Clay checkpoint already exists at {clay_checkpoint_path_local}\")\n",
    "\n",
    "# --- Define path to Clay metadata --- \n",
    "# Assuming it's within the cloned Clay repo structure inside src/models/Clay\n",
    "clay_metadata_path_local = project_root / \"src\" / \"models\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "if not clay_metadata_path_local.exists():\n",
    "    print(f\"Error: Clay metadata file not found at {clay_metadata_path_local}. Ensure the Clay submodule/repository structure is correct.\")\n",
    "    # clay_metadata_path_local = None # Indicate missing file\n",
    "else:\n",
    "     print(f\"Using Clay metadata from: {clay_metadata_path_local}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Configuration / Hyperparameters\n",
    "\n",
    "# --- Paths ---\n",
    "data_dir = project_root / \"data\" # Base directory for processed city data\n",
    "city_name = \"your_city_name\" # <<< CHANGE THIS to the city you processed\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Bounds: Either specify manually [lon_min, lat_min, lon_max, lat_max] or leave as None to load from bbox.csv\n",
    "bounds = None \n",
    "resolution_m = 30 # Spatial resolution used during data processing\n",
    "include_lst = True # Set to True if you processed LST and want to include it\n",
    "# Path to the *single* LST median file (created by create_sat_tensor_files)\n",
    "# Required if include_lst=True. Example: data/your_city_name/lst_median_2021-01-01_2021-12-31.tif\n",
    "single_lst_median_path = data_dir / city_name / \"lst_median_YYYY-MM-DD_YYYY-MM-DD.tif\" # <<< CHANGE THIS if using LST\n",
    "\n",
    "# --- Model Config ---\n",
    "# Clay Feature Extractor Args (Must match downloaded checkpoint and data)\n",
    "clay_model_size = \"large\" # Should match the checkpoint, e.g., v1.5 is large\n",
    "# Bands MUST match the order/names used when creating the cloudless mosaic!\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"] \n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10 # GSD of the input Sentinel-2 mosaic\n",
    "\n",
    "# UHINet Args (ConvGRU part)\n",
    "weather_channels = 3 # Assuming 3 weather variables (e.g., tmin, tmax, precip)\n",
    "time_embed_dim = 4 # Assuming 4D time embedding (sin/cos week, sin/cos hour)\n",
    "lst_channels = 1 if include_lst else 0 # Number of channels in the LST input\n",
    "gru_hidden_dim = 64\n",
    "gru_kernel_size = 3\n",
    "output_channels = 1 # Predicting single UHI value\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mae' # 'mae' or 'mse'\n",
    "patience = 10 # Early stopping patience\n",
    "cpu = False # Set to True to force CPU use\n",
    "\n",
    "# --- Derived Paths & Sanity Checks ---\n",
    "data_dir_path = Path(data_dir)\n",
    "city_data_dir = data_dir_path / city_name\n",
    "uhi_csv = city_data_dir / \"uhi_data.csv\"\n",
    "bbox_csv = city_data_dir / \"bbox.csv\"\n",
    "weather_csv = city_data_dir / \"weather_grid.csv\"\n",
    "cloudless_mosaic_path = city_data_dir / \"cloudless_mosaic.tif\"\n",
    "\n",
    "if include_lst and (not single_lst_median_path or not Path(single_lst_median_path).exists()):\n",
    "    raise FileNotFoundError(f\"Error: LST is included (include_lst=True) but single_lst_median_path '{single_lst_median_path}' is invalid or not found.\")\n",
    "\n",
    "# Check essential data files exist\n",
    "required_files = [uhi_csv, bbox_csv, weather_csv, cloudless_mosaic_path]\n",
    "for f in required_files:\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Required data file not found: {f}\")\n",
    "\n",
    "# Check Clay files exist (paths defined in previous cell)\n",
    "if not clay_checkpoint_path_local or not clay_checkpoint_path_local.exists():\n",
    "     raise FileNotFoundError(\"Clay checkpoint path is not valid. Check download step.\")\n",
    "if not clay_metadata_path_local or not clay_metadata_path_local.exists():\n",
    "     raise FileNotFoundError(\"Clay metadata path is not valid. Check path definition.\")\n",
    "\n",
    "# Load bounds from CSV if not specified\n",
    "if not bounds:\n",
    "    print(\"Bounds not provided, loading from bbox.csv\")\n",
    "    try:\n",
    "        bbox_df = pd.read_csv(bbox_csv)\n",
    "        bounds = [\n",
    "            bbox_df['longitudes'].min(), bbox_df['latitudes'].min(),\n",
    "            bbox_df['longitudes'].max(), bbox_df['latitudes'].max()\n",
    "        ]\n",
    "        print(f\"Loaded bounds from {bbox_csv}: {bounds}\")\n",
    "    except Exception as e:\n",
    "            raise ValueError(f\"Failed to load bounds from {bbox_csv}: {e}. Provide manually in config.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize Dataset and Dataloaders\n",
    "\n",
    "print(\"Initializing dataset...\")\n",
    "try:\n",
    "    dataset = CityDataSet(\n",
    "        bounds=bounds,\n",
    "        resolution_m=resolution_m,\n",
    "        uhi_csv=str(uhi_csv),\n",
    "        bbox_csv=str(bbox_csv),\n",
    "        weather_csv=str(weather_csv),\n",
    "        cloudless_mosaic_path=str(cloudless_mosaic_path),\n",
    "        data_dir=str(data_dir_path),\n",
    "        city_name=city_name,\n",
    "        include_lst=include_lst,\n",
    "        single_lst_median_path=str(single_lst_median_path) if include_lst else None\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    # Stop execution or handle error\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split ---\n",
    "val_percent = 0.15\n",
    "n_samples = len(dataset)\n",
    "if n_samples < 10: # Handle very small datasets\n",
    "    print(f\"Warning: Dataset size ({n_samples}) is very small. Validation split disabled.\")\n",
    "    n_val = 0\n",
    "    n_train = n_samples\n",
    "    train_ds = dataset\n",
    "    val_ds = None # No validation set\n",
    "else:\n",
    "    n_val = int(n_samples * val_percent)\n",
    "    n_train = n_samples - n_val\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f\"Dataset split: {n_train} training, {n_val or 0} validation samples.\")\n",
    "\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True) if val_ds else None\n",
    "\n",
    "print(\"Data loading setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize Model\n",
    "print(\"Initializing UHINet model...\")\n",
    "\n",
    "# Ensure checkpoint path is correct\n",
    "if not clay_checkpoint_path_local or not Path(clay_checkpoint_path_local).exists():\n",
    "    raise FileNotFoundError(f\"Clay checkpoint not found at expected path: {clay_checkpoint_path_local}\")\n",
    "if not clay_metadata_path_local or not Path(clay_metadata_path_local).exists():\n",
    "    raise FileNotFoundError(f\"Clay metadata not found at expected path: {clay_metadata_path_local}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    model = UHINet(\n",
    "        # Clay args\n",
    "        clay_checkpoint_path=str(clay_checkpoint_path_local),\n",
    "        clay_metadata_path=str(clay_metadata_path_local),\n",
    "        clay_model_size=clay_model_size,\n",
    "        clay_bands=clay_bands,\n",
    "        clay_platform=clay_platform,\n",
    "        clay_gsd=clay_gsd,\n",
    "        # Weather args\n",
    "        weather_channels=weather_channels,\n",
    "        # LST args\n",
    "        lst_channels=lst_channels,\n",
    "        use_lst=include_lst,\n",
    "        # Time embedding args\n",
    "        time_embed_dim=time_embed_dim,\n",
    "        # ConvGRU args\n",
    "        gru_hidden_dim=gru_hidden_dim,\n",
    "        gru_kernel_size=gru_kernel_size,\n",
    "        # Output args\n",
    "        output_channels=output_channels\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model initialized: {model.__class__.__name__}\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "        print(f\"Failed to initialize model: {e}\")\n",
    "        raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error initializing model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05698380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize Loss and Optimizer\n",
    "\n",
    "loss_fn = masked_mae_loss if loss_type == 'mae' else masked_mse_loss\n",
    "print(f\"Using loss function: {loss_type}\")\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "print(f\"Using optimizer: AdamW (lr={lr}, wd={weight_decay})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper Functions (Checkpointing, Train/Val Loops)\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\n",
    "    Args:\n",
    "        state (dict): Contains model's state_dict, optimizer state, epoch, etc.\n",
    "        is_best (bool): True if this is the best model seen so far.\n",
    "        filename (str): Path to save the latest checkpoint.\n",
    "        best_filename (str): Path to save the best checkpoint.\n",
    "    \"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    # Use tqdm.notebook for better integration\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        # Ensure all required keys are present\n",
    "        required_keys = ['sentinel_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "        if include_lst:\n",
    "            required_keys.append('lst_seq')\n",
    "        if not all(key in batch for key in required_keys):\n",
    "             missing = [key for key in required_keys if key not in batch]\n",
    "             print(f\"Warning: Skipping batch due to missing keys: {missing}\")\n",
    "             continue\n",
    "        \n",
    "        # Move batch to device\n",
    "        try:\n",
    "            batch_device = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving batch to device: {e}\")\n",
    "            continue # Skip batch if moving fails\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            predictions = model(batch_device)\n",
    "            loss = loss_fn(predictions, batch_device['target'], batch_device['mask'])\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                 print(\"Warning: NaN loss detected, skipping batch.\")\n",
    "                 continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "             # Catch CUDA OOM or other runtime errors\n",
    "             print(f\"Runtime error during training: {e}\")\n",
    "             if \"out of memory\" in str(e):\n",
    "                 print(\"CUDA out of memory. Try reducing batch size.\")\n",
    "             # Potentially skip batch or stop training depending on error\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             print(f\"Unexpected error during training step: {e}\")\n",
    "             continue # Skip this batch\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Ensure all required keys are present\n",
    "            required_keys = ['sentinel_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "            if include_lst:\n",
    "                required_keys.append('lst_seq')\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                missing = [key for key in required_keys if key not in batch]\n",
    "                print(f\"Warning: Skipping validation batch due to missing keys: {missing}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                batch_device = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "                predictions = model(batch_device)\n",
    "                loss = loss_fn(predictions, batch_device['target'], batch_device['mask'])\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Warning: NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                 print(f\"Error during validation step: {e}\")\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Training Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Create output directory for this run\n",
    "run_name = f\"{city_name}_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# Save configuration used for this run\n",
    "try:\n",
    "    config_dict = {\n",
    "        \"data_dir\": str(data_dir),\n",
    "        \"city_name\": city_name,\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"bounds\": bounds,\n",
    "        \"resolution_m\": resolution_m,\n",
    "        \"include_lst\": include_lst,\n",
    "        \"single_lst_median_path\": str(single_lst_median_path) if include_lst else None,\n",
    "        \"clay_model_size\": clay_model_size,\n",
    "        \"clay_bands\": clay_bands,\n",
    "        \"clay_platform\": clay_platform,\n",
    "        \"clay_gsd\": clay_gsd,\n",
    "        \"clay_checkpoint_path_local\": str(clay_checkpoint_path_local),\n",
    "        \"clay_metadata_path_local\": str(clay_metadata_path_local),\n",
    "        \"weather_channels\": weather_channels,\n",
    "        \"time_embed_dim\": time_embed_dim,\n",
    "        \"lst_channels\": lst_channels,\n",
    "        \"gru_hidden_dim\": gru_hidden_dim,\n",
    "        \"gru_kernel_size\": gru_kernel_size,\n",
    "        \"output_channels\": output_channels,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"loss_type\": loss_type,\n",
    "        \"patience\": patience,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    print(\"Saved configuration to config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save configuration: {e}\")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss = validate_epoch(model, val_loader, loss_fn, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        current_loss = val_loss\n",
    "        if np.isnan(current_loss):\n",
    "             print(\"Warning: Validation loss is NaN. Stopping training.\")\n",
    "             break\n",
    "    else:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} (No validation set)\")\n",
    "            current_loss = train_loss # Use train loss for checkpointing if no val set\n",
    "            if np.isnan(current_loss):\n",
    "                 print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                 break\n",
    "\n",
    "    is_best = current_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = current_loss\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"New best loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_loss': best_val_loss,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config_dict # Save config with checkpoint\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "\n",
    "    # Early stopping check\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best loss recorded: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved in: {output_dir}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
