{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87449c2f",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# --- Metrics --- \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.model import UHINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97feb2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.model import UHINet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e727ff",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35980f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.ingest.dataloader import CityDataSet\n",
    "from src.model import UHINet \n",
    "from src.train.loss import masked_mae_loss, masked_mse_loss\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca486df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bounds from /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters\n",
    "\n",
    "# --- Paths ---\n",
    "data_dir = project_root / \"data\" # Base directory for processed city data\n",
    "city_name = \"NYC\" \n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- Data Loading ---\n",
    "resolution_m = 10 # Spatial resolution used for the mosaic/grids (10m for Sentinel)\n",
    "include_lst = False # Set to True if you processed LST and want to include it\n",
    "\n",
    "# Path to the LST median file (created by download_data notebook)\n",
    "single_lst_median_path = data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "# Path to cloudless mosaic (created by download_data notebook)\n",
    "cloudless_mosaic_path = data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "\n",
    "# --- Model Config ---\n",
    "clay_model_size = \"large\" # Should match the checkpoint, e.g., v1.5 is large\n",
    "# Bands MUST match the order used when creating the cloudless mosaic!\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"] # [\"B02\", \"B03\", \"B04\", \"B08\"] from Sentinel data\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10 # GSD of the input Sentinel-2 mosaic\n",
    "\n",
    "# UHINet Args (ConvGRU part)\n",
    "weather_channels = 6 # Updated: air_temp, rel_humidity, avg_windspeed, wind_direction_cos, wind_direction_sin solar_flux\n",
    "time_embed_dim = 2 # sin/cos minute_of_day\n",
    "lst_channels = 1 if include_lst else 0 # Number of channels in the LST input\n",
    "proj_ch = 15 # Channels after projecting Clay features\n",
    "gru_hidden_dim = 64\n",
    "gru_kernel_size = 3\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "batch_size = 1 # Small dataset,\n",
    "num_workers = 4\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse' # 'mae' or 'mse'\n",
    "patience = 10 # Early stopping patience (will be removed from loop)\n",
    "cpu = False \n",
    "\n",
    "# --- Derived Paths & Sanity Checks ---\n",
    "data_dir_path = Path(data_dir)\n",
    "city_data_dir = data_dir_path / city_name\n",
    "uhi_csv = city_data_dir / \"uhi.csv\"\n",
    "uhi_df = pd.read_csv(uhi_csv)\n",
    "# Paths to station weather data\n",
    "bronx_weather_csv = city_data_dir / \"bronx_weather.csv\"\n",
    "manhattan_weather_csv = city_data_dir / \"mahattan_weather.csv\"\n",
    "\n",
    "# ---- Grid Bound ----\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {uhi_csv}: {bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd2dde-04dc-44fa-9c48-2840e2ec1e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2abac67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 00:43:59,146 - INFO - Loading cloudless mosaic from /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "2025-04-28 00:43:59,159 - INFO - Loaded mosaic with 4 bands and shape (1122, 1281)\n",
      "/home/jupyter/UHI/MLC-Project/src/ingest/dataloader.py:143: FutureWarning: Parsed string \"2021-07-24 06:00:00 EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  dt_naive_or_aware = pd.to_datetime(self.bronx_weather['datetime'], errors='raise')\n",
      "/home/jupyter/UHI/MLC-Project/src/ingest/dataloader.py:169: FutureWarning: Parsed string \"2021-07-24 06:00:00 EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  dt_naive_or_aware = pd.to_datetime(self.manhattan_weather['datetime'], errors='raise')\n",
      "2025-04-28 00:43:59,185 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-04-28 00:43:59,186 - INFO - Loaded Manhattan weather data: 169 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 00:44:01,072 - INFO - Computed grid cell coordinates and closest station map\n",
      "2025-04-28 00:44:01,075 - INFO - Grid cells assigned to Bronx: 370359\n",
      "2025-04-28 00:44:01,077 - INFO - Grid cells assigned to Manhattan: 712983\n",
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 333.49it/s]\n",
      "2025-04-28 00:44:01,264 - INFO - Dataset initialized for NYC with 59 unique timestamps. LST included: False\n",
      "2025-04-28 00:44:01,265 - INFO - Target grid size (H, W): (1118, 969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 36 training, 23 validation samples.\n",
      "Creating dataloaders...\n",
      "Data loading setup complete.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "## IGNORE TIMEZONE WARNING: Timezone is first incorrectly loaded by pandas but then fixed in our dataloader.\n",
    "\n",
    "print(\"Initializing dataset...\")\n",
    "try:\n",
    "    # Note: averaging_window is needed by constructor but might not be used internally if single_lst_median_path is set\n",
    "    # Use a placeholder value if needed, or ensure the dataloader handles its optional usage.\n",
    "    placeholder_avg_window = 30 # Example placeholder\n",
    "\n",
    "    dataset = CityDataSet(\n",
    "        bounds=bounds,\n",
    "        averaging_window=placeholder_avg_window, # Pass placeholder\n",
    "        resolution_m=resolution_m,\n",
    "        uhi_csv=str(uhi_csv),\n",
    "        # Use station weather CSVs\n",
    "        bronx_weather_csv=str(bronx_weather_csv),\n",
    "        manhattan_weather_csv=str(manhattan_weather_csv),\n",
    "        cloudless_mosaic_path=str(cloudless_mosaic_path),\n",
    "        data_dir=str(data_dir_path),\n",
    "        city_name=city_name,\n",
    "        include_lst=include_lst,\n",
    "        single_lst_median_path=str(single_lst_median_path) if include_lst else None\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    # Stop execution or handle error\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split (Sequential) ---\n",
    "val_percent = 0.40\n",
    "n_samples = len(dataset)\n",
    "\n",
    "if n_samples < 10: # Handle very small datasets\n",
    "    print(f\"Warning: Dataset size ({n_samples}) is very small. Using all data for training.\")\n",
    "    n_val = 0\n",
    "    n_train = n_samples\n",
    "else:\n",
    "    n_val = int(n_samples * val_percent)\n",
    "    n_train = n_samples - n_val\n",
    "\n",
    "# Create sequential split using Subset\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, n_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices) if n_val > 0 else None # Create val_ds only if n_val > 0\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training, {len(val_ds) if val_ds else 0} validation samples.\")\n",
    "\n",
    "print(\"Creating dataloaders...\")\n",
    "# Shuffle training data loader, but not validation\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True) if val_ds else None\n",
    "\n",
    "print(\"Data loading setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06690edc",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2a7dfe-0136-4eed-9e31-5a53a6563c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\n",
    "    Args:\n",
    "        state (dict): Contains model's state_dict, optimizer state, epoch, etc.\n",
    "        is_best (bool): True if this is the best model seen so far.\n",
    "        filename (str): Path to save the latest checkpoint.\n",
    "        best_filename (str): Path to save the best checkpoint.\n",
    "    \"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_preds = []\n",
    "    epoch_targets = []\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "        required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "        if include_lst:\n",
    "            required_keys.append('lst_seq')\n",
    "        if not all(key in batch for key in required_keys):\n",
    "            missing = [key for key in required_keys if key not in batch]\n",
    "            logging.warning(f\"Skipping batch due to missing keys: {missing}\")\n",
    "            continue\n",
    "\n",
    "        # Move batch to device\n",
    "        try:\n",
    "            cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "            weather_seq = batch[\"weather_seq\"].to(device)       # (B, T, C_weather, H, W)\n",
    "            lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None # (B, T, C_lst, H, W) - T=1\n",
    "            time_emb_seq = batch[\"time_emb_seq\"].to(device)     # (B, T, C_time, H, W)\n",
    "            target = batch[\"target\"].to(device)               # (B, H, W)\n",
    "            mask = batch[\"mask\"].to(device, dtype=torch.bool) # Ensure mask is boolean\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error moving batch to device: {e}\")\n",
    "            continue # Skip batch if moving fails\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "            _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "            # 1. Encode static features ONCE\n",
    "            static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None # Get T=0 slice\n",
    "            with torch.no_grad(): # Ensure Clay backbone remains frozen\n",
    "                 static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "            _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "            # 2. Initialize hidden state\n",
    "            h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "            # 3. Resize dynamic features if needed\n",
    "            if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "            else:\n",
    "                weather_seq_resized = weather_seq\n",
    "            if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "            else:\n",
    "                time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "            # 4. Loop through time steps\n",
    "            for t in range(T):\n",
    "                weather_t = weather_seq_resized[:, t, :, :, :]      # (B, C_weather, H', W')\n",
    "                time_emb_t = time_emb_seq_resized[:, t, :, :, :]    # (B, C_time, H', W')\n",
    "                x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                h = model.step(x_t_combined, h)\n",
    "\n",
    "            # 5. Predict from final hidden state\n",
    "            prediction = model.predict(h) # (B, 1, H', W')\n",
    "\n",
    "            # Resize prediction to target size if needed\n",
    "            if prediction.shape[2:] != target.shape[1:]:\n",
    "                 prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                 prediction_resized = prediction\n",
    "            prediction_final = prediction_resized.squeeze(1) # Shape (B, H, W)\n",
    "\n",
    "            # --- Calculate Loss --- \n",
    "            loss = loss_fn(prediction_final, target, mask) # Use boolean mask directly if loss supports it\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                 logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                 continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Store valid predictions/targets for metrics ---\n",
    "            with torch.no_grad():\n",
    "                valid_preds = prediction_final[mask].cpu().numpy()\n",
    "                valid_targets = target[mask].cpu().numpy()\n",
    "                if valid_preds.size > 0: # Only append if there are valid points in the batch\n",
    "                    epoch_preds.append(valid_preds)\n",
    "                    epoch_targets.append(valid_targets)\n",
    "            # --------------------------------------------------\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"Runtime error during training: {e}\")\n",
    "             if \"out of memory\" in str(e):\n",
    "                 logging.error(\"CUDA out of memory. Try reducing batch size.\")\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "             continue # Skip this batch\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    \n",
    "    # --- Calculate Epoch Metrics ---\n",
    "    rmse = np.nan\n",
    "    r2 = np.nan\n",
    "    if epoch_preds:\n",
    "        all_preds = np.concatenate(epoch_preds)\n",
    "        all_targets = np.concatenate(epoch_targets)\n",
    "        if all_preds.size > 0 and all_targets.size > 0:\n",
    "             try:\n",
    "                 rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "                 r2 = r2_score(all_targets, all_preds)\n",
    "             except Exception as metric_e:\n",
    "                  logging.error(f\"Error calculating epoch metrics: {metric_e}\")\n",
    "    # -------------------------------\n",
    "    \n",
    "    return avg_loss, rmse, r2\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_preds = []\n",
    "    epoch_targets = []\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Ensure all required keys are present\n",
    "            required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "            if include_lst:\n",
    "                required_keys.append('lst_seq')\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                missing = [key for key in required_keys if key not in batch]\n",
    "                logging.warning(f\"Skipping validation batch due to missing keys: {missing}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "                weather_seq = batch[\"weather_seq\"].to(device)\n",
    "                lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None\n",
    "                time_emb_seq = batch[\"time_emb_seq\"].to(device)\n",
    "                target = batch[\"target\"].to(device)\n",
    "                mask = batch[\"mask\"].to(device, dtype=torch.bool) # Ensure mask is boolean\n",
    "\n",
    "                # --- Model Forward Pass --- \n",
    "                B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "                _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "                static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None\n",
    "                static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "                _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "                h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "                if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "                else:\n",
    "                    weather_seq_resized = weather_seq\n",
    "                if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "                else:\n",
    "                    time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "                for t in range(T):\n",
    "                    weather_t = weather_seq_resized[:, t, :, :, :]\n",
    "                    time_emb_t = time_emb_seq_resized[:, t, :, :, :]\n",
    "                    x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                    h = model.step(x_t_combined, h)\n",
    "\n",
    "                prediction = model.predict(h)\n",
    "\n",
    "                # --- Resize and Process Prediction --- \n",
    "                if prediction.shape[2:] != target.shape[1:]:\n",
    "                    prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "                else:\n",
    "                    prediction_resized = prediction\n",
    "                prediction_final = prediction_resized.squeeze(1) # Shape (B, H, W)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(prediction_final, target, mask)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "                \n",
    "                # --- Store valid predictions/targets for metrics ---\n",
    "                valid_preds = prediction_final[mask].cpu().numpy()\n",
    "                valid_targets = target[mask].cpu().numpy()\n",
    "                if valid_preds.size > 0:\n",
    "                    epoch_preds.append(valid_preds)\n",
    "                    epoch_targets.append(valid_targets)\n",
    "                # --------------------------------------------------\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    \n",
    "    # --- Calculate Epoch Metrics ---\n",
    "    rmse = np.nan\n",
    "    r2 = np.nan\n",
    "    if epoch_preds:\n",
    "        all_preds = np.concatenate(epoch_preds)\n",
    "        all_targets = np.concatenate(epoch_targets)\n",
    "        if all_preds.size > 0 and all_targets.size > 0:\n",
    "            try:\n",
    "                rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "                r2 = r2_score(all_targets, all_preds)\n",
    "            except Exception as metric_e:\n",
    "                 logging.error(f\"Error calculating validation epoch metrics: {metric_e}\")\n",
    "    # -------------------------------\n",
    "    \n",
    "    return avg_loss, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029ea59-819d-4999-8d71-7a23ed2afc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc639c88-302c-4d12-a717-89683df94156",
   "metadata": {},
   "source": [
    "## Helper Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1b3749-8013-4b55-9478-722d87783b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\n",
    "    Args:\n",
    "        state (dict): Contains model's state_dict, optimizer state, epoch, etc.\n",
    "        is_best (bool): True if this is the best model seen so far.\n",
    "        filename (str): Path to save the latest checkpoint.\n",
    "        best_filename (str): Path to save the best checkpoint.\n",
    "    \"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "        required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "        if include_lst:\n",
    "            required_keys.append('lst_seq')\n",
    "        if not all(key in batch for key in required_keys):\n",
    "            missing = [key for key in required_keys if key not in batch]\n",
    "            logging.warning(f\"Skipping batch due to missing keys: {missing}\")\n",
    "            continue\n",
    "\n",
    "        # Move batch to device\n",
    "        try:\n",
    "            cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "            weather_seq = batch[\"weather_seq\"].to(device)       # (B, T, C_weather, H, W)\n",
    "            lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None # (B, T, C_lst, H, W) - T=1\n",
    "            time_emb_seq = batch[\"time_emb_seq\"].to(device)     # (B, T, C_time, H, W)\n",
    "            target = batch[\"target\"].to(device)               # (B, H, W)\n",
    "            mask = batch[\"mask\"].to(device)                   # (B, H, W)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error moving batch to device: {e}\")\n",
    "            continue # Skip batch if moving fails\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "            _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "            # 1. Encode static features ONCE\n",
    "            static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None # Get T=0 slice\n",
    "            # static_features shape: (B, proj_ch [+ C_lst], H', W')\n",
    "            with torch.no_grad(): # Ensure Clay backbone remains frozen\n",
    "                 # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                 static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "            _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "            # 2. Initialize hidden state\n",
    "            h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "            # 3. Resize dynamic features if needed\n",
    "            if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "            else:\n",
    "                weather_seq_resized = weather_seq\n",
    "            if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "            else:\n",
    "                time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "            # 4. Loop through time steps\n",
    "            for t in range(T):\n",
    "                weather_t = weather_seq_resized[:, t, :, :, :]      # (B, C_weather, H', W')\n",
    "                time_emb_t = time_emb_seq_resized[:, t, :, :, :]    # (B, C_time, H', W')\n",
    "                # Concatenate static + dynamic features\n",
    "                x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                # Update hidden state\n",
    "                h = model.step(x_t_combined, h)\n",
    "\n",
    "            # 5. Predict from final hidden state\n",
    "            prediction = model.predict(h) # (B, 1, H', W')\n",
    "            # --------------------------\n",
    "\n",
    "            # Resize prediction to target size if needed\n",
    "            if prediction.shape[2:] != target.shape[1:]:\n",
    "                 prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                 prediction_resized = prediction\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(prediction_resized.squeeze(1), target, mask)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                 logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                 continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"Runtime error during training: {e}\")\n",
    "             if \"out of memory\" in str(e):\n",
    "                 logging.error(\"CUDA out of memory. Try reducing batch size.\")\n",
    "                 # Consider breaking the loop or stopping training\n",
    "                 # break # Or raise e\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "             continue # Skip this batch\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Ensure all required keys are present\n",
    "            # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "            required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "            if include_lst:\n",
    "                required_keys.append('lst_seq')\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                missing = [key for key in required_keys if key not in batch]\n",
    "                logging.warning(f\"Skipping validation batch due to missing keys: {missing}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "                weather_seq = batch[\"weather_seq\"].to(device)\n",
    "                lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None\n",
    "                time_emb_seq = batch[\"time_emb_seq\"].to(device)\n",
    "                target = batch[\"target\"].to(device)\n",
    "                mask = batch[\"mask\"].to(device)\n",
    "\n",
    "                # --- New Validation Logic ---\n",
    "                B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "                _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "                # 1. Encode static features ONCE\n",
    "                static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None\n",
    "                # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "                _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "                # 2. Initialize hidden state\n",
    "                h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "                # 3. Resize dynamic features if needed\n",
    "                if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "                else:\n",
    "                    weather_seq_resized = weather_seq\n",
    "                if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "                else:\n",
    "                    time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "                # 4. Loop through time steps\n",
    "                for t in range(T):\n",
    "                    weather_t = weather_seq_resized[:, t, :, :, :]\n",
    "                    time_emb_t = time_emb_seq_resized[:, t, :, :, :]\n",
    "                    x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                    h = model.step(x_t_combined, h)\n",
    "\n",
    "                # 5. Predict from final hidden state\n",
    "                prediction = model.predict(h)\n",
    "                # --------------------------\n",
    "\n",
    "                # Resize prediction to target size if needed\n",
    "                if prediction.shape[2:] != target.shape[1:]:\n",
    "                    prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "                else:\n",
    "                    prediction_resized = prediction\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(prediction_resized.squeeze(1), target, mask)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d733113-cb81-4972-bdff-5e1cd1a4d691",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab087292-f6cb-4c31-8db0-6ae9bb6223f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints and logs will be saved to: /home/jupyter/UHI/MLC-Project/training_runs/NYC_run_20250428_003814\n",
      "Saved configuration to config.json\n",
      "Starting training...\n",
      "--- Epoch 1/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.5585, Val Loss=0.2308\n",
      "New best loss: 0.2308\n",
      "Saved new best model to /home/jupyter/UHI/MLC-Project/training_runs/NYC_run_20250428_003814/model_best.pth.tar\n",
      "--- Epoch 2/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0951, Val Loss=0.0117\n",
      "New best loss: 0.0117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo improvement in validation loss for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs_no_improve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_val_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Save config with checkpoint\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m     \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_last.pth.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_best.pth.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Early stopping check\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epochs_no_improve \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m patience:\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best, filename, best_filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves model checkpoint.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    state (dict): Contains model's state_dict, optimizer state, epoch, etc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    best_filename (str): Path to save the best checkpoint.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m Path(filename)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Ensure dir exists\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_best:\n\u001b[1;32m     12\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfile(filename, best_filename)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:964\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    961\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    965\u001b[0m         _save(\n\u001b[1;32m    966\u001b[0m             obj,\n\u001b[1;32m    967\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:798\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr,weight_decay=0.01)\n",
    "loss_fn= masked_mae_loss if loss_type==\"mae\" else masked_mse_loss\n",
    "\n",
    "# Create output directory for this run\n",
    "run_name = f\"{city_name}_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# Save configuration used for this run\n",
    "try:\n",
    "    config_dict = {\n",
    "        \"data_dir\": str(data_dir),\n",
    "        \"city_name\": city_name,\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"bounds\": bounds,\n",
    "        \"resolution_m\": resolution_m,\n",
    "        \"include_lst\": include_lst,\n",
    "        \"single_lst_median_path\": str(single_lst_median_path) if include_lst else None,\n",
    "        \"uhi_csv\": str(uhi_csv),\n",
    "        \"bronx_weather_csv\": str(bronx_weather_csv),\n",
    "        \"manhattan_weather_csv\": str(manhattan_weather_csv),\n",
    "        \"cloudless_mosaic_path\": str(cloudless_mosaic_path),\n",
    "        \"clay_model_size\": clay_model_size,\n",
    "        \"clay_bands\": clay_bands,\n",
    "        \"clay_platform\": clay_platform,\n",
    "        \"clay_gsd\": clay_gsd,\n",
    "        \"clay_checkpoint_path_local\": clay_checkpoint_path,\n",
    "        \"clay_metadata_path_local\": clay_metadata_path,\n",
    "        \"weather_channels\": weather_channels,\n",
    "        \"time_embed_dim\": time_embed_dim,\n",
    "        \"lst_channels\": lst_channels,\n",
    "        \"proj_ch\": proj_ch,\n",
    "        \"gru_hidden_dim\": gru_hidden_dim,\n",
    "        \"gru_kernel_size\": gru_kernel_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"loss_type\": loss_type,\n",
    "        \"patience\": patience,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    print(\"Saved configuration to config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save configuration: {e}\")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss = validate_epoch(model, val_loader, loss_fn, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        current_loss = val_loss\n",
    "        if np.isnan(current_loss):\n",
    "             print(\"Warning: Validation loss is NaN. Stopping training.\")\n",
    "             break\n",
    "    else:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} (No validation set)\")\n",
    "            current_loss = train_loss # Use train loss for checkpointing if no val set\n",
    "            if np.isnan(current_loss):\n",
    "                 print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                 break\n",
    "\n",
    "    is_best = current_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = current_loss\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"New best loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_loss': best_val_loss,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config_dict # Save config with checkpoint\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "\n",
    "    # Early stopping check\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best loss recorded: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9398d6-77ad-48c9-8ea0-cea7c00d691e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
