{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97feb2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Setup & Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset # MODIFIED: Added Subset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import shutil # For checkpoint saving\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(os.getcwd()).parent # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- Import Model Components ---\n",
    "from src.model import UHINetCNN # Import the CNN model\n",
    "from src.ingest.dataloader_cnn import CityDataSet # MODIFIED: Import the updated dataloader\n",
    "\n",
    "# --- Import Training Utilities & Loss --- ### MODIFIED ###\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the new utility module\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optionally import wandb if needed\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Import necessary metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c52a0-0d4b-4e3e-a2fc-363a45e9abc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### UNCOMMENT ON FIRST RUN IF USING Clay\n",
    "#!wget -q https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbab226-d51a-4a54-a97a-24665080db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded bounds from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "\n",
      "Run directory: /home/jupyter/MLC-Project/training_runs/UHINetCNN_NYC_20250507_061047\n",
      "UHINetCNN Configuration dictionary created:\n",
      "{\n",
      "  \"model_type\": \"UHINetCNN\",\n",
      "  \"project_root\": \"/home/jupyter/MLC-Project\",\n",
      "  \"city_name\": \"NYC\",\n",
      "  \"wandb_project_name\": \"MLC_UHI_Proj\",\n",
      "  \"wander_run_name_prefix\": \"NYC_UHINetCNN\",\n",
      "  \"feature_resolution_m\": 30,\n",
      "  \"uhi_grid_resolution_m\": 20,\n",
      "  \"enabled_weather_features\": [\n",
      "    \"rel_humidity\",\n",
      "    \"avg_windspeed\",\n",
      "    \"wind_direction\",\n",
      "    \"solar_flux\",\n",
      "    \"air_temp\"\n",
      "  ],\n",
      "  \"uhi_csv\": \"/home/jupyter/MLC-Project/data/NYC/uhi.csv\",\n",
      "  \"bronx_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/bronx_weather.csv\",\n",
      "  \"manhattan_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/manhattan_weather.csv\",\n",
      "  \"bounds\": [\n",
      "    -73.99445667,\n",
      "    40.75879167,\n",
      "    -73.87945833,\n",
      "    40.85949667\n",
      "  ],\n",
      "  \"feature_flags\": {\n",
      "    \"use_dem\": false,\n",
      "    \"use_dsm\": false,\n",
      "    \"use_clay\": true,\n",
      "    \"use_sentinel_composite\": false,\n",
      "    \"use_lst\": false,\n",
      "    \"use_ndvi\": false,\n",
      "    \"use_ndbi\": false,\n",
      "    \"use_ndwi\": false\n",
      "  },\n",
      "  \"sentinel_bands_to_load\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\",\n",
      "    \"swir16\",\n",
      "    \"swir22\"\n",
      "  ],\n",
      "  \"dem_path\": null,\n",
      "  \"dsm_path\": null,\n",
      "  \"elevation_nodata\": NaN,\n",
      "  \"cloudless_mosaic_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\",\n",
      "  \"single_lst_median_path\": null,\n",
      "  \"lst_nodata\": NaN,\n",
      "  \"unet_base_channels\": 64,\n",
      "  \"unet_depth\": 4,\n",
      "  \"clay_model_size\": \"large\",\n",
      "  \"clay_bands\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\"\n",
      "  ],\n",
      "  \"clay_platform\": \"sentinel-2-l2a\",\n",
      "  \"clay_gsd\": 10,\n",
      "  \"freeze_backbone\": false,\n",
      "  \"clay_checkpoint_path\": \"/home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\",\n",
      "  \"clay_metadata_path\": \"/home/jupyter/MLC-Project/src/Clay/configs/metadata.yaml\",\n",
      "  \"n_train_batches\": 10,\n",
      "  \"num_workers\": 2,\n",
      "  \"epochs\": 500,\n",
      "  \"lr\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"loss_type\": \"mse\",\n",
      "  \"patience\": 50,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"warmup_epochs\": 0,\n",
      "  \"device\": \"cuda\",\n",
      "  \"run_dir\": \"/home/jupyter/MLC-Project/training_runs/UHINetCNN_NYC_20250507_061047\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# %% Configuration / Hyperparameters (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path # For path validation\n",
    "from src.ingest.data_utils import calculate_actual_weather_channels # For dynamic weather channels\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "# project_root is defined in the first cell\n",
    "project_root_str = str(project_root)\n",
    "data_dir = project_root / \"data\"\n",
    "city_name = \"NYC\" # Should be defined or loaded\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_UHINetCNN\"\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "feature_resolution_m = 30 #At least 30\n",
    "uhi_grid_resolution_m = 20\n",
    "clay_proj_channels = 32 # Number of channels to project Clay features to\n",
    "\n",
    "# --- Define Absolute Input Data Paths Directly ---\n",
    "uhi_csv_path = data_dir / city_name / \"uhi.csv\"\n",
    "bronx_weather_csv_path = data_dir / city_name / \"bronx_weather.csv\"\n",
    "manhattan_weather_csv_path = data_dir / city_name / \"manhattan_weather.csv\" # Ensure this matches actual filename\n",
    "\n",
    "dem_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dem_nasadem_native-resolution_pc.tif\"\n",
    "dsm_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dsm_cop-dem-glo-30_native-resolution_pc.tif\"\n",
    "cloudless_mosaic_path = data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\" # Added .npy\n",
    "# Assuming the LST filename structure from download_data.ipynb if it's used\n",
    "lst_time_window_str_for_filename = \"20210601_to_20210901\" # Match download_data if it defines LST filename like this\n",
    "single_lst_median_path = data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_{lst_time_window_str_for_filename}.npy\" # Corrected and added .npy\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = np.nan # Or np.nan if that's what your files use\n",
    "lst_nodata = np.nan # Or np.nan\n",
    "\n",
    "# --- Weather Feature Selection --- NEW ---\n",
    "enabled_weather_features = [\n",
    "    \"rel_humidity\", \n",
    "    \"avg_windspeed\", \n",
    "    \"wind_direction\",       # This will be converted to sin/cos components by data_utils\n",
    "    \"solar_flux\",\n",
    "    \"air_temp\"\n",
    "] \n",
    "\n",
    "#enabled_weather_features = [\"rel_humidity\", \"avg_windspeed\", \"wind_direction\", \"solar_flux\", \"air_temp\"] \n",
    "\n",
    "# Calculate the number of actual weather channels that will be produced by the dataloader\n",
    "actual_dataloader_weather_channels = calculate_actual_weather_channels(enabled_weather_features)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Feature Selection Flags --- #\n",
    "feature_flags = {\n",
    "    \"use_dem\": False,\n",
    "    \"use_dsm\": False,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False, \n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"] #Don't change\n",
    "\n",
    "# --- Model Config (UHINetCNN) ---\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = False # Set to False to allow finetuning Clay's last layer + projection\n",
    "clay_checkpoint_path = project_root / \"notebooks\" / \"clay-v1.5.ckpt\"\n",
    "clay_metadata_path = project_root / \"src\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Head Configuration\n",
    "head_type = \"unet\"  # Options: \"unet\" or \"simple_cnn\"\n",
    "# U-Net specific\n",
    "unet_base_channels = 64 \n",
    "unet_depth = 4         \n",
    "unet_dropout_rate = 0.1 # Dropout for U-Net blocks\n",
    "# SimpleCNN specific\n",
    "simple_cnn_hidden_dims = [64, 32] # Example, adjust as needed\n",
    "simple_cnn_kernel_size = 3\n",
    "simple_cnn_dropout_rate = 0.1 # Dropout for SimpleCNN Head\n",
    "\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 2\n",
    "epochs = 500\n",
    "lr = 5e-5 # Initial: 5e-5. Consider 1e-3 or 5e-4 if using mean loss\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse' # Options: 'mse' or 'mae'. Ensure this matches your loss function implementation (sum vs mean)\n",
    "patience = 50\n",
    "cpu = False\n",
    "n_train_batches = 10\n",
    "max_grad_norm = 1.0 \n",
    "warmup_epochs = 0\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Validate Paths (using check_path for files that *must* exist) ---\n",
    "uhi_csv_path = check_path(uhi_csv_path, project_root, \"UHI CSV\")\n",
    "bronx_weather_csv_path = check_path(bronx_weather_csv_path, project_root, \"Bronx Weather CSV\")\n",
    "manhattan_weather_csv_path = check_path(manhattan_weather_csv_path, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "if feature_flags[\"use_dem\"]:\n",
    "    dem_path = check_path(dem_path, project_root, \"DEM TIF\")\n",
    "if feature_flags[\"use_dsm\"]:\n",
    "    dsm_path = check_path(dsm_path, project_root, \"DSM TIF\")\n",
    "if feature_flags[\"use_clay\"]:\n",
    "    clay_checkpoint_path = check_path(clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "    clay_metadata_path = check_path(clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "\n",
    "if (feature_flags[\"use_clay\"] or \n",
    "    feature_flags[\"use_sentinel_composite\"] or \n",
    "    feature_flags[\"use_ndvi\"] or \n",
    "    feature_flags[\"use_ndbi\"] or \n",
    "    feature_flags[\"use_ndwi\"]):\n",
    "    cloudless_mosaic_path = check_path(cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "\n",
    "if feature_flags[\"use_lst\"]:\n",
    "    single_lst_median_path = check_path(single_lst_median_path, project_root, \"Single LST Median\", should_exist=True)\n",
    "\n",
    "# --- Calculate Bounds --- #\n",
    "uhi_df = pd.read_csv(uhi_csv_path) \n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds_list = [ \n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {uhi_csv_path.name}: {bounds_list}\")\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"UHINetCNN\", \n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m,\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m,\n",
    "    \"clay_proj_channels\": clay_proj_channels, # Added\n",
    "    \"enabled_weather_features\": enabled_weather_features, \n",
    "    \"uhi_csv\": str(uhi_csv_path),\n",
    "    \"bronx_weather_csv\": str(bronx_weather_csv_path),\n",
    "    \"manhattan_weather_csv\": str(manhattan_weather_csv_path),\n",
    "    \"bounds\": bounds_list, \n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(dem_path) if feature_flags[\"use_dem\"] else None,\n",
    "    \"dsm_path\": str(dsm_path) if feature_flags[\"use_dsm\"] else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(cloudless_mosaic_path) if (\n",
    "        feature_flags.get(\"use_clay\") or \n",
    "        feature_flags.get(\"use_sentinel_composite\") or \n",
    "        feature_flags.get(\"use_ndvi\") or \n",
    "        feature_flags.get(\"use_ndbi\") or \n",
    "        feature_flags.get(\"use_ndwi\")\n",
    "    ) else None,\n",
    "    \"single_lst_median_path\": str(single_lst_median_path) if feature_flags[\"use_lst\"] else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config \n",
    "    \"head_type\": head_type, # Added\n",
    "    \"unet_base_channels\": unet_base_channels,\n",
    "    \"unet_depth\": unet_depth,\n",
    "    \"unet_dropout_rate\": unet_dropout_rate, # Added\n",
    "    \"simple_cnn_hidden_dims\": simple_cnn_hidden_dims, # Added\n",
    "    \"simple_cnn_kernel_size\": simple_cnn_kernel_size, # Added\n",
    "    \"simple_cnn_dropout_rate\": simple_cnn_dropout_rate, # Added\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"max_grad_norm\": max_grad_norm, \n",
    "    \"warmup_epochs\": warmup_epochs, \n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "# --- Create Run Directory & Update Config ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name_suffix = f\"{config['model_type']}_{city_name}_{config['head_type']}_{run_timestamp}\" # Added head_type to name\n",
    "run_dir = output_dir_base / run_name_suffix\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"run_dir\"] = str(run_dir) \n",
    "\n",
    "print(f\"\\\\nRun directory: {run_dir}\")\n",
    "print(\"UHINetCNN Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca486df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:10:47,593 - INFO - Dataloader will produce 6 weather channels based on enabled features: ['rel_humidity', 'avg_windspeed', 'wind_direction', 'solar_flux', 'air_temp']\n",
      "2025-05-07 06:10:47,594 - INFO - Target FEATURE grid size (H, W): (373, 323) @ 30m, CRS: EPSG:4326\n",
      "2025-05-07 06:10:47,595 - INFO - Target UHI grid size (H, W): (559, 485) @ 20m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CityDataSet (for CNN model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 1175.56it/s]\n",
      "2025-05-07 06:10:47,682 - INFO - Loading cloudless mosaic from /home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy with memory mapping\n",
      "2025-05-07 06:10:47,683 - INFO - Loaded mosaic shape (native res): (5, 1119, 1278)\n",
      "2025-05-07 06:10:47,692 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-05-07 06:10:47,692 - INFO - Loaded Manhattan weather data: 169 records\n",
      "2025-05-07 06:10:47,694 - INFO - Computed grid cell center coordinates for CRS: EPSG:4326.\n",
      "2025-05-07 06:10:47,696 - INFO - Computed grid cell center coordinates for weather grid at feature resolution.\n",
      "2025-05-07 06:10:47,697 - INFO - Dataset initialized for NYC with 59 unique timestamps.\n",
      "2025-05-07 06:10:47,697 - INFO - Enabled features (flags): {\"use_dem\": false, \"use_dsm\": false, \"use_clay\": true, \"use_sentinel_composite\": false, \"use_lst\": false, \"use_ndvi\": false, \"use_ndbi\": false, \"use_ndwi\": false}\n",
      "2025-05-07 06:10:47,698 - INFO - DEM loaded: False\n",
      "2025-05-07 06:10:47,698 - INFO - DSM loaded: False\n",
      "2025-05-07 06:10:47,699 - INFO - LST loaded: False\n",
      "2025-05-07 06:10:47,699 - INFO - Mosaic loaded: True\n",
      "2025-05-07 06:10:47,700 - INFO - Calculating UHI statistics from training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 47 training (indices 0-46), 12 validation (indices 47-58) samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stats: 100%|██████████| 47/47 [00:00<00:00, 4765.56it/s]\n",
      "2025-05-07 06:10:47,714 - INFO - Training UHI Mean: 1.0002, Std Dev: 0.0168\n",
      "2025-05-07 06:10:47,715 - INFO - Creating dataloaders...\n",
      "2025-05-07 06:10:47,716 - INFO - Using Train Batch Size: 4\n",
      "2025-05-07 06:10:47,717 - INFO - Using Validation Batch Size: 1\n",
      "2025-05-07 06:10:47,718 - INFO - Data loading setup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and preprocessing for CNN model complete.\n"
     ]
    }
   ],
   "source": [
    "# %% Data Loading and Preprocessing (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, \n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import random_split, Subset # MODIFIED: Added random_split\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing CityDataSet (for CNN model)...\")\n",
    "try:\n",
    "    # Ensure all necessary parameters from the config are passed to CityDataSet\n",
    "    dataset = CityDataSet(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"],\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        uhi_csv=config[\"uhi_csv\"],\n",
    "        bronx_weather_csv=config[\"bronx_weather_csv\"],\n",
    "        manhattan_weather_csv=config[\"manhattan_weather_csv\"],\n",
    "        data_dir=project_root_str, # data_dir should be project_root for this loader\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], # NEW: Pass from config\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"],\n",
    "        dsm_path=config[\"dsm_path\"],\n",
    "        elevation_nodata=config[\"elevation_nodata\"],\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split (Random, similar to old setup) --- # MODIFIED\n",
    "val_percent = 0.40 # MODIFIED: Changed to 40% validation\n",
    "num_samples = len(dataset)\n",
    "\n",
    "if num_samples < 2: \n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_val = int(num_samples * val_percent)\n",
    "n_train = num_samples - n_val\n",
    "\n",
    "if n_train == 0 or n_val == 0: \n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "# Use random_split for a random split, with a generator for reproducibility\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42)) # MODIFIED\n",
    "\n",
    "print(f\"Random dataset split: {len(train_ds)} training, {len(val_ds)} validation samples (using seed 42).\") # MODIFIED\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n",
    "print(\"Data loading and preprocessing for CNN model complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9848de83-b653-4fb8-b8d4-d6adb8d8844a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing UHINetCNN...\n",
      "Manually loading checkpoint: /home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\n",
      "Instantiating ClayMAEModule manually...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:10:54,369 - INFO - Loading pretrained weights from Hugging Face hub (timm/vit_large_patch14_reg4_dinov2.lvd142m)\n",
      "2025-05-07 06:10:54,497 - INFO - [timm/vit_large_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state_dict manually into self.model.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:10:58,065 - INFO - Identified final encoder layer as self.model.model.proj\n",
      "2025-05-07 06:10:58,066 - INFO - Unfreezing the final encoder layer (self.model.model.proj) of the Clay backbone.\n",
      "2025-05-07 06:10:58,071 - INFO - ClayFeatureExtractor output channels set to: 1024\n",
      "2025-05-07 06:10:58,113 - INFO - Initialized Clay model (large), output channels: 1024\n",
      "2025-05-07 06:10:58,114 - INFO - Total input channels for feature head: 1030\n",
      "2025-05-07 06:10:58,243 - INFO - Initialized UNetDecoder. In channels: 1030, Base channels: 64, Depth: 4\n",
      "2025-05-07 06:10:58,244 - INFO - UHINetCNN using UNetDecoder head. Output channels: 64\n",
      "2025-05-07 06:10:58,245 - INFO - UHINetCNN final processor target UHI grid: (559, 485)\n",
      "2025-05-07 06:10:58,246 - INFO - Initialized FinalUpsamplerAndProjection: Bicubic upsampling. InCh=64, Target=(559,485).\n",
      "2025-05-07 06:10:58,247 - INFO - UHINetCNN initialized completely with unet head.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Overriding patch size from hparams. Using fixed patch_size = 16\n",
      "Clay model properties: model_size=large, embed_dim=1024, patch_size=16 (patch_size OVERRIDDEN)\n",
      "Normalization prepared for bands: ['blue', 'green', 'red', 'nir']\n",
      "UHINetCNN initialized successfully.\n",
      "Optimizer (AdamW) initialized.\n",
      "Loss function set to masked_mse_loss.\n",
      "\n",
      "Model, optimizer, loss function, and scheduler setup complete.\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# %% Model Initialization (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import necessary components ---\n",
    "from src.model import UHINetCNN # Ensure this is the correct model for the CNN pipeline\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Added scheduler import\n",
    "import logging # Ensure logging is imported if not already\n",
    "\n",
    "# Instantiate the UHINetCNN (or your chosen CNN model)\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "try:\n",
    "    # Ensure parameters match the UHINetCNN constructor\n",
    "    model = UHINetCNN(\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], \n",
    "        bounds=config[\"bounds\"],\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        clay_proj_channels=config[\"clay_proj_channels\"], # Added\n",
    "        sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"),\n",
    "        clay_model_size=config.get(\"clay_model_size\"),\n",
    "        clay_bands=config.get(\"clay_bands\"),\n",
    "        clay_platform=config.get(\"clay_platform\"),\n",
    "        clay_gsd=config.get(\"clay_gsd\"),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "        clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "        # Head specific parameters\n",
    "        head_type=config[\"head_type\"],\n",
    "        unet_base_channels=config[\"unet_base_channels\"],\n",
    "        unet_depth=config[\"unet_depth\"],\n",
    "        unet_dropout_rate=config[\"unet_dropout_rate\"], # Added\n",
    "        simple_cnn_hidden_dims=config[\"simple_cnn_hidden_dims\"], # Added\n",
    "        simple_cnn_kernel_size=config[\"simple_cnn_kernel_size\"], # Added\n",
    "        simple_cnn_dropout_rate=config[\"simple_cnn_dropout_rate\"] # Added\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"{config['model_type']} initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing UHINetCNN: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception after logging\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "print(\"Optimizer (AdamW) initialized.\")\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "    print(\"Loss function set to masked_mse_loss.\")\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "    print(\"Loss function set to masked_mae_loss.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "scheduler = None \n",
    "\"\"\"\n",
    "if config.get(\"patience\"): \n",
    "    try:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=config.get(\"scheduler_patience\", 10), factor=0.5)\n",
    "        print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing scheduler: {e}\", exc_info=True)\n",
    "        print(\"Proceeding without LR scheduler due to initialization error.\")\n",
    "else:\n",
    "    print(\"Patience not set in config, proceeding without LR scheduler.\")\n",
    "\"\"\"\n",
    "print(\"\\\\nModel, optimizer, loss function, and scheduler setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abac67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model UHINetCNN training starting on cuda\n",
      "Checkpoints and logs will be saved to: /home/jupyter/MLC-Project/training_runs/UHINetCNN_NYC_20250507_061047\n",
      "Saved configuration to /home/jupyter/MLC-Project/training_runs/UHINetCNN_NYC_20250507_061047/config.json\n",
      "Using Training UHI Mean: 1.0002, Std Dev: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marnava1304\u001b[0m (\u001b[33marnava1304-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/MLC-Project/notebooks/wandb/run-20250507_061058-ho77t54m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/ho77t54m' target=\"_blank\">NYC_UHINetCNN_20250507_0610</a></strong> to <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/ho77t54m' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/ho77t54m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb initialized for run: UHINetCNN_NYC_20250507_061047\n",
      "Starting training for 500 epochs with patience 50\n",
      "--- Epoch 1/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1681, Train RMSE: 0.0181, Train R2: -0.1678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7209, Val RMSE:   0.0142, Val R2:   0.0045\n",
      "Early epoch 1/100. Skipping checkpointing and early stopping.\n",
      "Epoch 1/500 completed in 15.57s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 2/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1438, Train RMSE: 0.0180, Train R2: -0.1544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7358, Val RMSE:   0.0144, Val R2:   -0.0152\n",
      "Early epoch 2/100. Skipping checkpointing and early stopping.\n",
      "Epoch 2/500 completed in 14.56s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 3/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9999, Train RMSE: 0.0168, Train R2: -0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7129, Val RMSE:   0.0141, Val R2:   0.0196\n",
      "Early epoch 3/100. Skipping checkpointing and early stopping.\n",
      "Epoch 3/500 completed in 14.60s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 4/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9272, Train RMSE: 0.0163, Train R2: 0.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6983, Val RMSE:   0.0140, Val R2:   0.0396\n",
      "Early epoch 4/100. Skipping checkpointing and early stopping.\n",
      "Epoch 4/500 completed in 14.70s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 5/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0197, Train RMSE: 0.0170, Train R2: -0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6947, Val RMSE:   0.0139, Val R2:   0.0487\n",
      "Early epoch 5/100. Skipping checkpointing and early stopping.\n",
      "Epoch 5/500 completed in 14.78s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 6/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0006, Train RMSE: 0.0167, Train R2: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7794, Val RMSE:   0.0148, Val R2:   -0.0677\n",
      "Early epoch 6/100. Skipping checkpointing and early stopping.\n",
      "Epoch 6/500 completed in 14.67s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 7/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9700, Train RMSE: 0.0166, Train R2: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7284, Val RMSE:   0.0142, Val R2:   0.0079\n",
      "Early epoch 7/100. Skipping checkpointing and early stopping.\n",
      "Epoch 7/500 completed in 14.66s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 8/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9721, Train RMSE: 0.0167, Train R2: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8013, Val RMSE:   0.0150, Val R2:   -0.0962\n",
      "Early epoch 8/100. Skipping checkpointing and early stopping.\n",
      "Epoch 8/500 completed in 14.55s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 9/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9816, Train RMSE: 0.0166, Train R2: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6828, Val RMSE:   0.0138, Val R2:   0.0633\n",
      "Early epoch 9/100. Skipping checkpointing and early stopping.\n",
      "Epoch 9/500 completed in 14.60s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 10/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9326, Train RMSE: 0.0162, Train R2: 0.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6723, Val RMSE:   0.0137, Val R2:   0.0810\n",
      "Early epoch 10/100. Skipping checkpointing and early stopping.\n",
      "Epoch 10/500 completed in 14.59s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 11/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9540, Train RMSE: 0.0164, Train R2: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7185, Val RMSE:   0.0142, Val R2:   0.0173\n",
      "Early epoch 11/100. Skipping checkpointing and early stopping.\n",
      "Epoch 11/500 completed in 14.57s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 12/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9455, Train RMSE: 0.0162, Train R2: 0.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6983, Val RMSE:   0.0140, Val R2:   0.0406\n",
      "Early epoch 12/100. Skipping checkpointing and early stopping.\n",
      "Epoch 12/500 completed in 14.63s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 13/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8979, Train RMSE: 0.0161, Train R2: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7284, Val RMSE:   0.0143, Val R2:   -0.0021\n",
      "Early epoch 13/100. Skipping checkpointing and early stopping.\n",
      "Epoch 13/500 completed in 14.66s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 14/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8682, Train RMSE: 0.0157, Train R2: 0.1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6973, Val RMSE:   0.0139, Val R2:   0.0489\n",
      "Early epoch 14/100. Skipping checkpointing and early stopping.\n",
      "Epoch 14/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 15/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9056, Train RMSE: 0.0157, Train R2: 0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6729, Val RMSE:   0.0137, Val R2:   0.0769\n",
      "Early epoch 15/100. Skipping checkpointing and early stopping.\n",
      "Epoch 15/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 16/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8276, Train RMSE: 0.0152, Train R2: 0.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6930, Val RMSE:   0.0139, Val R2:   0.0568\n",
      "Early epoch 16/100. Skipping checkpointing and early stopping.\n",
      "Epoch 16/500 completed in 14.67s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 17/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8463, Train RMSE: 0.0154, Train R2: 0.1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7577, Val RMSE:   0.0145, Val R2:   -0.0358\n",
      "Early epoch 17/100. Skipping checkpointing and early stopping.\n",
      "Epoch 17/500 completed in 14.63s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 18/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8258, Train RMSE: 0.0153, Train R2: 0.1681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6837, Val RMSE:   0.0138, Val R2:   0.0705\n",
      "Early epoch 18/100. Skipping checkpointing and early stopping.\n",
      "Epoch 18/500 completed in 14.61s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 19/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7871, Train RMSE: 0.0149, Train R2: 0.2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7003, Val RMSE:   0.0140, Val R2:   0.0412\n",
      "Early epoch 19/100. Skipping checkpointing and early stopping.\n",
      "Epoch 19/500 completed in 14.61s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 20/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8284, Train RMSE: 0.0152, Train R2: 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7451, Val RMSE:   0.0144, Val R2:   -0.0113\n",
      "Early epoch 20/100. Skipping checkpointing and early stopping.\n",
      "Epoch 20/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 21/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7494, Train RMSE: 0.0146, Train R2: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7214, Val RMSE:   0.0142, Val R2:   0.0165\n",
      "Early epoch 21/100. Skipping checkpointing and early stopping.\n",
      "Epoch 21/500 completed in 14.70s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 22/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7717, Train RMSE: 0.0148, Train R2: 0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7876, Val RMSE:   0.0147, Val R2:   -0.0648\n",
      "Early epoch 22/100. Skipping checkpointing and early stopping.\n",
      "Epoch 22/500 completed in 14.65s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 23/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7962, Train RMSE: 0.0149, Train R2: 0.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7539, Val RMSE:   0.0145, Val R2:   -0.0285\n",
      "Early epoch 23/100. Skipping checkpointing and early stopping.\n",
      "Epoch 23/500 completed in 14.69s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 24/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7197, Train RMSE: 0.0142, Train R2: 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7422, Val RMSE:   0.0144, Val R2:   -0.0157\n",
      "Early epoch 24/100. Skipping checkpointing and early stopping.\n",
      "Epoch 24/500 completed in 14.66s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 25/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6941, Train RMSE: 0.0141, Train R2: 0.2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.6937, Val RMSE:   0.0139, Val R2:   0.0573\n",
      "Early epoch 25/100. Skipping checkpointing and early stopping.\n",
      "Epoch 25/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 26/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7325, Train RMSE: 0.0142, Train R2: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7320, Val RMSE:   0.0143, Val R2:   0.0037\n",
      "Early epoch 26/100. Skipping checkpointing and early stopping.\n",
      "Epoch 26/500 completed in 14.63s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 27/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6819, Train RMSE: 0.0138, Train R2: 0.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8284, Val RMSE:   0.0151, Val R2:   -0.1220\n",
      "Early epoch 27/100. Skipping checkpointing and early stopping.\n",
      "Epoch 27/500 completed in 14.66s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 28/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6475, Train RMSE: 0.0135, Train R2: 0.3491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8891, Val RMSE:   0.0157, Val R2:   -0.2151\n",
      "Early epoch 28/100. Skipping checkpointing and early stopping.\n",
      "Epoch 28/500 completed in 14.70s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 29/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6540, Train RMSE: 0.0135, Train R2: 0.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8553, Val RMSE:   0.0155, Val R2:   -0.1739\n",
      "Early epoch 29/100. Skipping checkpointing and early stopping.\n",
      "Epoch 29/500 completed in 14.69s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 30/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6446, Train RMSE: 0.0135, Train R2: 0.3548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8066, Val RMSE:   0.0150, Val R2:   -0.0993\n",
      "Early epoch 30/100. Skipping checkpointing and early stopping.\n",
      "Epoch 30/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 31/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6484, Train RMSE: 0.0134, Train R2: 0.3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.7604, Val RMSE:   0.0145, Val R2:   -0.0321\n",
      "Early epoch 31/100. Skipping checkpointing and early stopping.\n",
      "Epoch 31/500 completed in 14.62s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 32/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5851, Train RMSE: 0.0130, Train R2: 0.4013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8326, Val RMSE:   0.0152, Val R2:   -0.1345\n",
      "Early epoch 32/100. Skipping checkpointing and early stopping.\n",
      "Epoch 32/500 completed in 14.60s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 33/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6359, Train RMSE: 0.0133, Train R2: 0.3695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8184, Val RMSE:   0.0151, Val R2:   -0.1145\n",
      "Early epoch 33/100. Skipping checkpointing and early stopping.\n",
      "Epoch 33/500 completed in 14.65s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 34/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6100, Train RMSE: 0.0132, Train R2: 0.3821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8012, Val RMSE:   0.0150, Val R2:   -0.0983\n",
      "Early epoch 34/100. Skipping checkpointing and early stopping.\n",
      "Epoch 34/500 completed in 14.60s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 35/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6325, Train RMSE: 0.0132, Train R2: 0.3808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8967, Val RMSE:   0.0158, Val R2:   -0.2297\n",
      "Early epoch 35/100. Skipping checkpointing and early stopping.\n",
      "Epoch 35/500 completed in 14.70s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 36/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5838, Train RMSE: 0.0129, Train R2: 0.4094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.8821, Val RMSE:   0.0157, Val R2:   -0.2021\n",
      "Early epoch 36/100. Skipping checkpointing and early stopping.\n",
      "Epoch 36/500 completed in 14.65s\n",
      "Current LR: 5.00e-05\n",
      "--------------------------------------------------------------------------------\n",
      "--- Epoch 37/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# %% Training Loop (CNN Model)\n",
    "\n",
    "# --- Imports ---\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import src.train.train_utils as train_utils # Import the full module\n",
    "import numpy as np # Added for isnan check\n",
    "import pandas as pd # For saving log\n",
    "\n",
    "# --- Setup ---\n",
    "print(f\"Model {config['model_type']} training starting on {device}\")\n",
    "\n",
    "# --- Tracking Variables --- #\n",
    "best_val_r2 = -float('inf') \n",
    "patience_counter = 0\n",
    "training_log_list = [] # Local log for metrics\n",
    "\n",
    "# --- Output Directory & Run Name --- #\n",
    "run_dir = Path(config[\"run_dir\"]) # Get from config\n",
    "model_save_dir = run_dir / \"checkpoints\"\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoints and logs will be saved to: {run_dir}\")\n",
    "\n",
    "# --- Save Config --- #\n",
    "config_path = run_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x)\n",
    "print(f\"Saved configuration to {config_path}\")\n",
    "\n",
    "# --- Retrieve UHI Stats from Config --- #\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean/uhi_std not in config. Run data loading cell.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f}\")\n",
    "\n",
    "# --- WANDB Init --- #\n",
    "if wandb:\n",
    "    try:\n",
    "        if wandb.run is not None: wandb.finish() # Finish previous run if any\n",
    "        wandb.init(\n",
    "            project=config[\"wandb_project_name\"],\n",
    "            name=f\"{config['wander_run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Optional: watch model parameters\n",
    "        wandb.watch(model)\n",
    "        print(f\"Wandb initialized for run: {run_dir.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization failed: {e}\")\n",
    "        wandb = None\n",
    "else:\n",
    "    print(\"Wandb not available, skipping logging.\")\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "# Get warmup_epochs from config, default to 0 if not present\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 0)\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, uhi_mean, uhi_std,\n",
    "                feature_flags=config[\"feature_flags\"],\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            current_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            current_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(current_metrics)\n",
    "        training_log_list.append(current_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, uhi_mean, uhi_std,\n",
    "                feature_flags=config[\"feature_flags\"]\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            current_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after 50 epochs\n",
    "            if epoch >= 100:\n",
    "                # Check for improvement (using validation R2 now)\n",
    "                if val_r2 > best_val_r2:\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    train_utils.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "                        'best_val_r2': best_val_r2,\n",
    "                        'config': config\n",
    "                    }, is_best=True, output_dir=model_save_dir)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_r2 {val_r2:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Early epoch {epoch+1}/100. Skipping checkpointing and early stopping.\")\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log_list)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00bb52-afcb-4cdc-b443-67031c27324a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b58f3-130e-4ab3-8a29-afe5dc8f9369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c661d-93d3-4d70-bc3a-f7c81e146a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
