{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97feb2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Setup & Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset # MODIFIED: Added Subset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import shutil # For checkpoint saving\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(os.getcwd()).parent # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- Import Model Components ---\n",
    "from src.model import UHINetCNN # Import the CNN model\n",
    "from src.ingest.dataloader_cnn import CityDataSet # MODIFIED: Import the updated dataloader\n",
    "\n",
    "# --- Import Training Utilities & Loss --- ### MODIFIED ###\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the new utility module\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optionally import wandb if needed\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Import necessary metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860c52a0-0d4b-4e3e-a2fc-363a45e9abc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### UNCOMMENT ON FIRST RUN IF USING Clay\n",
    "#!wget -q https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e727ff",
   "metadata": {},
   "source": [
    "# %% Configuration / Hyperparameters (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path # For path validation\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "# project_root is defined in the first cell\n",
    "project_root_str = str(project_root)\n",
    "data_dir = project_root / \"data\"\n",
    "city_name = \"NYC\" # Should be defined or loaded\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_UHINetCNN\"\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "feature_resolution_m = 50\n",
    "uhi_grid_resolution_m = 50\n",
    "\n",
    "# --- Define Absolute Input Data Paths Directly ---\n",
    "uhi_csv_path = data_dir / city_name / \"uhi.csv\"\n",
    "bronx_weather_csv_path = data_dir / city_name / \"bronx_weather.csv\"\n",
    "manhattan_weather_csv_path = data_dir / city_name / \"manhattan_weather.csv\" # Ensure this matches actual filename\n",
    "\n",
    "dem_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dem_nasadem_native-resolution_pc.tif\"\n",
    "dsm_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dsm_cop-dem-glo-30_native-resolution_pc.tif\"\n",
    "cloudless_mosaic_path = data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\" # Added .npy\n",
    "# Assuming the LST filename structure from download_data.ipynb if it's used\n",
    "lst_time_window_str_for_filename = \"20210601_to_20210901\" # Match download_data if it defines LST filename like this\n",
    "single_lst_median_path = data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_{lst_time_window_str_for_filename}.npy\" # Corrected and added .npy\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0 # Or np.nan if that's what your files use\n",
    "lst_nodata = 0.0 # Or np.nan\n",
    "\n",
    "# --- Feature Selection Flags --- #\n",
    "feature_flags = {\n",
    "    \"use_dem\": False,\n",
    "    \"use_dsm\": True,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False, # Set to True if you intend to use LST\n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# --- Model Config (UHINetCNN) ---\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "clay_checkpoint_path = project_root / \"notebooks\" / \"clay-v1.5.ckpt\"\n",
    "clay_metadata_path = project_root / \"src\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "weather_channels = 6\n",
    "unet_base_channels = 64 # For UHINetCNN's U-Net like structure\n",
    "unet_depth = 4         # For UHINetCNN's U-Net like structure\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 4\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "n_train_batches = 9\n",
    "max_grad_norm = 1.0 # Added for consistency with branched\n",
    "warmup_epochs = 5 # Added for consistency with branched\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Validate Paths (using check_path for files that *must* exist) ---\n",
    "# Fixed: Removed is_absolute parameter to match updated function signature\n",
    "uhi_csv_path = check_path(uhi_csv_path, project_root, \"UHI CSV\")\n",
    "bronx_weather_csv_path = check_path(bronx_weather_csv_path, project_root, \"Bronx Weather CSV\")\n",
    "manhattan_weather_csv_path = check_path(manhattan_weather_csv_path, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "if feature_flags[\"use_dem\"]:\n",
    "    dem_path = check_path(dem_path, project_root, \"DEM TIF\")\n",
    "if feature_flags[\"use_dsm\"]:\n",
    "    dsm_path = check_path(dsm_path, project_root, \"DSM TIF\")\n",
    "if feature_flags[\"use_clay\"]:\n",
    "    clay_checkpoint_path = check_path(clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "    clay_metadata_path = check_path(clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "# Check cloudless mosaic if Clay or direct Sentinel composite is used\n",
    "if feature_flags[\"use_clay\"] or feature_flags[\"use_sentinel_composite\"]:\n",
    "    cloudless_mosaic_path = check_path(cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "if feature_flags[\"use_lst\"]:\n",
    "    single_lst_median_path = check_path(single_lst_median_path, project_root, \"Single LST Median\", should_exist=True)\n",
    "\n",
    "\n",
    "# --- Calculate Bounds --- #\n",
    "uhi_df = pd.read_csv(uhi_csv_path) # Use the validated path\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds_list = [ # Renamed to avoid conflict if 'bounds' is a global from another notebook\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {uhi_csv_path.name}: {bounds_list}\")\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"UHINetCNN\", # Specific to this notebook\n",
    "    \"project_root\": project_root_str,\n",
    "    # \"run_dir\" will be added below\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m,\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m,\n",
    "    \"uhi_csv\": str(uhi_csv_path),\n",
    "    \"bronx_weather_csv\": str(bronx_weather_csv_path),\n",
    "    \"manhattan_weather_csv\": str(manhattan_weather_csv_path),\n",
    "    \"bounds\": bounds_list, # Use the locally defined bounds_list\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(dem_path) if feature_flags[\"use_dem\"] else None,\n",
    "    \"dsm_path\": str(dsm_path) if feature_flags[\"use_dsm\"] else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(cloudless_mosaic_path) if feature_flags.get(\"use_clay\") or feature_flags.get(\"use_sentinel_composite\") else None,\n",
    "    \"single_lst_median_path\": str(single_lst_median_path) if feature_flags[\"use_lst\"] else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config (UHINetCNN specific)\n",
    "    \"weather_channels\": weather_channels, # For single weather grid input\n",
    "    \"unet_base_channels\": unet_base_channels,\n",
    "    \"unet_depth\": unet_depth,\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"max_grad_norm\": max_grad_norm, # Added\n",
    "    \"warmup_epochs\": warmup_epochs, # Added\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "# --- Create Run Directory & Update Config ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name_suffix = f\"{config['model_type']}_{city_name}_{run_timestamp}\"\n",
    "run_dir = output_dir_base / run_name_suffix\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"run_dir\"] = str(run_dir) # Add to config for later use\n",
    "\n",
    "print(f\"\\nRun directory: {run_dir}\")\n",
    "print(\"UHINetCNN Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca486df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "check_path() got an unexpected keyword argument 'is_absolute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# --- Validate Paths (using check_path for files that *must* exist) ---\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Ensure check_path can handle is_absolute=True or adapts for already absolute Path objects\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m uhi_csv_path \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43muhi_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUHI CSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_absolute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m bronx_weather_csv_path \u001b[38;5;241m=\u001b[39m check_path(bronx_weather_csv_path, project_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBronx Weather CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_absolute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m manhattan_weather_csv_path \u001b[38;5;241m=\u001b[39m check_path(manhattan_weather_csv_path, project_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManhattan Weather CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_absolute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: check_path() got an unexpected keyword argument 'is_absolute'"
     ]
    }
   ],
   "source": [
    "# %% Data Loading and Preprocessing (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # MODIFIED: Removed split_data as we do sequential split here\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Ensure Subset is imported\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing CityDataSet (for CNN model)...\")\n",
    "try:\n",
    "    # Ensure all necessary parameters from the config are passed to CityDataSet\n",
    "    dataset = CityDataSet(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"],\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        uhi_csv=config[\"uhi_csv\"],\n",
    "        bronx_weather_csv=config[\"bronx_weather_csv\"],\n",
    "        manhattan_weather_csv=config[\"manhattan_weather_csv\"],\n",
    "        data_dir=project_root_str, # data_dir should be project_root for this loader\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"],\n",
    "        dsm_path=config[\"dsm_path\"],\n",
    "        elevation_nodata=config[\"elevation_nodata\"],\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split (More common for time-series like data if order matters) ---\n",
    "val_percent = 0.20 # Example: 20% for validation\n",
    "num_samples = len(dataset)\n",
    "\n",
    "if num_samples < 2: # Check if there are enough samples for a split\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0: # Check if split results in empty sets\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "# Create sequential indices for train and validation\n",
    "# This assumes your UHI data is chronologically ordered if that's desired for the split\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n",
    "print(\"Data loading and preprocessing for CNN model complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9848de83-b653-4fb8-b8d4-d6adb8d8844a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 23:48:14,353 - INFO - Target FEATURE grid size (H, W): (224, 194) @ 50m, CRS: EPSG:4326\n",
      "2025-05-06 23:48:14,354 - INFO - Target UHI grid size (H, W): (224, 194) @ 50m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CityDataSet (for CNN model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 3740.39it/s]\n",
      "2025-05-06 23:48:14,422 - INFO - Loading DSM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_cop-dem-glo-30_native-resolution_pc.tif\n",
      "2025-05-06 23:48:14,445 - INFO - Clipping DSM to bounds: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "2025-05-06 23:48:14,446 - INFO - Opened DSM (lazy load). Native shape (approx): (1, 364, 415)\n",
      "2025-05-06 23:48:14,447 - INFO - Calculating global DSM min/max...\n",
      "2025-05-06 23:48:14,448 - INFO - Global DSM Min: -14.068702697753906, Max: 186.3260040283203\n",
      "2025-05-06 23:48:14,448 - INFO - Loading cloudless mosaic from /home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy with memory mapping\n",
      "2025-05-06 23:48:14,449 - INFO - Loaded mosaic shape (native res): (4, 1119, 1278)\n",
      "2025-05-06 23:48:14,458 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-05-06 23:48:14,458 - INFO - Loaded Manhattan weather data: 169 records\n",
      "2025-05-06 23:48:14,460 - INFO - Computed grid cell center coordinates.\n",
      "2025-05-06 23:48:14,461 - INFO - Dataset initialized for NYC with 59 unique timestamps.\n",
      "2025-05-06 23:48:14,461 - INFO - Enabled features (flags): {\"use_dem\": false, \"use_dsm\": true, \"use_clay\": true, \"use_sentinel_composite\": false, \"use_lst\": false, \"use_ndvi\": false, \"use_ndbi\": false, \"use_ndwi\": false}\n",
      "2025-05-06 23:48:14,462 - INFO - DEM loaded: False\n",
      "2025-05-06 23:48:14,462 - INFO - DSM loaded: True\n",
      "2025-05-06 23:48:14,463 - INFO - LST loaded: False\n",
      "2025-05-06 23:48:14,463 - INFO - Mosaic loaded: True\n",
      "2025-05-06 23:48:14,464 - INFO - Calculating UHI statistics from training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 47 training (indices 0-46), 12 validation (indices 47-58) samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stats: 100%|██████████| 47/47 [00:00<00:00, 8484.28it/s]\n",
      "2025-05-06 23:48:14,473 - INFO - Training UHI Mean: 1.0004, Std Dev: 0.0169\n",
      "2025-05-06 23:48:14,475 - INFO - Creating dataloaders...\n",
      "2025-05-06 23:48:14,475 - INFO - Using Train Batch Size: 5\n",
      "2025-05-06 23:48:14,476 - INFO - Using Validation Batch Size: 1\n",
      "2025-05-06 23:48:14,477 - INFO - Data loading setup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and preprocessing for CNN model complete.\n"
     ]
    }
   ],
   "source": [
    "# %% Model Initialization (CNN Model + Common Resampling)\n",
    "\n",
    "# --- Import necessary components ---\n",
    "from src.model import UHINetCNN # Ensure this is the correct model for the CNN pipeline\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Added scheduler import\n",
    "import logging # Ensure logging is imported if not already\n",
    "\n",
    "# Instantiate the UHINetCNN (or your chosen CNN model)\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "try:\n",
    "    # Ensure parameters match the UHINetCNN constructor\n",
    "    model = UHINetCNN(\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        weather_channels=config[\"weather_channels\"], # Used for single weather grid\n",
    "        sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"),\n",
    "        clay_model_size=config.get(\"clay_model_size\"),\n",
    "        clay_bands=config.get(\"clay_bands\"),\n",
    "        clay_platform=config.get(\"clay_platform\"),\n",
    "        clay_gsd=config.get(\"clay_gsd\"),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "        clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "        base_channels=config[\"unet_base_channels\"], # Typically CNNs use a U-Net like backbone\n",
    "        depth=config[\"unet_depth\"],\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"{config['model_type']} initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing UHINetCNN: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception after logging\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "print(\"Optimizer (AdamW) initialized.\")\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "    print(\"Loss function set to masked_mse_loss.\")\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "    print(\"Loss function set to masked_mae_loss.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "# Ensure scheduler is defined, even if None, for the training loop\n",
    "scheduler = None \n",
    "if config.get(\"patience\"): # Check if patience is set for scheduler\n",
    "    try:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=config.get(\"scheduler_patience\", 10), factor=0.5)\n",
    "        print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing scheduler: {e}\", exc_info=True)\n",
    "        print(\"Proceeding without LR scheduler due to initialization error.\")\n",
    "else:\n",
    "    print(\"Patience not set in config, proceeding without LR scheduler.\")\n",
    "\n",
    "print(\"\\nModel, optimizer, loss function, and scheduler setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2abac67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing UHINetCNN_CommonRes (CNN variant)...\n",
      "Manually loading checkpoint: /home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\n",
      "Instantiating ClayMAEModule manually...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 23:52:14,411 - INFO - Loading pretrained weights from Hugging Face hub (timm/vit_large_patch14_reg4_dinov2.lvd142m)\n",
      "2025-05-06 23:52:14,520 - INFO - [timm/vit_large_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state_dict manually into self.model.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 23:52:17,655 - INFO - Identified final encoder layer as self.model.model.proj\n",
      "2025-05-06 23:52:17,656 - INFO - Keeping Clay backbone frozen.\n",
      "2025-05-06 23:52:17,660 - INFO - ClayFeatureExtractor output channels set to: 1024\n",
      "2025-05-06 23:52:17,664 - INFO - Initialized Clay model (large), output channels: 1024\n",
      "2025-05-06 23:52:17,664 - INFO - Total calculated input channels for U-Net: 1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Overriding patch size from hparams. Using fixed patch_size = 16\n",
      "Clay model properties: model_size=large, embed_dim=1024, patch_size=16 (patch_size OVERRIDDEN)\n",
      "Normalization prepared for bands: ['blue', 'green', 'red', 'nir']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 23:52:17,919 - INFO - Initialized UNetDecoder. In channels: 1031, Base channels: 64, Depth: 4\n",
      "2025-05-06 23:52:17,921 - INFO - UHINetCNN initialized. U-Net Input Ch: 1031, Base Ch: 64, Depth: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UHINetCNN_CommonRes (CNN variant) initialized successfully.\n",
      "Optimizer (AdamW) initialized.\n",
      "Loss function set to masked_mse_loss.\n",
      "Initialized ReduceLROnPlateau scheduler.\n",
      "\n",
      "CNN Model, optimizer, loss function, and scheduler setup complete.\n"
     ]
    }
   ],
   "source": [
    "# %% Training Loop (CNN Model)\n",
    "\n",
    "# --- Imports ---\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import src.train.train_utils as train_utils # Import the full module\n",
    "import numpy as np # Added for isnan check\n",
    "import pandas as pd # For saving log\n",
    "\n",
    "# --- Setup ---\n",
    "print(f\"Model {config['model_type']} training starting on {device}\")\n",
    "\n",
    "# --- Tracking Variables --- #\n",
    "best_val_r2 = -float('inf') \n",
    "patience_counter = 0\n",
    "training_log_list = [] # Local log for metrics\n",
    "\n",
    "# --- Output Directory & Run Name --- #\n",
    "run_dir = Path(config[\"run_dir\"]) # Get from config\n",
    "model_save_dir = run_dir / \"checkpoints\"\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoints and logs will be saved to: {run_dir}\")\n",
    "\n",
    "# --- Save Config --- #\n",
    "config_path = run_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x)\n",
    "print(f\"Saved configuration to {config_path}\")\n",
    "\n",
    "# --- Retrieve UHI Stats from Config --- #\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean/uhi_std not in config. Run data loading cell.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f}\")\n",
    "\n",
    "# --- WANDB Init --- #\n",
    "if wandb:\n",
    "    try:\n",
    "        if wandb.run is not None: wandb.finish() # Finish previous run if any\n",
    "        wandb.init(\n",
    "            project=config[\"wandb_project_name\"],\n",
    "            name=f\"{config['wander_run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Optional: watch model parameters\n",
    "        wandb.watch(model)\n",
    "        print(f\"Wandb initialized for run: {run_dir.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization failed: {e}\")\n",
    "        wandb = None\n",
    "else:\n",
    "    print(\"Wandb not available, skipping logging.\")\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "# Get warmup_epochs from config, default to 0 if not present\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 0)\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, uhi_mean, uhi_std,\n",
    "                feature_flags=config[\"feature_flags\"],\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            current_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            current_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(current_metrics)\n",
    "        training_log_list.append(current_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, uhi_mean, uhi_std,\n",
    "                feature_flags=config[\"feature_flags\"]\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            current_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after warmup_epochs\n",
    "            if epoch >= warmup_epochs:\n",
    "                # Check for improvement (using validation R2 now)\n",
    "                if val_r2 > best_val_r2:\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    train_utils.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "                        'best_val_r2': best_val_r2,\n",
    "                        'config': config\n",
    "                    }, is_best=True, output_dir=model_save_dir)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_r2 {val_r2:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Warmup epoch {epoch+1}/{warmup_epochs}. Skipping checkpointing and early stopping.\")\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log_list)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
