{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87449c2f",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# --- Metrics --- \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.model import UHINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97feb2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# --- WANDB --- #\n",
    "import wandb\n",
    "# ------------ #\n",
    "\n",
    "# --- Metrics --- \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# ------------ #\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- MODIFIED: Import UHINetCNN --- \n",
    "from src.model import UHINetCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e727ff",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35980f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\\nfrom src.ingest.dataloader import CityDataSet\\n# --- MODIFIED: Import UHINetCNN --- \\nfrom src.model import UHINetCNN \\nfrom src.train.loss import masked_mae_loss, masked_mse_loss\\n\\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca486df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bounds from /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_UHINetCNN\" # Modified prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "resolution_m = 10\n",
    "include_lst = False\n",
    "\n",
    "# Input Data Paths (relative to project root for portability in config)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\" # Corrected potential typo\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "# --- Model Config ---\n",
    "# Clay Backbone\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "# Example Clay Checkpoint/Metadata Paths (replace with your actual paths relative to project root)\n",
    "relative_clay_checkpoint_path = Path(\"models\") / \"Clay_v1.5_epoch=19_val_loss=0.45.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"metadata.yaml\"\n",
    "\n",
    "# UHINetCNN\n",
    "weather_channels = 6\n",
    "time_embed_dim = 2\n",
    "lst_channels = 1 if include_lst else 0\n",
    "proj_ch = 32\n",
    "cnn_hidden_dims = [64, 32] # Defined earlier\n",
    "cnn_kernel_size = 3      # Defined earlier\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "batch_size = 4 # Keep increased size for CNN\n",
    "num_workers = 4\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 10\n",
    "cpu = False\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Sanity Checks and Absolute Paths ---\n",
    "def check_path(relative_path, description):\n",
    "    abs_path = project_root / relative_path\n",
    "    if not abs_path.exists():\n",
    "        raise FileNotFoundError(f\"{description} not found at expected location: {abs_path}\")\n",
    "    return abs_path\n",
    "\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, \"Manhattan Weather CSV\")\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, \"Cloudless Mosaic\")\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, \"Clay Checkpoint\")\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, \"Clay Metadata\")\n",
    "absolute_single_lst_median_path = None\n",
    "if include_lst:\n",
    "    absolute_single_lst_median_path = check_path(relative_single_lst_median_path, \"Single LST Median\")\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Central Config Dictionary for Logging --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"UHINetCNN\", # Updated model type\n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"resolution_m\": resolution_m,\n",
    "    \"include_lst\": include_lst,\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(relative_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(relative_manhattan_weather_csv),\n",
    "    \"cloudless_mosaic_path\": str(relative_cloudless_mosaic_path),\n",
    "    \"single_lst_median_path\": str(relative_single_lst_median_path) if include_lst else None,\n",
    "    \"bounds\": bounds,\n",
    "    # Model Config\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"clay_checkpoint_path\": str(relative_clay_checkpoint_path),\n",
    "    \"clay_metadata_path\": str(relative_clay_metadata_path),\n",
    "    \"weather_channels\": weather_channels,\n",
    "    \"time_embed_dim\": time_embed_dim,\n",
    "    \"lst_channels\": lst_channels,\n",
    "    \"proj_ch\": proj_ch,\n",
    "    \"cnn_hidden_dims\": cnn_hidden_dims, # Added CNN specific\n",
    "    \"cnn_kernel_size\": cnn_kernel_size, # Added CNN specific\n",
    "    # Training Hyperparameters\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration dictionary created:\")\n",
    "# print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd2dde-04dc-44fa-9c48-2840e2ec1e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2abac67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 00:43:59,146 - INFO - Loading cloudless mosaic from /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "2025-04-28 00:43:59,159 - INFO - Loaded mosaic with 4 bands and shape (1122, 1281)\n",
      "/home/jupyter/UHI/MLC-Project/src/ingest/dataloader.py:143: FutureWarning: Parsed string \"2021-07-24 06:00:00 EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  dt_naive_or_aware = pd.to_datetime(self.bronx_weather['datetime'], errors='raise')\n",
      "/home/jupyter/UHI/MLC-Project/src/ingest/dataloader.py:169: FutureWarning: Parsed string \"2021-07-24 06:00:00 EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  dt_naive_or_aware = pd.to_datetime(self.manhattan_weather['datetime'], errors='raise')\n",
      "2025-04-28 00:43:59,185 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-04-28 00:43:59,186 - INFO - Loaded Manhattan weather data: 169 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 00:44:01,072 - INFO - Computed grid cell coordinates and closest station map\n",
      "2025-04-28 00:44:01,075 - INFO - Grid cells assigned to Bronx: 370359\n",
      "2025-04-28 00:44:01,077 - INFO - Grid cells assigned to Manhattan: 712983\n",
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 333.49it/s]\n",
      "2025-04-28 00:44:01,264 - INFO - Dataset initialized for NYC with 59 unique timestamps. LST included: False\n",
      "2025-04-28 00:44:01,265 - INFO - Target grid size (H, W): (1118, 969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 36 training, 23 validation samples.\n",
      "Creating dataloaders...\n",
      "Data loading setup complete.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "## IGNORE TIMEZONE WARNING: Timezone is first incorrectly loaded by pandas but then fixed in our dataloader.\n",
    "\n",
    "print(\"Initializing dataset...\")\n",
    "try:\n",
    "    # Note: averaging_window is needed by constructor but might not be used internally if single_lst_median_path is set\n",
    "    # Use a placeholder value if needed, or ensure the dataloader handles its optional usage.\n",
    "    placeholder_avg_window = 30 # Example placeholder\n",
    "\n",
    "    dataset = CityDataSet(\n",
    "        bounds=bounds,\n",
    "        averaging_window=placeholder_avg_window, # Pass placeholder\n",
    "        resolution_m=resolution_m,\n",
    "        uhi_csv=str(uhi_csv),\n",
    "        # Use station weather CSVs\n",
    "        bronx_weather_csv=str(bronx_weather_csv),\n",
    "        manhattan_weather_csv=str(manhattan_weather_csv),\n",
    "        cloudless_mosaic_path=str(cloudless_mosaic_path),\n",
    "        data_dir=str(data_dir_path),\n",
    "        city_name=city_name,\n",
    "        include_lst=include_lst,\n",
    "        single_lst_median_path=str(single_lst_median_path) if include_lst else None\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    # Stop execution or handle error\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split (Sequential) ---\n",
    "val_percent = 0.40\n",
    "n_samples = len(dataset)\n",
    "\n",
    "if n_samples < 10: # Handle very small datasets\n",
    "    print(f\"Warning: Dataset size ({n_samples}) is very small. Using all data for training.\")\n",
    "    n_val = 0\n",
    "    n_train = n_samples\n",
    "else:\n",
    "    n_val = int(n_samples * val_percent)\n",
    "    n_train = n_samples - n_val\n",
    "\n",
    "# Create sequential split using Subset\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, n_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices) if n_val > 0 else None # Create val_ds only if n_val > 0\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training, {len(val_ds) if val_ds else 0} validation samples.\")\n",
    "\n",
    "print(\"Creating dataloaders...\")\n",
    "# Shuffle training data loader, but not validation\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True) if val_ds else None\n",
    "\n",
    "print(\"Data loading setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06690edc",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2a7dfe-0136-4eed-9e31-5a53a6563c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: train_epoch for UHINetCNN with Dynamic Clay ---\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_preds = []\n",
    "    epoch_targets = []\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Fetch data - including new tensors for dynamic Clay\n",
    "        required_keys = ['cloudless_mosaic', 'weather', 'target', 'mask', 'norm_time_tensor', 'norm_latlon_tensor']\n",
    "        # Note: 'time_emb' is no longer strictly required by the CNN forward pass itself\n",
    "        if model.use_lst: \n",
    "            required_keys.append('lst')\n",
    "        if not all(key in batch for key in required_keys):\n",
    "            missing = [key for key in required_keys if key not in batch]\n",
    "            logging.warning(f\"Skipping batch due to missing keys: {missing}\")\n",
    "            continue\n",
    "\n",
    "        # Move batch to device\n",
    "        try:\n",
    "            cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "            weather = batch[\"weather\"].to(device)             # (B, 1, C_weather, H, W)\n",
    "            lst = batch[\"lst\"].to(device) if model.use_lst else None # (B, 1, C_lst, H, W)\n",
    "            # time_emb = batch[\"time_emb\"].to(device) # Not used by CNN forward pass\n",
    "            target = batch[\"target\"].to(device)               # (B, H, W)\n",
    "            mask = batch[\"mask\"].to(device, dtype=torch.bool) # Ensure mask is boolean\n",
    "            # Tensors for dynamic Clay metadata\n",
    "            norm_time_tensor = batch[\"norm_time_tensor\"].to(device) # (B, 4)\n",
    "            norm_latlon_tensor = batch[\"norm_latlon_tensor\"].to(device) # (B, 4)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error moving batch to device: {e}\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            # Target shape needed for final resizing within model\n",
    "            target_h_w = target.shape[1:]\n",
    "\n",
    "            # --- Single Forward Pass for CNN with Dynamic Clay ---\n",
    "            prediction = model(cloudless_mosaic, \n",
    "                               norm_time_tensor, \n",
    "                               norm_latlon_tensor, \n",
    "                               weather, \n",
    "                               target_h_w, \n",
    "                               lst) # Output: (B, 1, H_target, W_target)\n",
    "            prediction_final = prediction.squeeze(1) # Shape (B, H_target, W_target)\n",
    "            # -----------------------------------------------------\n",
    "\n",
    "            # --- Calculate Loss --- \n",
    "            loss = loss_fn(prediction_final, target, mask)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                 logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                 continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Store valid predictions/targets for metrics ---\n",
    "            with torch.no_grad():\n",
    "                valid_preds = prediction_final[mask].cpu().numpy()\n",
    "                valid_targets = target[mask].cpu().numpy()\n",
    "                if valid_preds.size > 0:\n",
    "                    epoch_preds.append(valid_preds)\n",
    "                    epoch_targets.append(valid_targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"Runtime error during training: {e}\")\n",
    "             if \"out of memory\" in str(e):\n",
    "                 logging.error(\"CUDA out of memory. Try reducing batch size or check model complexity.\")\n",
    "             continue\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "             continue\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    \n",
    "    # --- Calculate Epoch Metrics ---\n",
    "    rmse = np.nan\n",
    "    r2 = np.nan\n",
    "    if epoch_preds:\n",
    "        all_preds = np.concatenate(epoch_preds)\n",
    "        all_targets = np.concatenate(epoch_targets)\n",
    "        if all_preds.size > 0 and all_targets.size > 0:\n",
    "             try:\n",
    "                 rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "                 r2 = r2_score(all_targets, all_preds)\n",
    "             except Exception as metric_e:\n",
    "                  logging.error(f\"Error calculating epoch metrics: {metric_e}\")\n",
    "    \n",
    "    return avg_loss, rmse, r2\n",
    "\n",
    "# --- MODIFIED: validate_epoch for UHINetCNN with Dynamic Clay ---\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_preds = []\n",
    "    epoch_targets = []\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Fetch data\n",
    "            required_keys = ['cloudless_mosaic', 'weather', 'target', 'mask', 'norm_time_tensor', 'norm_latlon_tensor']\n",
    "            if model.use_lst:\n",
    "                required_keys.append('lst')\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                missing = [key for key in required_keys if key not in batch]\n",
    "                logging.warning(f\"Skipping validation batch due to missing keys: {missing}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "                weather = batch[\"weather\"].to(device)\n",
    "                lst = batch[\"lst\"].to(device) if model.use_lst else None\n",
    "                target = batch[\"target\"].to(device)\n",
    "                mask = batch[\"mask\"].to(device, dtype=torch.bool)\n",
    "                norm_time_tensor = batch[\"norm_time_tensor\"].to(device)\n",
    "                norm_latlon_tensor = batch[\"norm_latlon_tensor\"].to(device)\n",
    "\n",
    "                # --- Model Forward Pass --- \n",
    "                target_h_w = target.shape[1:]\n",
    "                prediction = model(cloudless_mosaic, \n",
    "                                   norm_time_tensor, \n",
    "                                   norm_latlon_tensor, \n",
    "                                   weather, \n",
    "                                   target_h_w, \n",
    "                                   lst)\n",
    "                prediction_final = prediction.squeeze(1)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(prediction_final, target, mask)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "                \n",
    "                # Store valid predictions/targets for metrics\n",
    "                valid_preds = prediction_final[mask].cpu().numpy()\n",
    "                valid_targets = target[mask].cpu().numpy()\n",
    "                if valid_preds.size > 0:\n",
    "                    epoch_preds.append(valid_preds)\n",
    "                    epoch_targets.append(valid_targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    \n",
    "    # Calculate Epoch Metrics\n",
    "    rmse = np.nan\n",
    "    r2 = np.nan\n",
    "    if epoch_preds:\n",
    "        all_preds = np.concatenate(epoch_preds)\n",
    "        all_targets = np.concatenate(epoch_targets)\n",
    "        if all_preds.size > 0 and all_targets.size > 0:\n",
    "            try:\n",
    "                rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "                r2 = r2_score(all_targets, all_preds)\n",
    "            except Exception as metric_e:\n",
    "                 logging.error(f\"Error calculating validation epoch metrics: {metric_e}\")\n",
    "    \n",
    "    return avg_loss, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029ea59-819d-4999-8d71-7a23ed2afc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc639c88-302c-4d12-a717-89683df94156",
   "metadata": {},
   "source": [
    "## Helper Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1b3749-8013-4b55-9478-722d87783b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\n",
    "    Args:\n",
    "        state (dict): Contains model's state_dict, optimizer state, epoch, etc.\n",
    "        is_best (bool): True if this is the best model seen so far.\n",
    "        filename (str): Path to save the latest checkpoint.\n",
    "        best_filename (str): Path to save the best checkpoint.\n",
    "    \"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "        required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "        if include_lst:\n",
    "            required_keys.append('lst_seq')\n",
    "        if not all(key in batch for key in required_keys):\n",
    "            missing = [key for key in required_keys if key not in batch]\n",
    "            logging.warning(f\"Skipping batch due to missing keys: {missing}\")\n",
    "            continue\n",
    "\n",
    "        # Move batch to device\n",
    "        try:\n",
    "            cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "            weather_seq = batch[\"weather_seq\"].to(device)       # (B, T, C_weather, H, W)\n",
    "            lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None # (B, T, C_lst, H, W) - T=1\n",
    "            time_emb_seq = batch[\"time_emb_seq\"].to(device)     # (B, T, C_time, H, W)\n",
    "            target = batch[\"target\"].to(device)               # (B, H, W)\n",
    "            mask = batch[\"mask\"].to(device)                   # (B, H, W)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error moving batch to device: {e}\")\n",
    "            continue # Skip batch if moving fails\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "            _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "            # 1. Encode static features ONCE\n",
    "            static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None # Get T=0 slice\n",
    "            # static_features shape: (B, proj_ch [+ C_lst], H', W')\n",
    "            with torch.no_grad(): # Ensure Clay backbone remains frozen\n",
    "                 # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                 static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "            _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "            # 2. Initialize hidden state\n",
    "            h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "            # 3. Resize dynamic features if needed\n",
    "            if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "            else:\n",
    "                weather_seq_resized = weather_seq\n",
    "            if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "            else:\n",
    "                time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "            # 4. Loop through time steps\n",
    "            for t in range(T):\n",
    "                weather_t = weather_seq_resized[:, t, :, :, :]      # (B, C_weather, H', W')\n",
    "                time_emb_t = time_emb_seq_resized[:, t, :, :, :]    # (B, C_time, H', W')\n",
    "                # Concatenate static + dynamic features\n",
    "                x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                # Update hidden state\n",
    "                h = model.step(x_t_combined, h)\n",
    "\n",
    "            # 5. Predict from final hidden state\n",
    "            prediction = model.predict(h) # (B, 1, H', W')\n",
    "            # --------------------------\n",
    "\n",
    "            # Resize prediction to target size if needed\n",
    "            if prediction.shape[2:] != target.shape[1:]:\n",
    "                 prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                 prediction_resized = prediction\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(prediction_resized.squeeze(1), target, mask)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                 logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                 continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"Runtime error during training: {e}\")\n",
    "             if \"out of memory\" in str(e):\n",
    "                 logging.error(\"CUDA out of memory. Try reducing batch size.\")\n",
    "                 # Consider breaking the loop or stopping training\n",
    "                 # break # Or raise e\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "             continue # Skip this batch\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Ensure all required keys are present\n",
    "            # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "            required_keys = ['cloudless_mosaic', 'weather_seq', 'time_emb_seq', 'target', 'mask']\n",
    "            if include_lst:\n",
    "                required_keys.append('lst_seq')\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                missing = [key for key in required_keys if key not in batch]\n",
    "                logging.warning(f\"Skipping validation batch due to missing keys: {missing}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                cloudless_mosaic = batch[\"cloudless_mosaic\"].to(device)\n",
    "                weather_seq = batch[\"weather_seq\"].to(device)\n",
    "                lst_seq = batch[\"lst_seq\"].to(device) if include_lst else None\n",
    "                time_emb_seq = batch[\"time_emb_seq\"].to(device)\n",
    "                target = batch[\"target\"].to(device)\n",
    "                mask = batch[\"mask\"].to(device)\n",
    "\n",
    "                # --- New Validation Logic ---\n",
    "                B, T, C_weather, H_in, W_in = weather_seq.shape\n",
    "                _, _, C_time, _, _ = time_emb_seq.shape\n",
    "\n",
    "                # 1. Encode static features ONCE\n",
    "                static_lst_map = lst_seq[:, 0, :, :, :] if include_lst and lst_seq is not None else None\n",
    "                # --- MODIFIED: Use 'cloudless_mosaic' key ---\n",
    "                static_features = model.encode_and_project_static(cloudless_mosaic, static_lst_map)\n",
    "                _, C_static, H_feat, W_feat = static_features.shape\n",
    "\n",
    "                # 2. Initialize hidden state\n",
    "                h = torch.zeros(B, model.gru_hidden_dim, H_feat, W_feat, device=device)\n",
    "\n",
    "                # 3. Resize dynamic features if needed\n",
    "                if weather_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    weather_seq_resized = F.interpolate(weather_seq.view(B*T, C_weather, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_weather, H_feat, W_feat)\n",
    "                else:\n",
    "                    weather_seq_resized = weather_seq\n",
    "                if time_emb_seq.shape[3:] != (H_feat, W_feat):\n",
    "                    time_emb_seq_resized = F.interpolate(time_emb_seq.view(B*T, C_time, H_in, W_in), size=(H_feat, W_feat), mode='bilinear', align_corners=False).view(B, T, C_time, H_feat, W_feat)\n",
    "                else:\n",
    "                    time_emb_seq_resized = time_emb_seq\n",
    "\n",
    "                # 4. Loop through time steps\n",
    "                for t in range(T):\n",
    "                    weather_t = weather_seq_resized[:, t, :, :, :]\n",
    "                    time_emb_t = time_emb_seq_resized[:, t, :, :, :]\n",
    "                    x_t_combined = torch.cat([static_features, weather_t, time_emb_t], dim=1)\n",
    "                    h = model.step(x_t_combined, h)\n",
    "\n",
    "                # 5. Predict from final hidden state\n",
    "                prediction = model.predict(h)\n",
    "                # --------------------------\n",
    "\n",
    "                # Resize prediction to target size if needed\n",
    "                if prediction.shape[2:] != target.shape[1:]:\n",
    "                    prediction_resized = F.interpolate(prediction, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "                else:\n",
    "                    prediction_resized = prediction\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(prediction_resized.squeeze(1), target, mask)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d733113-cb81-4972-bdff-5e1cd1a4d691",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab087292-f6cb-4c31-8db0-6ae9bb6223f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints and logs will be saved to: /home/jupyter/UHI/MLC-Project/training_runs/NYC_run_20250428_003814\n",
      "Saved configuration to config.json\n",
      "Starting training...\n",
      "--- Epoch 1/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.5585, Val Loss=0.2308\n",
      "New best loss: 0.2308\n",
      "Saved new best model to /home/jupyter/UHI/MLC-Project/training_runs/NYC_run_20250428_003814/model_best.pth.tar\n",
      "--- Epoch 2/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0951, Val Loss=0.0117\n",
      "New best loss: 0.0117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo improvement in validation loss for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs_no_improve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_val_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Save config with checkpoint\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m     \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_last.pth.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_best.pth.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Early stopping check\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epochs_no_improve \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m patience:\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best, filename, best_filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves model checkpoint.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    state (dict): Contains model's state_dict, optimizer state, epoch, etc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    best_filename (str): Path to save the best checkpoint.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m Path(filename)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Ensure dir exists\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_best:\n\u001b[1;32m     12\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfile(filename, best_filename)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:964\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    961\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    965\u001b[0m         _save(\n\u001b[1;32m    966\u001b[0m             obj,\n\u001b[1;32m    967\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:798\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Initialize Model ---\n",
    "model = UHINetCNN(\n",
    "    clay_checkpoint_path=str(absolute_clay_checkpoint_path),\n",
    "    clay_metadata_path=str(absolute_clay_metadata_path),\n",
    "    weather_channels=config[\"weather_channels\"],\n",
    "    time_embed_dim=config[\"time_embed_dim\"],\n",
    "    proj_ch=config[\"proj_ch\"],\n",
    "    clay_model_size=config[\"clay_model_size\"],\n",
    "    clay_bands=config[\"clay_bands\"],\n",
    "    clay_platform=config[\"clay_platform\"],\n",
    "    clay_gsd=config[\"clay_gsd\"],\n",
    "    lst_channels=config[\"lst_channels\"],\n",
    "    use_lst=config[\"include_lst\"],\n",
    "    cnn_hidden_dims=config[\"cnn_hidden_dims\"],\n",
    "    cnn_kernel_size=config[\"cnn_kernel_size\"]\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model UHINetCNN initialized on {device}\")\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "loss_fn = masked_mae_loss if config[\"loss_type\"] == \"mae\" else masked_mse_loss\n",
    "\n",
    "# --- Output Directory & Run Name ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{config['wander_run_name_prefix']}_{run_timestamp}\"\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"output_dir\"] = str(output_dir) # Update config with actual output dir\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# --- Initialize WANDB ---\n",
    "wandb.init(\n",
    "    project=config[\"wandb_project_name\"],\n",
    "    name=run_name,\n",
    "    config=config # Log the entire config dictionary\n",
    ")\n",
    "print(f\"Wandb initialized for run: {run_name}\")\n",
    "\n",
    "# Save configuration used for this run locally as well\n",
    "try:\n",
    "    # Convert Path objects in config to strings for JSON serialization\n",
    "    config_serializable = {k: str(v) if isinstance(v, Path) else v for k, v in config.items()}\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_serializable, f, indent=2)\n",
    "    print(\"Saved local configuration to config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local configuration: {e}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting CNN training...\")\n",
    "training_log = [] # Local log\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "    # Ensure train_epoch/validate_epoch return loss, rmse, r2\n",
    "    train_loss, train_rmse, train_r2 = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    log_metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_r2\": train_r2\n",
    "    }\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss, val_rmse, val_r2 = validate_epoch(model, val_loader, loss_fn, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} | Val Loss={val_loss:.4f} RMSE={val_rmse:.4f} R2={val_r2:.4f}\")\n",
    "        log_metrics.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"val_r2\": val_r2\n",
    "        })\n",
    "        current_loss = val_loss\n",
    "        if np.isnan(current_loss):\n",
    "             print(\"Warning: Validation loss is NaN. Stopping training.\")\n",
    "             break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} (No validation set)\")\n",
    "        current_loss = train_loss # Use train loss for checkpointing if no val set\n",
    "        if np.isnan(current_loss):\n",
    "             print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "             break\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log(log_metrics)\n",
    "    training_log.append(log_metrics) # Also keep local log\n",
    "\n",
    "    is_best = current_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = current_loss\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"New best loss: {best_val_loss:.4f}\")\n",
    "        wandb.run.summary[\"best_val_loss\"] = best_val_loss # Log best loss to summary\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    # Ensure config_dict passed to save_checkpoint is the serializable one if needed by the function\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_loss': best_val_loss,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config # Pass the original config potentially\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "    # Optional: Log checkpoint artifact to wandb\n",
    "    # if is_best:\n",
    "    #     best_model_artifact = wandb.Artifact(f'{run_name}-best_model', type='model')\n",
    "    #     best_model_artifact.add_file(output_dir / 'model_best.pth.tar')\n",
    "    #     wandb.log_artifact(best_model_artifact)\n",
    "\n",
    "    # Early stopping check\n",
    "    if epochs_no_improve >= config[\"patience\"]:\n",
    "        print(f\"Early stopping triggered after {config['patience']} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "# --- Save Local Training Log ---\n",
    "try:\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_df.to_csv(output_dir / 'training_log.csv', index=False)\n",
    "    print(f\"Saved local training log to {output_dir / 'training_log.csv'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local training log: {e}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best loss recorded: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved in: {output_dir}\")\n",
    "\n",
    "# --- Finish WANDB run ---\n",
    "wandb.finish()\n",
    "print(\"Wandb run finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9398d6-77ad-48c9-8ea0-cea7c00d691e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
