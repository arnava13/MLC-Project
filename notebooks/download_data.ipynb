{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64d05ec",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fad66",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94725a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "# Add the project root to the Python path to allow importing from src\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d213cf9-4623-420a-af80-3d7658873ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv\n",
      "Representative UHI date (from first row): 2021-07-24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration for Data Download ---\n",
    "\n",
    "# Configure parameters for the target city (e.g., NYC)\n",
    "city_name = \"NYC\"\n",
    "\n",
    "# Time window matching original notebooks (adjust if needed)\n",
    "sentinel_time_window = \"2021-06-01/2021-09-01\"\n",
    "lst_time_window = \"2021-06-01/2021-09-01\"\n",
    "\n",
    "# Input files and general settings\n",
    "data_dir = Path(\"data\")\n",
    "abs_output_dir = project_root / data_dir\n",
    "uhi_csv = data_dir / city_name / \"uhi.csv\" # Path to UHI data\n",
    "abs_uhi_csv = project_root / uhi_csv\n",
    "# bbox_csv is no longer needed for bounds calculation\n",
    "\n",
    "if not abs_uhi_csv.exists():\n",
    "    raise FileNotFoundError(f\"UHI data CSV not found at {abs_uhi_csv}. Cannot derive bounds.\")\n",
    "print(f\"Loading bounds from UHI data: {abs_uhi_csv}\")\n",
    "uhi_df = pd.read_csv(abs_uhi_csv)\n",
    "# Check if required columns exist\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "     raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "\n",
    "# Load bounds\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "\n",
    "# Load observation da\n",
    "first_datetime_obj = pd.to_datetime(uhi_df['datetime'].iloc[0], format='%d-%m-%Y %H:%M')\n",
    "# Format the date object into 'YYYY-MM-DD' string format\n",
    "uhi_date_str = first_datetime_obj.strftime('%Y-%m-%d')\n",
    "print(f\"Representative UHI date (from first row): {uhi_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af99245",
   "metadata": {},
   "source": [
    "## 2. Download Satellite Data for Cities\n",
    "\n",
    "Now we'll download satellite imagery data (Sentinel-2 median composites, Landsat LST medians) for specific cities and time periods derived from the UHI data timestamps. Data is saved locally for use by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7fbf75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv\n",
      "City: NYC\n",
      "Sentinel-2 Time Window: 2021-06-01/2021-09-01\n",
      "Sentinel-2 Cloud Cover Threshold: 30%\n",
      "LST Time Window: 2021-06-01/2021-09-01\n",
      "Bounds derived from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "Target mosaic output path: /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "Include LST: False\n"
     ]
    }
   ],
   "source": [
    "# Cell from notebooks/download_data.ipynb (modified)\n",
    "\n",
    "# Import functions\n",
    "from src.ingest.get_median import create_and_save_cloudless_mosaic\n",
    "# Import the modified LST download function\n",
    "from src.ingest.create_sat_tensor_files import download_single_lst_median\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Parameters for Cloudless Mosaic (matching Sentinel2_GeoTIFF.ipynb)\n",
    "mosaic_bands = [\"B02\", \"B03\", \"B04\", \"B08\"] # RGB+NIR for Clay compatibility\n",
    "mosaic_resolution_m = 10\n",
    "mosaic_cloud_cover = 30\n",
    "\n",
    "# Parameters for LST Median (matching Landsat_LST.ipynb)\n",
    "include_lst = False         # Whether to download LST\n",
    "lst_resolution_m = 30      # Native resolution for Landsat LST\n",
    "\n",
    "# Generate output path for the mosaic based on the new time window\n",
    "start_dt_str = sentinel_time_window.split('/')[0].replace('-','')\n",
    "end_dt_str = sentinel_time_window.split('/')[1].replace('-','')\n",
    "band_str = \"_\".join(mosaic_bands)\n",
    "cloudless_mosaic_filename = f\"sentinel_{city_name}_{start_dt_str}_to_{end_dt_str}_cloudless_mosaic.npy\"\n",
    "cloudless_mosaic_path = abs_output_dir / city_name / \"sat_files\" / cloudless_mosaic_filename\n",
    "# --- Verification ---\n",
    "print(f\"City: {city_name}\")\n",
    "print(f\"Sentinel-2 Time Window: {sentinel_time_window}\")\n",
    "print(f\"Sentinel-2 Cloud Cover Threshold: {mosaic_cloud_cover}%\")\n",
    "print(f\"LST Time Window: {lst_time_window}\")\n",
    "print(f\"Bounds derived from {uhi_csv.name}: {bounds}\")\n",
    "print(f\"Target mosaic output path: {cloudless_mosaic_path}\")\n",
    "print(f\"Include LST: {include_lst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4f9b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:00:41,109 - INFO - Cloudless mosaic /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy already exists. Skipping generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Cloudless Mosaic (2021-06-01/2021-09-01) ---\n",
      "Cloudless mosaic saved/found at: /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "\n",
      "--- Downloading Single LST Median (Include: False, Window: 2021-06-01/2021-09-01) ---\n",
      "Skipping LST median download as include_lst is False.\n",
      "\n",
      "Verifying output files:\n",
      "  Mosaic path (sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy) exists: True\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Generate Cloudless Mosaic --- \n",
    "print(f\"\\n--- Generating Cloudless Mosaic ({sentinel_time_window}) ---\")\n",
    "\n",
    "mosaic_output_path = create_and_save_cloudless_mosaic(\n",
    "    city_name=city_name,\n",
    "    bounds=bounds,\n",
    "    output_dir=abs_output_dir,\n",
    "    time_window=sentinel_time_window, # Use the explicit time window\n",
    "    selected_bands=mosaic_bands,\n",
    "    resolution_m=mosaic_resolution_m,\n",
    "    cloud_cover=mosaic_cloud_cover # Use the updated cloud cover\n",
    ")\n",
    "\n",
    "if mosaic_output_path:\n",
    "    print(f\"Cloudless mosaic saved/found at: {mosaic_output_path}\")\n",
    "else:\n",
    "    # Stop if mosaic fails, as it's required\n",
    "    raise RuntimeError(\"Failed to generate cloudless mosaic.\")\n",
    "\n",
    "# --- 2. Download Single LST Median (if enabled) ---\n",
    "print(f\"\\n--- Downloading Single LST Median (Include: {include_lst}, Window: {lst_time_window}) ---\")\n",
    "\n",
    "single_lst_median_file_path = None # Initialize path variable\n",
    "if include_lst:\n",
    "    # No need to check UHI CSV, we provide the time window directly\n",
    "    \n",
    "    # Download the single LST median using the explicit time window\n",
    "    single_lst_median_file_path = download_single_lst_median(\n",
    "        city_name=city_name,\n",
    "        bounds=bounds,\n",
    "        output_dir=abs_output_dir,\n",
    "        time_window=lst_time_window, # Provide explicit window\n",
    "        # uhi_csv_path and averaging_window are omitted/None\n",
    "        resolution_m=lst_resolution_m\n",
    "        # lst_cloud_cover is handled internally by load_lst_tensor_from_bbox_median\n",
    "    )\n",
    "\n",
    "    if single_lst_median_file_path:\n",
    "        print(f\"Single LST median saved/found at: {single_lst_median_file_path}\")\n",
    "    else:\n",
    "        print(\"Failed to generate single LST median.\")\n",
    "else:\n",
    "    print(\"Skipping LST median download as include_lst is False.\")\n",
    "\n",
    "# --- Verification ---\n",
    "sat_files_check_dir = Path(abs_output_dir) / city_name / \"sat_files\"\n",
    "print(f\"\\nVerifying output files:\")\n",
    "print(f\"  Mosaic path ({cloudless_mosaic_path.name}) exists: {cloudless_mosaic_path.exists()}\")\n",
    "if include_lst:\n",
    "    # Construct expected LST filename based on the explicit window\n",
    "    lst_start_str = lst_time_window.split('/')[0].replace('-','')\n",
    "    lst_end_str = lst_time_window.split('/')[1].replace('-','')\n",
    "    expected_lst_filename = f\"lst_{city_name}_median_{lst_start_str}_to_{lst_end_str}.npy\"\n",
    "    expected_lst_path = sat_files_check_dir / expected_lst_filename\n",
    "    print(f\"  Single LST median path ({expected_lst_filename}) exists: {expected_lst_path.exists()}\")\n",
    "    # Update the variable used by later cells if generation was successful\n",
    "    if single_lst_median_file_path and not single_lst_median_file_path.exists():\n",
    "         # This case shouldn't happen if the function worked, but good sanity check\n",
    "         print(f\"Warning: LST download function returned a path but it doesn't exist: {single_lst_median_file_path}\")\n",
    "         single_lst_median_file_path = None # Ensure later cells know it failed\n",
    "    elif not single_lst_median_file_path and expected_lst_path.exists():\n",
    "         # File existed previously, update path variable for later cells\n",
    "         single_lst_median_file_path = expected_lst_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244ca17",
   "metadata": {},
   "source": [
    "## 3. Download DEM/DSM Data (NYC)\n",
    "\n",
    "Download Digital Elevation Model (DEM) and Digital Surface Model (DSM) data for the city. These provide ground elevation and surface elevation (including buildings/trees) respectively.\n",
    "\n",
    "We will use the 1ft resolution data from NYC Open Data (derived from 2017 LiDAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Downloading DEM/DSM for {city_name} ---\")\n",
    "\n",
    "# --- URLs and Paths ---\n",
    "# DEM: Query the ArcGIS REST API to find the download link\n",
    "dem_api_query_url = \"https://elevation.its.ny.gov/arcgis/rest/services/Dem_Indexes/FeatureServer/0/query\"\n",
    "# DSM: Use the direct export link, but note it might fail (404 previously)\n",
    "# If it fails, the correct download needs to be found manually on the dataset page:\n",
    "# https://data.cityofnewyork.us/City-Government/1-foot-Digital-Surface-Model-DSM-2017-/btfm-ttmn\n",
    "dsm_direct_url = \"https://data.cityofnewyork.us/api/geospatial/btfm-ttmn?method=export&format=Original\" # NYC 1ft DSM (2017 LiDAR)\n",
    "\n",
    "sat_files_dir = abs_output_dir / city_name / \"sat_files\"\n",
    "sat_files_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dem_filename_zip = \"nyc_dem_1ft_2017.zip\" # Assuming API link points to a zip\n",
    "dsm_filename_zip = \"nyc_dsm_1ft_2017.zip\"\n",
    "\n",
    "dem_output_path_zip = sat_files_dir / dem_filename_zip\n",
    "dsm_output_path_zip = sat_files_dir / dsm_filename_zip\n",
    "\n",
    "# Define final expected TIF paths (adjust if filename inside zip is different)\n",
    "final_dem_path_tif = sat_files_dir / \"nyc_dem_1ft_2017.tif\"\n",
    "final_dsm_path_tif = sat_files_dir / \"nyc_dsm_1ft_2017.tif\"\n",
    "\n",
    "# --- Helper Function to Download ---\n",
    "def download_file(url, output_path):\n",
    "    if not url:\n",
    "        print(f\"Error: No URL provided for {output_path.name}.\")\n",
    "        return False\n",
    "    if output_path.exists():\n",
    "        print(f\"File {output_path.name} already exists. Skipping download.\")\n",
    "        return True\n",
    "    try:\n",
    "        print(f\"Downloading {output_path.name} from {url}...\")\n",
    "        response = requests.get(url, stream=True, timeout=60) # Added timeout\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192 # Increased block size\n",
    "\n",
    "        with open(output_path, 'wb') as f, tqdm(\n",
    "            desc=output_path.name,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        print(f\"Successfully downloaded {output_path.name}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {output_path.name}: {e}\")\n",
    "        if output_path.exists(): # Clean up partial download\n",
    "            os.remove(output_path)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download of {output_path.name}: {e}\")\n",
    "        if output_path.exists(): # Clean up partial download\n",
    "             os.remove(output_path)\n",
    "        return False\n",
    "\n",
    "# --- Helper Function to Unzip ---\n",
    "def unzip_file(zip_path, extract_dir):\n",
    "    if not zip_path.exists():\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        return False\n",
    "    # Check if the final TIF already exists to avoid unnecessary unzipping\n",
    "    expected_tif_name = zip_path.stem + \".tif\" # e.g., nyc_dem_1ft_2017.tif\n",
    "    expected_tif_path = extract_dir / expected_tif_name\n",
    "    if expected_tif_path.exists():\n",
    "        print(f\"Expected TIF file {expected_tif_path.name} already exists. Skipping unzip.\")\n",
    "        return True\n",
    "        \n",
    "    try:\n",
    "        print(f\"Unzipping {zip_path.name} to {extract_dir}...\")\n",
    "        # Use -o to overwrite existing files without prompting\n",
    "        result = subprocess.run(['unzip', '-o', str(zip_path), '-d', str(extract_dir)],\n",
    "                                capture_output=True, text=True, check=True, timeout=300) # Added timeout\n",
    "        print(f\"Successfully unzipped {zip_path.name}\")\n",
    "        \n",
    "        # Verify the expected TIF file exists after unzipping\n",
    "        if not expected_tif_path.exists():\n",
    "             print(f\"Warning: Expected TIF file {expected_tif_path.name} not found directly in {extract_dir} after unzipping. Check subdirectories or zip contents.\")\n",
    "             # Look for any .tif file as a fallback check\n",
    "             tif_files = list(extract_dir.glob('*.tif')) + list(extract_dir.glob('*/*.tif'))\n",
    "             if tif_files:\n",
    "                 print(f\"  Found other TIF files: {[f.name for f in tif_files]}. Manual check might be needed.\")\n",
    "             else:\n",
    "                 print(f\"  No .tif files found in {extract_dir} or immediate subdirectories.\")\n",
    "             return False # Consider it failed if the *specific* expected file isn't there\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'unzip' command not found. Please install it.\")\n",
    "        return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Error: Unzipping {zip_path.name} timed out.\")\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error unzipping {zip_path.name}: {e}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during unzipping of {zip_path.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Download DEM ---\n",
    "dem_download_url = None\n",
    "try:\n",
    "    print(f\"Querying DEM API: {dem_api_query_url}\")\n",
    "    params = {'where': '1=1', 'outFields': 'DIRECT_DL', 'f': 'json'}\n",
    "    api_response = requests.get(dem_api_query_url, params=params, timeout=30)\n",
    "    api_response.raise_for_status()\n",
    "    api_data = api_response.json()\n",
    "    \n",
    "    if 'features' in api_data and len(api_data['features']) > 0:\n",
    "        # Assume the first feature's link is sufficient or representative\n",
    "        dem_download_url = api_data['features'][0].get('attributes', {}).get('DIRECT_DL')\n",
    "        if dem_download_url:\n",
    "            print(f\"Found DEM download URL via API: {dem_download_url}\")\n",
    "        else:\n",
    "            print(\"Error: Found features in DEM API response, but no 'DIRECT_DL' attribute.\")\n",
    "    else:\n",
    "        print(\"Error: No features found in DEM API query response.\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error querying DEM API: {e}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: Could not decode JSON response from DEM API.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during DEM API query: {e}\")\n",
    "\n",
    "dem_downloaded = False\n",
    "dem_unzipped = False\n",
    "if dem_download_url:\n",
    "    dem_downloaded = download_file(dem_download_url, dem_output_path_zip)\n",
    "    if dem_downloaded:\n",
    "        dem_unzipped = unzip_file(dem_output_path_zip, sat_files_dir)\n",
    "else:\n",
    "    print(\"Skipping DEM download and unzip as URL could not be retrieved from API.\")\n",
    "\n",
    "# --- Download DSM ---\n",
    "print(\"\\nAttempting direct download for DSM (Note: Link may be broken, check dataset page if fails)\")\n",
    "dsm_downloaded = download_file(dsm_direct_url, dsm_output_path_zip)\n",
    "dsm_unzipped = False\n",
    "if dsm_downloaded:\n",
    "    dsm_unzipped = unzip_file(dsm_output_path_zip, sat_files_dir)\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\nVerifying final DEM/DSM TIF files:\")\n",
    "print(f\"  Expected DEM TIF: {final_dem_path_tif} -> Exists: {final_dem_path_tif.exists()}\")\n",
    "print(f\"  Expected DSM TIF: {final_dsm_path_tif} -> Exists: {final_dsm_path_tif.exists()}\")\n",
    "\n",
    "# Store paths for potential use in later cells or for the training notebook config\n",
    "config_dem_path_relative = Path(\"data\") / city_name / \"sat_files\" / final_dem_path_tif.name\n",
    "config_dsm_path_relative = Path(\"data\") / city_name / \"sat_files\" / final_dsm_path_tif.name\n",
    "print(f\"\\nRelative paths for config:\")\n",
    "print(f\"  DEM: {config_dem_path_relative}\")\n",
    "print(f\"  DSM: {config_dsm_path_relative}\")\n",
    "\n",
    "# Optional: Clean up zip files after successful unzip and verification\n",
    "if dem_unzipped and final_dem_path_tif.exists() and dem_output_path_zip.exists():\n",
    "    print(f\"Cleaning up {dem_output_path_zip.name}...\")\n",
    "    # os.remove(dem_output_path_zip)\n",
    "if dsm_unzipped and final_dsm_path_tif.exists() and dsm_output_path_zip.exists():\n",
    "     print(f\"Cleaning up {dsm_output_path_zip.name}...\")\n",
    "     # os.remove(dsm_output_path_zip) # Uncomment to enable cleanup\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
