{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0bd36b",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3faad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d05ec",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Download and process UHI GeoTIFF files from sources like Fort Lauderdale (FTL) and convert them to CSV format\n",
    "2. Download satellite imagery for specific cities and time periods and save them locally\n",
    "3. Use local satellite data files with the dataloader instead of direct API calls\n",
    "\n",
    "The pipeline is designed to work with different city datasets with the same structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fad66",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Add the project root to the Python path to allow importing from src\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af99245",
   "metadata": {},
   "source": [
    "## 2. Download Satellite Data for Cities\n",
    "\n",
    "Now we'll download satellite imagery data (Sentinel-2 median composites, Landsat LST medians) for specific cities and time periods derived from the UHI data timestamps. Data is saved locally for use by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from src.ingest.get_median import create_and_save_cloudless_mosaic\n",
    "# Import the modified LST download function\n",
    "from src.ingest.create_sat_tensor_files import download_single_lst_median \n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration for Data Download ---\n",
    "\n",
    "# Configure parameters for the target city (e.g., NYC)\n",
    "city_name = \"NYC\"\n",
    "\n",
    "# Time window matching original notebooks\n",
    "sentinel_time_window = \"2021-06-01/2021-09-01\"\n",
    "lst_time_window = \"2021-06-01/2021-09-01\"\n",
    "\n",
    "# Input files and general settings\n",
    "data_dir = Path(\"data\")\n",
    "abs_output_dir = project_root / data_dir\n",
    "# UHI CSV is now only needed for verification/context, not LST window calculation\n",
    "uhi_csv = data_dir / city_name / \"uhi_data.csv\"\n",
    "abs_uhi_csv = project_root / uhi_csv \n",
    "bbox_csv = data_dir / city_name / \"bbox.csv\"\n",
    "abs_bbox_csv = project_root / bbox_csv\n",
    "\n",
    "# Load bounds from bbox.csv for mosaic generation\n",
    "if not abs_bbox_csv.exists():\n",
    "    raise FileNotFoundError(f\"bbox.csv not found at {abs_bbox_csv}\")\n",
    "bbox_df = pd.read_csv(abs_bbox_csv)\n",
    "bounds = [\n",
    "    bbox_df['longitudes'].min(),\n",
    "    bbox_df['latitudes'].min(),\n",
    "    bbox_df['longitudes'].max(),\n",
    "    bbox_df['latitudes'].max()\n",
    "]\n",
    "\n",
    "# Parameters for Cloudless Mosaic (matching Sentinel2_GeoTIFF.ipynb)\n",
    "mosaic_bands = [\"B02\", \"B03\", \"B04\", \"B08\"] # Still using RGB+NIR for Clay\n",
    "mosaic_resolution_m = 10\n",
    "mosaic_cloud_cover = 30 # Changed from 5 to 30\n",
    "\n",
    "# Parameters for LST Median (matching Landsat_LST.ipynb)\n",
    "include_lst = True         # Whether to download LST\n",
    "lst_resolution_m = 30      # Native resolution for Landsat LST\n",
    "# LST cloud cover fixed at 50 in load_lst_tensor_from_bbox_median\n",
    "\n",
    "# Generate output path for the mosaic based on the new time window\n",
    "start_dt_str = sentinel_time_window.split('/')[0].replace('-','')\n",
    "end_dt_str = sentinel_time_window.split('/')[1].replace('-','')\n",
    "cloudless_mosaic_filename = f\"sentinel_{city_name}_{start_dt_str}_to_{end_dt_str}_cloudless_mosaic.npy\"\n",
    "cloudless_mosaic_path = abs_output_dir / city_name / \"sat_files\" / cloudless_mosaic_filename\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"City: {city_name}\")\n",
    "print(f\"Sentinel-2 Time Window: {sentinel_time_window}\")\n",
    "print(f\"Sentinel-2 Cloud Cover Threshold: {mosaic_cloud_cover}%\")\n",
    "print(f\"LST Time Window: {lst_time_window}\")\n",
    "print(f\"Bounds from {bbox_csv.name}: {bounds}\")\n",
    "print(f\"Checking if UHI CSV exists (for context): {abs_uhi_csv.exists()}\")\n",
    "print(f\"Target mosaic output path: {cloudless_mosaic_path}\")\n",
    "print(f\"Include LST: {include_lst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generate Cloudless Mosaic --- \n",
    "print(f\"\\n--- Generating Cloudless Mosaic ({sentinel_time_window}) ---\")\n",
    "\n",
    "mosaic_output_path = create_and_save_cloudless_mosaic(\n",
    "    city_name=city_name,\n",
    "    bounds=bounds,\n",
    "    output_dir=abs_output_dir,\n",
    "    time_window=sentinel_time_window, # Use the explicit time window\n",
    "    selected_bands=mosaic_bands,\n",
    "    resolution_m=mosaic_resolution_m,\n",
    "    cloud_cover=mosaic_cloud_cover # Use the updated cloud cover\n",
    ")\n",
    "\n",
    "if mosaic_output_path:\n",
    "    print(f\"Cloudless mosaic saved/found at: {mosaic_output_path}\")\n",
    "else:\n",
    "    # Stop if mosaic fails, as it's required\n",
    "    raise RuntimeError(\"Failed to generate cloudless mosaic.\")\n",
    "\n",
    "# --- 2. Download Single LST Median (if enabled) ---\n",
    "print(f\"\\n--- Downloading Single LST Median (Include: {include_lst}, Window: {lst_time_window}) ---\")\n",
    "\n",
    "single_lst_median_file_path = None # Initialize path variable\n",
    "if include_lst:\n",
    "    # No need to check UHI CSV, we provide the time window directly\n",
    "    \n",
    "    # Download the single LST median using the explicit time window\n",
    "    single_lst_median_file_path = download_single_lst_median(\n",
    "        city_name=city_name,\n",
    "        bounds=bounds,\n",
    "        output_dir=abs_output_dir,\n",
    "        time_window=lst_time_window, # Provide explicit window\n",
    "        # uhi_csv_path and averaging_window are omitted/None\n",
    "        resolution_m=lst_resolution_m\n",
    "        # lst_cloud_cover is handled internally by load_lst_tensor_from_bbox_median\n",
    "    )\n",
    "\n",
    "    if single_lst_median_file_path:\n",
    "        print(f\"Single LST median saved/found at: {single_lst_median_file_path}\")\n",
    "    else:\n",
    "        print(\"Failed to generate single LST median.\")\n",
    "else:\n",
    "    print(\"Skipping LST median download as include_lst is False.\")\n",
    "\n",
    "# --- Verification ---\n",
    "sat_files_check_dir = Path(abs_output_dir) / city_name / \"sat_files\"\n",
    "print(f\"\\nVerifying output files:\")\n",
    "print(f\"  Mosaic path ({cloudless_mosaic_path.name}) exists: {cloudless_mosaic_path.exists()}\")\n",
    "if include_lst:\n",
    "    # Construct expected LST filename based on the explicit window\n",
    "    lst_start_str = lst_time_window.split('/')[0].replace('-','')\n",
    "    lst_end_str = lst_time_window.split('/')[1].replace('-','')\n",
    "    expected_lst_filename = f\"lst_{city_name}_median_{lst_start_str}_to_{lst_end_str}.npy\"\n",
    "    expected_lst_path = sat_files_check_dir / expected_lst_filename\n",
    "    print(f\"  Single LST median path ({expected_lst_filename}) exists: {expected_lst_path.exists()}\")\n",
    "    # Update the variable used by later cells if generation was successful\n",
    "    if single_lst_median_file_path and not single_lst_median_file_path.exists():\n",
    "         # This case shouldn't happen if the function worked, but good sanity check\n",
    "         print(f\"Warning: LST download function returned a path but it doesn't exist: {single_lst_median_file_path}\")\n",
    "         single_lst_median_file_path = None # Ensure later cells know it failed\n",
    "    elif not single_lst_median_file_path and expected_lst_path.exists():\n",
    "         # File existed previously, update path variable for later cells\n",
    "         single_lst_median_file_path = expected_lst_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2dff",
   "metadata": {},
   "source": [
    "## 4. Using Local Satellite Data with the Dataloader\n",
    "\n",
    "Finally, we'll demonstrate how to use the modified dataloader that works with local satellite data files instead of making API calls directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c990db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the updated data loader\n",
    "from src.ingest.dataloader import CityDataSet\n",
    "\n",
    "# --- Parameters for Dataloader ---\n",
    "# Most parameters are now defined in Cell 6 (config cell)\n",
    "\n",
    "# Paths relative to project root\n",
    "uhi_csv_rel = f\"data/{city_name}/uhi_data.csv\"\n",
    "bbox_csv_rel = f\"data/{city_name}/bbox.csv\"\n",
    "weather_csv_rel = f\"data/{city_name}/weather_grid.csv\"\n",
    "\n",
    "# Construct absolute paths\n",
    "abs_uhi_csv = project_root / uhi_csv_rel\n",
    "abs_bbox_csv = project_root / bbox_csv_rel\n",
    "abs_weather_csv = project_root / weather_csv_rel\n",
    "abs_data_dir = project_root / data_dir # data_dir defined in Cell 6\n",
    "\n",
    "# Path to the cloudless mosaic generated earlier (defined in Cell 6)\n",
    "# Path to the single LST median generated earlier (defined in Cell 7, may be None)\n",
    "# single_lst_median_file_path\n",
    "\n",
    "# Required files check\n",
    "required_files_exist = all([\n",
    "    abs_uhi_csv.exists(),\n",
    "    abs_bbox_csv.exists(),\n",
    "    abs_weather_csv.exists(),\n",
    "    cloudless_mosaic_path.exists(),\n",
    "    # Check LST file only if it was supposed to be generated\n",
    "    (single_lst_median_file_path.exists() if include_lst and single_lst_median_file_path else True)\n",
    "])\n",
    "\n",
    "print(f\"Target Resolution for Grids: {mosaic_resolution_m}m\") # Use mosaic resolution for consistency here\n",
    "print(f\"UHI CSV exists: {abs_uhi_csv.exists()}\")\n",
    "print(f\"Bbox CSV exists: {abs_bbox_csv.exists()}\")\n",
    "print(f\"Weather CSV exists: {abs_weather_csv.exists()}\")\n",
    "print(f\"Cloudless Mosaic exists: {cloudless_mosaic_path.exists()}\")\n",
    "if include_lst:\n",
    "    lst_exists = single_lst_median_file_path.exists() if single_lst_median_file_path else False\n",
    "    print(f\"Single LST Median (needed={include_lst}) exists: {lst_exists}\")\n",
    "print(f\"All required files exist: {required_files_exist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset if all required files exist\n",
    "if required_files_exist:\n",
    "    try:\n",
    "        # Parameters from config cell (Cell 6)\n",
    "        target_resolution_m = mosaic_resolution_m # Use mosaic resolution\n",
    "        print(f\"\\nInitializing dataset with target resolution: {target_resolution_m}m\")\n",
    "\n",
    "        # Pass the path to the single LST median file if it exists\n",
    "        lst_path_arg = str(single_lst_median_file_path) if include_lst and single_lst_median_file_path else None\n",
    "\n",
    "        dataset = CityDataSet(\n",
    "            bounds=bounds,\n",
    "            averaging_window=averaging_window_lst, # Still needed by constructor, though not used for LST if path provided\n",
    "            resolution_m=target_resolution_m,\n",
    "            uhi_csv=abs_uhi_csv,\n",
    "            bbox_csv=abs_bbox_csv,\n",
    "            weather_csv=abs_weather_csv,\n",
    "            cloudless_mosaic_path=str(cloudless_mosaic_path),\n",
    "            data_dir=abs_data_dir,\n",
    "            city_name=city_name,\n",
    "            include_lst=include_lst,\n",
    "            single_lst_median_path=lst_path_arg\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSuccessfully initialized dataset with {len(dataset)} samples. LST included: {dataset.include_lst}\")\n",
    "\n",
    "        # --- Inspect First Sample ---\n",
    "        if len(dataset) > 0:\n",
    "            first_sample = dataset[0]\n",
    "            print(\"\\nSample keys:\", list(first_sample.keys()))\n",
    "\n",
    "            print(\"\\nTensor shapes in first sample:\")\n",
    "            for key, tensor in first_sample.items():\n",
    "                # Only print shape, handle potential non-tensor items gracefully if any added later\n",
    "                if hasattr(tensor, 'shape'):\n",
    "                     print(f\"  {key}: {tensor.shape} (dtype: {tensor.dtype})\")\n",
    "                else:\n",
    "                     print(f\"  {key}: {type(tensor)}\")\n",
    "\n",
    "            # Plot the cloudless mosaic (RGB)\n",
    "            mosaic_tensor = first_sample['cloudless_mosaic']\n",
    "            if mosaic_tensor.shape[0] >= 3:\n",
    "                rgb_indices = []\n",
    "                required = [\"B04\", \"B03\", \"B02\"]\n",
    "                missing = []\n",
    "                for band in required:\n",
    "                     try: rgb_indices.append(mosaic_bands.index(band))\n",
    "                     except ValueError: missing.append(band)\n",
    "\n",
    "                if not missing:\n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    rgb = mosaic_tensor[rgb_indices, :, :]\n",
    "                    rgb = np.transpose(rgb, (1, 2, 0))\n",
    "                    min_val, max_val = rgb.min(), rgb.max()\n",
    "                    if max_val > min_val: rgb = (rgb - min_val) / (max_val - min_val)\n",
    "                    else: rgb = np.zeros_like(rgb)\n",
    "                    plt.imshow(rgb)\n",
    "                    plt.title(f\"Cloudless Mosaic RGB ({city_name}, {mosaic_year})\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(f\"\\nCannot display RGB composite: Missing bands {missing} in mosaic_bands {mosaic_bands}\")\n",
    "\n",
    "            # Plot the target UHI grid for the first sample\n",
    "            target_uhi = first_sample['target']\n",
    "            uhi_mask = first_sample['mask']\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            # Handle cases where target might be all NaN after masking/grouping\n",
    "            valid_uhi = target_uhi[uhi_mask > 0.5] # Use mask to select valid points\n",
    "            vmin = np.nanmin(valid_uhi) if valid_uhi.size > 0 else 0\n",
    "            vmax = np.nanmax(valid_uhi) if valid_uhi.size > 0 else 1\n",
    "            plt.imshow(target_uhi, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            plt.colorbar(label='UHI Index')\n",
    "            plt.title(f\"Target UHI Grid (Sample 0) - Masked areas are NaN/White\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError initializing or inspecting dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nCannot initialize dataset: Not all required files were found. Please run previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d2960",
   "metadata": {},
   "source": [
    "## 5. Adding a New City to the Pipeline\n",
    "\n",
    "Here's how to add a new city to the data pipeline:\n",
    "\n",
    "1. Prepare the UHI data CSV (lat;long;uhi) and bounding box CSV\n",
    "2. Create a directory structure: `data/CITY_NAME/`\n",
    "3. Run the satellite data download process for the new city\n",
    "4. Use the local dataloader with the new city's satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for adding a new city (commented out - template for reference)\n",
    "\"\"\"\n",
    "# Step 1: Set up directory structure\n",
    "new_city = \"MIAMI\"\n",
    "os.makedirs(f\"data/{new_city}\", exist_ok=True)\n",
    "\n",
    "# Step 2: If you have UHI GeoTIFFs, convert them to CSV\n",
    "geotiff_dir = f\"data/UHI_Surfaces_{new_city}\"\n",
    "if os.path.exists(geotiff_dir):\n",
    "    process_uhi_directories(\n",
    "        input_dirs=[geotiff_dir],\n",
    "        output_dir=\"data\"\n",
    "    )\n",
    "\n",
    "# Step 3: Prepare parameters\n",
    "city_bounds = [-80.32, 25.70, -80.12, 25.90]  # Example for Miami [min_lon, min_lat, max_lon, max_lat]\n",
    "uhi_csv = f\"data/{new_city}/uhi_data.csv\"\n",
    "bbox_csv = f\"data/{new_city}/bbox.csv\"\n",
    "weather_csv = f\"data/{new_city}/weather_grid.csv\"\n",
    "\n",
    "# Step 4: Download satellite data\n",
    "download_data_from_uhi_csv(\n",
    "    city_name=new_city,\n",
    "    uhi_csv=uhi_csv,\n",
    "    bbox_csv=bbox_csv,\n",
    "    averaging_window=30,\n",
    "    output_dir=\"data\",\n",
    "    selected_bands=[\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    resolution_m=10,\n",
    "    include_lst=True\n",
    ")\n",
    "\n",
    "# Step 5: Initialize the dataset with local data\n",
    "dataset = CityDataSet(\n",
    "    bounds=city_bounds,\n",
    "    averaging_window=30,\n",
    "    selected_bands=[\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    resolution_m=10,\n",
    "    include_lst=True,\n",
    "    uhi_csv=uhi_csv,\n",
    "    bbox_csv=bbox_csv,\n",
    "    weather_csv=weather_csv,\n",
    "    data_dir=\"data\",\n",
    "    city_name=new_city\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cafb44",
   "metadata": {},
   "source": [
    "## 3a. Download Weather Grid Data for the City\n",
    "\n",
    "Next, we download daily weather data (max/min temperature, precipitation) for a grid covering the city's bounding box. This data is used by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8748a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Function to fetch weather data from Open-Meteo\n",
    "def get_openmeteo_weather(lat, lon, start_date, end_date):\n",
    "    url = (\n",
    "        \"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "        f\"latitude={lat}&longitude={lon}\"\n",
    "        f\"&start_date={start_date}&end_date={end_date}\"\n",
    "        \"&daily=temperature_2m_max,temperature_2m_min,precipitation_sum\"\n",
    "        \"&timezone=America/New_York\" # Consider making timezone configurable if needed\n",
    "    )\n",
    "    try:\n",
    "        res = requests.get(url, timeout=30) # Added timeout\n",
    "        res.raise_for_status() # Raise HTTPError for bad responses (4XX, 5XX)\n",
    "        return res.json()[\"daily\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"API request failed for ({lat},{lon}): {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed processing weather for ({lat},{lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Weather Configuration ---\n",
    "# Use the city_name and bbox_csv defined in the previous cell (Cell 9)\n",
    "weather_city_name = city_name\n",
    "weather_bbox_csv = project_root / bbox_csv # Use the relative path defined earlier\n",
    "\n",
    "# Define grid interval and date range\n",
    "grid_interval = 0.01 # Degrees (~1km) - adjust if needed\n",
    "start_date = \"2021-06-01\" # Match the example script - adjust if needed\n",
    "end_date = \"2021-09-01\"   # Match the example script - adjust if needed\n",
    "\n",
    "# Output path for weather data\n",
    "weather_output_dir = project_root / \"data\" / weather_city_name\n",
    "weather_output_file = weather_output_dir / \"weather_grid.csv\"\n",
    "\n",
    "# --- Weather Data Download ---\n",
    "print(f\"\\nStarting weather data download for {weather_city_name}...\")\n",
    "print(f\"Using bbox file: {weather_bbox_csv}\")\n",
    "print(f\"Output file: {weather_output_file}\")\n",
    "\n",
    "if not os.path.exists(weather_bbox_csv):\n",
    "    print(f\"Error: Bounding box file not found at {weather_bbox_csv}. Skipping weather download.\")\n",
    "else:\n",
    "    # Read bounding box\n",
    "    bbox_df = pd.read_csv(weather_bbox_csv)\n",
    "    min_lat, max_lat = bbox_df['latitudes'].min(), bbox_df['latitudes'].max()\n",
    "    min_lon, max_lon = bbox_df['longitudes'].min(), bbox_df['longitudes'].max()\n",
    "\n",
    "    # Create grid points, ensuring ranges cover the max values\n",
    "    lats = np.arange(min_lat, max_lat + grid_interval, grid_interval)\n",
    "    lons = np.arange(min_lon, max_lon + grid_interval, grid_interval)\n",
    "    grid_points = [(round(lat, 4), round(lon, 4)) for lat in lats for lon in lons] # Rounded for precision\n",
    "    print(f\"Generated {len(grid_points)} grid points for weather data.\")\n",
    "\n",
    "    # Get weather data for each grid point\n",
    "    weather_records = []\n",
    "    successful_fetches = 0\n",
    "    for i, (lat, lon) in enumerate(grid_points):\n",
    "        if (i + 1) % 50 == 0: # Log progress every 50 points\n",
    "             print(f\"  Fetching weather for point {i+1}/{len(grid_points)} ({lat}, {lon})...\")\n",
    "             \n",
    "        daily_data = get_openmeteo_weather(lat, lon, start_date, end_date)\n",
    "        if daily_data and 'time' in daily_data: # Check if data was fetched successfully\n",
    "            successful_fetches += 1\n",
    "            for idx, date in enumerate(daily_data[\"time\"]):\n",
    "                weather_records.append({\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                    \"date\": date,\n",
    "                    \"temp_max\": daily_data[\"temperature_2m_max\"][idx],\n",
    "                    \"temp_min\": daily_data[\"temperature_2m_min\"][idx],\n",
    "                    \"precip\": daily_data[\"precipitation_sum\"][idx],\n",
    "                })\n",
    "        # Optional: Add a small delay between requests if needed\n",
    "        # import time\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "    print(f\"Successfully fetched data for {successful_fetches} out of {len(grid_points)} grid points.\")\n",
    "\n",
    "    if not weather_records:\n",
    "        print(\"No weather data was successfully downloaded. CSV file not created.\")\n",
    "    else:\n",
    "        # Save the weather data to a CSV file\n",
    "        df_weather = pd.DataFrame(weather_records)\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(weather_output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Saving weather data with {len(df_weather)} records to {weather_output_file}...\")\n",
    "        df_weather.to_csv(weather_output_file, index=False, float_format='%.4f')\n",
    "        print(f\"Weather data saved successfully.\")\n",
    "\n",
    "print(\"\\nWeather data download process finished.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
