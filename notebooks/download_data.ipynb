{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64d05ec",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fad66",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94725a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import zipfile # For more robust unzipping\n",
    "import time\n",
    "\n",
    "# Imports for tile index processing\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Add the project root to the Python path to allow importing from src\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d213cf9-4623-420a-af80-3d7658873ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /Users/arnav/MLC-Project/data/NYC/uhi.csv\n",
      "Representative UHI date (from first row): 2021-07-24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration for Data Download ---\n",
    "\n",
    "# Configure parameters for the target city (e.g., NYC)\n",
    "city_name = \"NYC\"\n",
    "\n",
    "# Time window matching original notebooks (adjust if needed)\n",
    "sentinel_time_window = \"2021-06-01/2021-09-01\"\n",
    "lst_time_window = \"2021-06-01/2021-09-01\"\n",
    "\n",
    "# Input files and general settings\n",
    "data_dir = Path(\"data\")\n",
    "abs_output_dir = project_root / data_dir\n",
    "uhi_csv = data_dir / city_name / \"uhi.csv\" # Path to UHI data\n",
    "abs_uhi_csv = project_root / uhi_csv\n",
    "# bbox_csv is no longer needed for bounds calculation\n",
    "\n",
    "if not abs_uhi_csv.exists():\n",
    "    raise FileNotFoundError(f\"UHI data CSV not found at {abs_uhi_csv}. Cannot derive bounds.\")\n",
    "print(f\"Loading bounds from UHI data: {abs_uhi_csv}\")\n",
    "uhi_df = pd.read_csv(abs_uhi_csv)\n",
    "# Check if required columns exist\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "     raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "\n",
    "# Load bounds\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "\n",
    "# Load observation da\n",
    "first_datetime_obj = pd.to_datetime(uhi_df['datetime'].iloc[0], format='%d-%m-%Y %H:%M')\n",
    "# Format the date object into 'YYYY-MM-DD' string format\n",
    "uhi_date_str = first_datetime_obj.strftime('%Y-%m-%d')\n",
    "print(f\"Representative UHI date (from first row): {uhi_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af99245",
   "metadata": {},
   "source": [
    "## 2. Download Satellite Data for Cities\n",
    "\n",
    "Now we'll download satellite imagery data (Sentinel-2 median composites, Landsat LST medians) for specific cities and time periods derived from the UHI data timestamps. Data is saved locally for use by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be014273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv\n",
      "City: NYC\n",
      "Sentinel-2 Time Window: 2021-06-01/2021-09-01\n",
      "Sentinel-2 Cloud Cover Threshold: 30%\n",
      "LST Time Window: 2021-06-01/2021-09-01\n",
      "Bounds derived from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "Target mosaic output path: /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "Include LST: False\n"
     ]
    }
   ],
   "source": [
    "# Cell from notebooks/download_data.ipynb (modified)\n",
    "\n",
    "# Import functions\n",
    "from src.ingest.get_median import create_and_save_cloudless_mosaic\n",
    "# Import the modified LST download function\n",
    "from src.ingest.create_sat_tensor_files import download_single_lst_median\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Parameters for Cloudless Mosaic (matching Sentinel2_GeoTIFF.ipynb)\n",
    "mosaic_bands = [\"B02\", \"B03\", \"B04\", \"B08\"] # RGB+NIR for Clay compatibility\n",
    "mosaic_resolution_m = 10\n",
    "mosaic_cloud_cover = 30\n",
    "\n",
    "# Parameters for LST Median (matching Landsat_LST.ipynb)\n",
    "include_lst = False         # Whether to download LST\n",
    "lst_resolution_m = 30      # Native resolution for Landsat LST\n",
    "\n",
    "# Generate output path for the mosaic based on the new time window\n",
    "start_dt_str = sentinel_time_window.split('/')[0].replace('-','')\n",
    "end_dt_str = sentinel_time_window.split('/')[1].replace('-','')\n",
    "band_str = \"_\".join(mosaic_bands)\n",
    "cloudless_mosaic_filename = f\"sentinel_{city_name}_{start_dt_str}_to_{end_dt_str}_cloudless_mosaic.npy\"\n",
    "cloudless_mosaic_path = abs_output_dir / city_name / \"sat_files\" / cloudless_mosaic_filename\n",
    "# --- Verification ---\n",
    "print(f\"City: {city_name}\")\n",
    "print(f\"Sentinel-2 Time Window: {sentinel_time_window}\")\n",
    "print(f\"Sentinel-2 Cloud Cover Threshold: {mosaic_cloud_cover}%\")\n",
    "print(f\"LST Time Window: {lst_time_window}\")\n",
    "print(f\"Bounds derived from {uhi_csv.name}: {bounds}\")\n",
    "print(f\"Target mosaic output path: {cloudless_mosaic_path}\")\n",
    "print(f\"Include LST: {include_lst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244ca17",
   "metadata": {},
   "source": [
    "### 3. Download + Build DEM / DSM for NYC (AOI‑bounded, 1 ft grid)\n",
    "\n",
    "1. **Tile‑index fetch**  \n",
    "   * Pull both tile‑index shapefiles (DEM & LAS) from the NYS topo‑bathymetric 2017 FTP mirror.  \n",
    "   * They live in `/BE_DEM/…zip` and `/LAS/…zip`.  \n",
    "   * They’re unzipped to a temp folder, read by GeoPandas, then deleted.\n",
    "\n",
    "2. **Intersect AOI**  \n",
    "   * Your AOI bounds (given in decimal lat/long) are re‑projected to EPSG 2263.  \n",
    "   * We select only the DEM `.tif` tiles and LAS `.laz` tiles whose polygons hit that AOI.\n",
    "\n",
    "3. **Download source tiles**  \n",
    "   * DEM tiles (`be_NYC_###.tif`) stream straight from FTP to `dem_tiles/`.  \n",
    "   * LAS tiles (`hh_NYC_###.laz`) stream to `las_tiles/`.  \n",
    "   * Progress bars show raw byte count; no FTP `SIZE` calls (the server blocks those).\n",
    "\n",
    "4. **Build rasters**  \n",
    "   * **DEM** – `gdal_merge.py` mosaics the few BE tiles → `dem_merged_epsg2263.tif`.  \n",
    "   * **DSM** – PDAL crops the LAS hits, bins highest return (`output_type=max`), writes `dsm_epsg2263.tif` at 0.3048 m (1 ft) resolution.  \n",
    "   * Both rasters are then re‑projected with `gdalwarp` to EPSG 4326:  \n",
    "     `dem_epsg4326.tif`, `dsm_epsg4326.tif`.\n",
    "\n",
    "5. **Clean‑up**  \n",
    "   * All temp folders (`indices/`, `dem_tiles/`, `las_tiles/`) and intermediate rasters in EPSG 2263 are removed once the 4326 GeoTIFFs are verified non‑empty.  \n",
    "   * Leftover artefacts = **zero**. Only the two final products remain.\n",
    "\n",
    "**Data Source**\n",
    "\n",
    "* Source LiDAR survey: *NYC Topobathymetric LiDAR 2017* FTP server.\n",
    "* DEM tiles: bare‑earth (`be_…`) rasters published by NYS GIS Clearinghouse.  \n",
    "* DSM: freshly generated from the raw point cloud because no public HH DSM tiles exist, although the\n",
    "  state did publish them in 2017, they seem to have been deleted since.\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "| File | CRS / Units | Resolution | Description |\n",
    "|------|-------------|------------|-------------|\n",
    "| `dem_epsg4326.tif` | EPSG 4326, metres in Z | ≈0.00000274° (~0.3048 m) | Bare‑earth ground elevation |\n",
    "| `dsm_epsg4326.tif` | EPSG 4326, metres in Z | same grid | Surface elevation (roofs, canopy) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d71ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Downloading DEM/DSM for NYC ---\n",
      "Querying DEM API: https://elevation.its.ny.gov/arcgis/rest/services/Dem_Indexes/FeatureServer/0/query\n",
      "Found DEM download URL via API: https://gisdata.ny.gov/elevation/DEM/NYC_TopoBathymetric2017/be_NYC_001.tif\n",
      "Downloading nyc_dem_1ft_2017.zip from https://gisdata.ny.gov/elevation/DEM/NYC_TopoBathymetric2017/be_NYC_001.tif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nyc_dem_1ft_2017.zip:  27%|██▋       | 21.0M/77.8M [00:06<00:16, 3.57MiB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m dem_unzipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dem_download_url:\n\u001b[0;32m--> 132\u001b[0m     dem_downloaded \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdem_download_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdem_output_path_zip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dem_downloaded:\n\u001b[1;32m    134\u001b[0m         dem_unzipped \u001b[38;5;241m=\u001b[39m unzip_file(dem_output_path_zip, sat_files_dir)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, output_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8192\u001b[39m \u001b[38;5;66;03m# Increased block size\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f, tqdm(\n\u001b[1;32m     37\u001b[0m     desc\u001b[38;5;241m=\u001b[39moutput_path\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     38\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m bar:\n\u001b[0;32m---> 43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "FTP_HOST       = \"ftp.gis.ny.gov\"\n",
    "FTP_ROOT       = \"/elevation/LIDAR/NYC_TopoBathymetric2017\"\n",
    "\n",
    "# --- URLs and Paths ---\n",
    "# DEM: Query the ArcGIS REST API to find the download link\n",
    "dem_api_query_url = \"https://elevation.its.ny.gov/arcgis/rest/services/Dem_Indexes/FeatureServer/0/query\"\n",
    "# DSM: Will be fetched from Planetary Computer\n",
    "\n",
    "CRS_WGS84      = \"EPSG:4326\"\n",
    "CRS_NAD83_NY   = \"EPSG:2263\"\n",
    "\n",
    "dem_filename_zip = \"nyc_dem_1ft_2017.zip\" # Assuming API link points to a zip\n",
    "# DSM filename will be determined by PC query\n",
    "dem_output_path_zip = sat_files_dir / dem_filename_zip\n",
    "\n",
    "# Define final expected TIF paths (DSM path updated)\n",
    "final_dem_path_tif = sat_files_dir / \"nyc_dem_1ft_2017.tif\"\n",
    "final_dsm_path_tif = sat_files_dir / \"nyc_dsm_1m_pc.tif\" # Using 1m resolution from PC\n",
    "\n",
    "# --- Helper Function to Download (requests, for DEM API) ---\n",
    "def download_file(url, output_path):\n",
    "    if not url:\n",
    "        print(f\"Error: No URL provided for {output_path.name}.\")\n",
    "        return False\n",
    "    if output_path.exists():\n",
    "        print(f\"File {output_path.name} already exists. Skipping download.\")\n",
    "        return True\n",
    "    try:\n",
    "        print(f\"Downloading {output_path.name} from {url}...\")\n",
    "        response = requests.get(url, stream=True, timeout=120) # Increased timeout\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192 # Increased block size\n",
    "\n",
    "        with open(output_path, 'wb') as f, tqdm(\n",
    "            desc=output_path.name,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        print(f\"Successfully downloaded {output_path.name}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {output_path.name}: {e}\")\n",
    "        if output_path.exists(): os.remove(output_path)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download of {output_path.name}: {e}\")\n",
    "        if output_path.exists(): os.remove(output_path)\n",
    "        return False\n",
    "\n",
    "# --- Helper Function to Unzip (for DEM) ---\n",
    "def unzip_file(zip_path, extract_dir):\n",
    "    if not zip_path.exists():\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        return False\n",
    "    expected_tif_name = zip_path.stem + \".tif\"\n",
    "    expected_tif_path = extract_dir / expected_tif_name\n",
    "    if expected_tif_path.exists():\n",
    "        print(f\"Expected TIF file {expected_tif_path.name} already exists. Skipping unzip.\")\n",
    "        return True\n",
    "        \n",
    "    try:\n",
    "        print(f\"Unzipping {zip_path.name} to {extract_dir}...\")\n",
    "        subprocess.run(['unzip', '-o', str(zip_path), '-d', str(extract_dir)],\n",
    "                       capture_output=True, text=True, check=True, timeout=300)\n",
    "        print(f\"Successfully unzipped {zip_path.name}\")\n",
    "        if not expected_tif_path.exists():\n",
    "             print(f\"Warning: Expected TIF file {expected_tif_path.name} not found after unzipping.\")\n",
    "             return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error during unzipping of {zip_path.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Download DEM (Existing Logic) ---\n",
    "dem_download_url = None\n",
    "try:\n",
    "    print(f\"Querying DEM API: {dem_api_query_url}\")\n",
    "    params = {'where': '1=1', 'outFields': 'DIRECT_DL', 'f': 'json'}\n",
    "    api_response = requests.get(dem_api_query_url, params=params, timeout=30)\n",
    "    api_response.raise_for_status()\n",
    "    api_data = api_response.json()\n",
    "    if 'features' in api_data and len(api_data['features']) > 0:\n",
    "        dem_download_url = api_data['features'][0].get('attributes', {}).get('DIRECT_DL')\n",
    "        if dem_download_url:\n",
    "            print(f\"Found DEM download URL via API: {dem_download_url}\")\n",
    "        else:\n",
    "            print(\"Error: DEM API response missing 'DIRECT_DL'.\")\n",
    "    else:\n",
    "        print(\"Error: No features found in DEM API query response.\")\n",
    "dexcept Exception as e:\n",
    "    print(f\"Error querying DEM API: {e}\")\n",
    "\n",
    "dem_downloaded = False\n",
    "dem_unzipped = False\n",
    "if dem_download_url:\n",
    "    dem_downloaded = download_file(dem_download_url, dem_output_path_zip)\n",
    "    if dem_downloaded:\n",
    "        dem_unzipped = unzip_file(dem_output_path_zip, sat_files_dir)\n",
    "else:\n",
    "    print(\"Skipping DEM download and unzip.\")\n",
    "\n",
    "# --- Download DSM from Planetary Computer (No Auth Key Needed) --- \n",
    "print(\"\\n--- Downloading DSM from Planetary Computer ---\")\n",
    "dsm_downloaded_pc = False\n",
    "if final_dsm_path_tif.exists():\n",
    "    print(f\"DSM file {final_dsm_path_tif.name} already exists. Skipping download.\")\n",
    "    dsm_downloaded_pc = True\n",
    "else:\n",
    "    try:\n",
    "        # Define bounding box and CRS for STAC query\n",
    "        bbox = bounds # Use the same bounds calculated earlier\n",
    "        print(f\"Using bounding box for DSM query: {bbox}\")\n",
    "\n",
    "        # Search the 3dep-lidar-dsm collection using the public STAC endpoint\n",
    "        catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "        search = catalog.search(\n",
    "            collections=[\"3dep-lidar-dsm\"],\n",
    "            bbox=bbox,\n",
    "        )\n",
    "        items = search.item_collection()\n",
    "        print(f\"Found {len(items)} 3DEP DSM items for the bounding box.\")\n",
    "        \n",
    "        if not items:\n",
    "            print(\"No 3DEP DSM items found for the specified area. Cannot download DSM.\")\n",
    "        else:\n",
    "            # Sign items - this might be needed for direct access depending on collection settings\n",
    "            # but often works without an explicit key for public data\n",
    "            signed_items = planetary_computer.sign(items)\n",
    "            \n",
    "            # Load items into an xarray DataArray using stackstac\n",
    "            # We will use the 'data' asset which is the Cloud Optimized GeoTIFF\n",
    "            # Requesting resolution close to 1m (approx 0.00001 degrees)\n",
    "            dsm_data = stackstac.stack(\n",
    "                signed_items, # Use signed items\n",
    "                assets=[\"data\"],\n",
    "                resolution=0.00001, # Approx 1m\n",
    "                dtype=np.float32,\n",
    "                fill_value=np.nan, # Use NaN for fill\n",
    "                bounds_latlon=bbox # Ensure stack uses the query bounds\n",
    "            ).squeeze() # Remove time/band dims if unnecessary\n",
    "            \n",
    "            print(f\"Created DSM xarray with shape: {dsm_data.shape}\")\n",
    "            # Assign CRS if missing (should be EPSG:4326 from PC)\n",
    "            if dsm_data.rio.crs is None:\n",
    "                 dsm_data = dsm_data.rio.write_crs(\"EPSG:4326\")\n",
    "                 \n",
    "            # Set nodata value explicitly for writing\n",
    "            dsm_data.rio.write_nodata(np.nan, inplace=True)\n",
    "            \n",
    "            # Save the DataArray to a GeoTIFF\n",
    "            print(f\"Saving DSM to {final_dsm_path_tif}...\")\n",
    "            dsm_data.rio.to_raster(final_dsm_path_tif, driver=\"COG\") # Use Cloud Optimized GeoTIFF driver\n",
    "            print(\"Successfully downloaded and saved DSM from Planetary Computer.\")\n",
    "            dsm_downloaded_pc = True\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"Error: Missing libraries for Planetary Computer access ({e}). Install pystac-client, planetary-computer, stackstac, rioxarray.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Planetary Computer DSM download: {e}\")\n",
    "\n",
    "# reproject DEM to EPSG:4326\n",
    "if not dem4326.exists():\n",
    "    subprocess.run([\"gdalwarp\", \"-t_srs\", CRS_WGS84, \"-r\", \"bilinear\",\n",
    "                    \"-dstnodata\", \"-9999\", \"-overwrite\",\n",
    "                    merged_dem2263, dem4326], check=True)\n",
    "\n",
    "# --- Update Relative Paths for Config ---\n",
    "config_dem_path_relative = Path(\"data\") / city_name / \"sat_files\" / final_dem_path_tif.name\n",
    "config_dsm_path_relative = Path(\"data\") / city_name / \"sat_files\" / final_dsm_path_tif.name # Use the new filename\n",
    "print(f\"\\nRelative paths for config:\")\n",
    "print(f\"  DEM: {config_dem_path_relative}\")\n",
    "print(f\"  DSM: {config_dsm_path_relative}\")\n",
    "\n",
    "# Optional: Clean up DEM zip file\n",
    "if dem_unzipped and final_dem_path_tif.exists() and dem_output_path_zip.exists():\n",
    "    print(f\"Cleaning up {dem_output_path_zip.name}...\")\n",
    "    # os.remove(dem_output_path_zip) # Uncomment to enable cleanup\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
