{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0bd36b",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3faad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d05ec",
   "metadata": {},
   "source": [
    "# UHI Data Download and Processing Pipeline\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Download and process UHI GeoTIFF files from sources like Fort Lauderdale (FTL) and convert them to CSV format\n",
    "2. Download satellite imagery for specific cities and time periods and save them locally\n",
    "3. Use local satellite data files with the dataloader instead of direct API calls\n",
    "\n",
    "The pipeline is designed to work with different city datasets with the same structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fad66",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94725a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Add the project root to the Python path to allow importing from src\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d213cf9-4623-420a-af80-3d7658873ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv\n",
      "Representative UHI date (from first row): 2021-07-24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration for Data Download ---\n",
    "\n",
    "# Configure parameters for the target city (e.g., NYC)\n",
    "city_name = \"NYC\"\n",
    "\n",
    "# Time window matching original notebooks (adjust if needed)\n",
    "sentinel_time_window = \"2021-06-01/2021-09-01\"\n",
    "lst_time_window = \"2021-06-01/2021-09-01\"\n",
    "\n",
    "# Input files and general settings\n",
    "data_dir = Path(\"data\")\n",
    "abs_output_dir = project_root / data_dir\n",
    "uhi_csv = data_dir / city_name / \"uhi.csv\" # Path to UHI data\n",
    "abs_uhi_csv = project_root / uhi_csv\n",
    "# bbox_csv is no longer needed for bounds calculation\n",
    "\n",
    "if not abs_uhi_csv.exists():\n",
    "    raise FileNotFoundError(f\"UHI data CSV not found at {abs_uhi_csv}. Cannot derive bounds.\")\n",
    "print(f\"Loading bounds from UHI data: {abs_uhi_csv}\")\n",
    "uhi_df = pd.read_csv(abs_uhi_csv)\n",
    "# Check if required columns exist\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "     raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "\n",
    "# Load bounds\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "\n",
    "# Load observation da\n",
    "first_datetime_obj = pd.to_datetime(uhi_df['datetime'].iloc[0], format='%d-%m-%Y %H:%M')\n",
    "# Format the date object into 'YYYY-MM-DD' string format\n",
    "uhi_date_str = first_datetime_obj.strftime('%Y-%m-%d')\n",
    "print(f\"Representative UHI date (from first row): {uhi_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af99245",
   "metadata": {},
   "source": [
    "## 2. Download Satellite Data for Cities\n",
    "\n",
    "Now we'll download satellite imagery data (Sentinel-2 median composites, Landsat LST medians) for specific cities and time periods derived from the UHI data timestamps. Data is saved locally for use by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7fbf75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bounds from UHI data: /home/jupyter/UHI/MLC-Project/data/NYC/uhi.csv\n",
      "City: NYC\n",
      "Sentinel-2 Time Window: 2021-06-01/2021-09-01\n",
      "Sentinel-2 Cloud Cover Threshold: 30%\n",
      "LST Time Window: 2021-06-01/2021-09-01\n",
      "Bounds derived from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "Target mosaic output path: /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "Include LST: False\n"
     ]
    }
   ],
   "source": [
    "# Cell from notebooks/download_data.ipynb (modified)\n",
    "\n",
    "# Import functions\n",
    "from src.ingest.get_median import create_and_save_cloudless_mosaic\n",
    "# Import the modified LST download function\n",
    "from src.ingest.create_sat_tensor_files import download_single_lst_median\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Parameters for Cloudless Mosaic (matching Sentinel2_GeoTIFF.ipynb)\n",
    "mosaic_bands = [\"B02\", \"B03\", \"B04\", \"B08\"] # RGB+NIR for Clay compatibility\n",
    "mosaic_resolution_m = 10\n",
    "mosaic_cloud_cover = 30\n",
    "\n",
    "# Parameters for LST Median (matching Landsat_LST.ipynb)\n",
    "include_lst = False         # Whether to download LST\n",
    "lst_resolution_m = 30      # Native resolution for Landsat LST\n",
    "\n",
    "# Generate output path for the mosaic based on the new time window\n",
    "start_dt_str = sentinel_time_window.split('/')[0].replace('-','')\n",
    "end_dt_str = sentinel_time_window.split('/')[1].replace('-','')\n",
    "band_str = \"_\".join(mosaic_bands)\n",
    "cloudless_mosaic_filename = f\"sentinel_{city_name}_{start_dt_str}_to_{end_dt_str}_cloudless_mosaic.npy\"\n",
    "cloudless_mosaic_path = abs_output_dir / city_name / \"sat_files\" / cloudless_mosaic_filename\n",
    "# --- Verification ---\n",
    "print(f\"City: {city_name}\")\n",
    "print(f\"Sentinel-2 Time Window: {sentinel_time_window}\")\n",
    "print(f\"Sentinel-2 Cloud Cover Threshold: {mosaic_cloud_cover}%\")\n",
    "print(f\"LST Time Window: {lst_time_window}\")\n",
    "print(f\"Bounds derived from {uhi_csv.name}: {bounds}\")\n",
    "print(f\"Target mosaic output path: {cloudless_mosaic_path}\")\n",
    "print(f\"Include LST: {include_lst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4f9b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:00:41,109 - INFO - Cloudless mosaic /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy already exists. Skipping generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Cloudless Mosaic (2021-06-01/2021-09-01) ---\n",
      "Cloudless mosaic saved/found at: /home/jupyter/UHI/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "\n",
      "--- Downloading Single LST Median (Include: False, Window: 2021-06-01/2021-09-01) ---\n",
      "Skipping LST median download as include_lst is False.\n",
      "\n",
      "Verifying output files:\n",
      "  Mosaic path (sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy) exists: True\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Generate Cloudless Mosaic --- \n",
    "print(f\"\\n--- Generating Cloudless Mosaic ({sentinel_time_window}) ---\")\n",
    "\n",
    "mosaic_output_path = create_and_save_cloudless_mosaic(\n",
    "    city_name=city_name,\n",
    "    bounds=bounds,\n",
    "    output_dir=abs_output_dir,\n",
    "    time_window=sentinel_time_window, # Use the explicit time window\n",
    "    selected_bands=mosaic_bands,\n",
    "    resolution_m=mosaic_resolution_m,\n",
    "    cloud_cover=mosaic_cloud_cover # Use the updated cloud cover\n",
    ")\n",
    "\n",
    "if mosaic_output_path:\n",
    "    print(f\"Cloudless mosaic saved/found at: {mosaic_output_path}\")\n",
    "else:\n",
    "    # Stop if mosaic fails, as it's required\n",
    "    raise RuntimeError(\"Failed to generate cloudless mosaic.\")\n",
    "\n",
    "# --- 2. Download Single LST Median (if enabled) ---\n",
    "print(f\"\\n--- Downloading Single LST Median (Include: {include_lst}, Window: {lst_time_window}) ---\")\n",
    "\n",
    "single_lst_median_file_path = None # Initialize path variable\n",
    "if include_lst:\n",
    "    # No need to check UHI CSV, we provide the time window directly\n",
    "    \n",
    "    # Download the single LST median using the explicit time window\n",
    "    single_lst_median_file_path = download_single_lst_median(\n",
    "        city_name=city_name,\n",
    "        bounds=bounds,\n",
    "        output_dir=abs_output_dir,\n",
    "        time_window=lst_time_window, # Provide explicit window\n",
    "        # uhi_csv_path and averaging_window are omitted/None\n",
    "        resolution_m=lst_resolution_m\n",
    "        # lst_cloud_cover is handled internally by load_lst_tensor_from_bbox_median\n",
    "    )\n",
    "\n",
    "    if single_lst_median_file_path:\n",
    "        print(f\"Single LST median saved/found at: {single_lst_median_file_path}\")\n",
    "    else:\n",
    "        print(\"Failed to generate single LST median.\")\n",
    "else:\n",
    "    print(\"Skipping LST median download as include_lst is False.\")\n",
    "\n",
    "# --- Verification ---\n",
    "sat_files_check_dir = Path(abs_output_dir) / city_name / \"sat_files\"\n",
    "print(f\"\\nVerifying output files:\")\n",
    "print(f\"  Mosaic path ({cloudless_mosaic_path.name}) exists: {cloudless_mosaic_path.exists()}\")\n",
    "if include_lst:\n",
    "    # Construct expected LST filename based on the explicit window\n",
    "    lst_start_str = lst_time_window.split('/')[0].replace('-','')\n",
    "    lst_end_str = lst_time_window.split('/')[1].replace('-','')\n",
    "    expected_lst_filename = f\"lst_{city_name}_median_{lst_start_str}_to_{lst_end_str}.npy\"\n",
    "    expected_lst_path = sat_files_check_dir / expected_lst_filename\n",
    "    print(f\"  Single LST median path ({expected_lst_filename}) exists: {expected_lst_path.exists()}\")\n",
    "    # Update the variable used by later cells if generation was successful\n",
    "    if single_lst_median_file_path and not single_lst_median_file_path.exists():\n",
    "         # This case shouldn't happen if the function worked, but good sanity check\n",
    "         print(f\"Warning: LST download function returned a path but it doesn't exist: {single_lst_median_file_path}\")\n",
    "         single_lst_median_file_path = None # Ensure later cells know it failed\n",
    "    elif not single_lst_median_file_path and expected_lst_path.exists():\n",
    "         # File existed previously, update path variable for later cells\n",
    "         single_lst_median_file_path = expected_lst_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2dff",
   "metadata": {},
   "source": [
    "## 3. Using Local Satellite Data with the Dataloader\n",
    "\n",
    "Finally, we'll demonstrate how to use the modified dataloader that works with local satellite data files instead of making API calls directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c990db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking existence of required files for CityDataSet initialization:\n",
      "  MISSING: Weather CSV at /home/jupyter/UHI/MLC-Project/data/NYC/weather_grid.csv\n",
      "\n",
      "Error: Not all required files exist. Cannot initialize CityDataSet.\n"
     ]
    }
   ],
   "source": [
    "from src.ingest.dataloader import CityDataSet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weather_csv = data_dir / city_name / \"weather.csv\"\n",
    "\n",
    "\n",
    "# Construct the *correct* path for the generated mosaic file\n",
    "start_dt_str = sentinel_time_window.split('/')[0].replace('-','')\n",
    "end_dt_str = sentinel_time_window.split('/')[1].replace('-','')\n",
    "actual_mosaic_filename = f\"sentinel_{city_name}_{start_dt_str}_to_{end_dt_str}_cloudless_mosaic.npy\"\n",
    "actual_mosaic_path = abs_data_dir / city_name / \"sat_files\" / actual_mosaic_filename\n",
    "\n",
    "# Construct the *expected* path for the LST median file (used if include_lst=True)\n",
    "single_lst_median_file_path = None # Initialize\n",
    "if include_lst:\n",
    "    lst_start_str = lst_time_window.split('/')[0].replace('-','')\n",
    "    lst_end_str = lst_time_window.split('/')[1].replace('-','')\n",
    "    expected_lst_filename = f\"lst_{city_name}_median_{lst_start_str}_to_{lst_end_str}.npy\"\n",
    "    single_lst_median_file_path = abs_data_dir / city_name / \"sat_files\" / expected_lst_filename\n",
    "    if not single_lst_median_file_path.exists():\n",
    "        print(f\"Warning: Expected LST median file not found at {single_lst_median_file_path}. LST will likely be disabled by DataLoader.\")\n",
    "        # Keep the path so DataLoader can log the error if needed, or set to None\n",
    "        # single_lst_median_file_path = None\n",
    "\n",
    "# --- Check required files before initializing Dataset ---\n",
    "required_paths_dict_ds = {\n",
    "    \"UHI CSV\": abs_uhi_csv,\n",
    "    \"Weather CSV\": abs_weather_csv,\n",
    "    \"Cloudless Mosaic\": actual_mosaic_path\n",
    "}\n",
    "if include_lst and single_lst_median_file_path: # Only require LST path if include_lst is True and path is defined\n",
    "     required_paths_dict_ds[\"Single LST Median\"] = single_lst_median_file_path\n",
    "\n",
    "print(\"Checking existence of required files for CityDataSet initialization:\")\n",
    "all_ds_files_exist = True\n",
    "for name, path in required_paths_dict_ds.items():\n",
    "    exists = False\n",
    "    if path and Path(path).exists():\n",
    "        exists = True\n",
    "    else:\n",
    "        all_ds_files_exist = False\n",
    "        print(f\"  MISSING: {name} at {path}\")\n",
    "    # print(f\"  {name}: {path} -> Exists: {exists}\") # Optional verbose print\n",
    "\n",
    "if not all_ds_files_exist:\n",
    "    print(\"\\nError: Not all required files exist. Cannot initialize CityDataSet.\")\n",
    "else:\n",
    "    print(\"\\nAll required files found. Initializing dataset...\")\n",
    "    try:\n",
    "        # Use mosaic_resolution_m for the target resolution\n",
    "        target_resolution_m = mosaic_resolution_m\n",
    "\n",
    "        # Use the string path for LST, or None\n",
    "        lst_path_arg = str(single_lst_median_file_path) if include_lst and single_lst_median_file_path and single_lst_median_file_path.exists() else None\n",
    "\n",
    "        dataset = CityDataSet(\n",
    "            bounds=bounds,\n",
    "            averaging_window=averaging_window_lst, # Pass required arg\n",
    "            resolution_m=target_resolution_m,\n",
    "            uhi_csv=str(abs_uhi_csv),\n",
    "            bbox_csv=str(abs_bbox_csv),\n",
    "            weather_csv=str(abs_weather_csv),\n",
    "            cloudless_mosaic_path=str(actual_mosaic_path), # Use correct mosaic path\n",
    "            data_dir=str(abs_data_dir),\n",
    "            city_name=city_name,\n",
    "            include_lst=include_lst,\n",
    "            single_lst_median_path=lst_path_arg # Pass path or None\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSuccessfully initialized dataset with {len(dataset)} samples.\")\n",
    "        print(f\"  LST included in output: {dataset.include_lst}\")\n",
    "        print(f\"  Target grid resolution: {dataset.resolution_m}m\")\n",
    "        print(f\"  Target grid shape (H, W): ({dataset.sat_H}, {dataset.sat_W})\")\n",
    "\n",
    "\n",
    "        # --- Inspect First Sample ---\n",
    "        if len(dataset) > 0:\n",
    "            first_sample = dataset[0]\n",
    "            print(\"\\nSample keys:\", list(first_sample.keys()))\n",
    "\n",
    "            print(\"\\nTensor shapes in first sample:\")\n",
    "            for key, tensor in first_sample.items():\n",
    "                if hasattr(tensor, 'shape'):\n",
    "                     print(f\"  {key}: {tensor.shape} (dtype: {tensor.dtype})\")\n",
    "                else:\n",
    "                     print(f\"  {key}: {type(tensor)}\") # Should all be tensors\n",
    "\n",
    "            # Plot the cloudless mosaic (RGB) - Requires bands B04, B03, B02 in that order in the mosaic\n",
    "            # NOTE: This requires knowing the band order within the saved .npy file.\n",
    "            # Assuming standard RGB order [Red (B04), Green (B03), Blue (B02)] might be indices [2, 1, 0]\n",
    "            # if the mosaic was saved with [\"B02\", \"B03\", \"B04\", \"B08\"] order. Adjust if needed.\n",
    "            mosaic_tensor = first_sample['cloudless_mosaic']\n",
    "            if mosaic_tensor.shape[0] >= 3:\n",
    "                try:\n",
    "                    # Assuming Red=idx 2, Green=idx 1, Blue=idx 0 based on typical [\"B02\", \"B03\", \"B04\", ...] order\n",
    "                    rgb_indices = [2, 1, 0]\n",
    "                    rgb = mosaic_tensor[rgb_indices, :, :]\n",
    "                    rgb = np.transpose(rgb.numpy(), (1, 2, 0)) # Transpose to H, W, C for imshow\n",
    "\n",
    "                    # Normalize for display (simple min-max scaling)\n",
    "                    min_val, max_val = np.percentile(rgb, [2, 98]) # Clip outliers\n",
    "                    rgb_display = np.clip((rgb - min_val) / (max_val - min_val), 0, 1)\n",
    "\n",
    "                    plt.figure(figsize=(8, 8))\n",
    "                    plt.imshow(rgb_display)\n",
    "                    plt.title(f\"Cloudless Mosaic RGB Approx. ({city_name})\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                except Exception as plot_e:\n",
    "                    print(f\"\\nCould not plot mosaic RGB: {plot_e}\")\n",
    "\n",
    "\n",
    "            # Plot the target UHI grid for the first sample\n",
    "            target_uhi = first_sample['target'].numpy() # Convert to numpy for plotting\n",
    "            uhi_mask = first_sample['mask'].numpy() > 0.5 # Boolean mask\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            valid_uhi = target_uhi[uhi_mask]\n",
    "            vmin = np.nanmin(valid_uhi) if valid_uhi.size > 0 else 0\n",
    "            vmax = np.nanmax(valid_uhi) if valid_uhi.size > 0 else 1\n",
    "            # Display masked areas explicitly (e.g., as white)\n",
    "            display_uhi = np.where(uhi_mask, target_uhi, np.nan)\n",
    "            plt.imshow(display_uhi, cmap='viridis', vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "            plt.colorbar(label='UHI Index')\n",
    "            plt.title(f\"Target UHI Grid (Sample 0) - Masked areas are NaN/White\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            # Plot LST if included\n",
    "            if dataset.include_lst:\n",
    "                 lst_grid = first_sample['lst_seq'][0,0,:,:].numpy() # Remove T and C dims\n",
    "                 plt.figure(figsize=(8, 8))\n",
    "                 plt.imshow(lst_grid, cmap='plasma') # LST often uses plasma/inferno\n",
    "                 plt.colorbar(label='Normalized LST')\n",
    "                 plt.title(f\"Static LST Median (Sample 0)\")\n",
    "                 plt.axis('off')\n",
    "                 plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError initializing or inspecting dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset if all required files exist\n",
    "if required_files_exist:\n",
    "    try:\n",
    "        # Parameters from config cell (Cell 6)\n",
    "        target_resolution_m = mosaic_resolution_m # Use mosaic resolution\n",
    "        print(f\"\\nInitializing dataset with target resolution: {target_resolution_m}m\")\n",
    "\n",
    "        # Pass the path to the single LST median file if it exists\n",
    "        lst_path_arg = str(single_lst_median_file_path) if include_lst and single_lst_median_file_path else None\n",
    "\n",
    "        dataset = CityDataSet(\n",
    "            bounds=bounds,\n",
    "            averaging_window=averaging_window_lst, # Still needed by constructor, though not used for LST if path provided\n",
    "            resolution_m=target_resolution_m,\n",
    "            uhi_csv=abs_uhi_csv,\n",
    "            bbox_csv=abs_bbox_csv,\n",
    "            weather_csv=abs_weather_csv,\n",
    "            cloudless_mosaic_path=str(cloudless_mosaic_path),\n",
    "            data_dir=abs_data_dir,\n",
    "            city_name=city_name,\n",
    "            include_lst=include_lst,\n",
    "            single_lst_median_path=lst_path_arg\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSuccessfully initialized dataset with {len(dataset)} samples. LST included: {dataset.include_lst}\")\n",
    "\n",
    "        # --- Inspect First Sample ---\n",
    "        if len(dataset) > 0:\n",
    "            first_sample = dataset[0]\n",
    "            print(\"\\nSample keys:\", list(first_sample.keys()))\n",
    "\n",
    "            print(\"\\nTensor shapes in first sample:\")\n",
    "            for key, tensor in first_sample.items():\n",
    "                # Only print shape, handle potential non-tensor items gracefully if any added later\n",
    "                if hasattr(tensor, 'shape'):\n",
    "                     print(f\"  {key}: {tensor.shape} (dtype: {tensor.dtype})\")\n",
    "                else:\n",
    "                     print(f\"  {key}: {type(tensor)}\")\n",
    "\n",
    "            # Plot the cloudless mosaic (RGB)\n",
    "            mosaic_tensor = first_sample['cloudless_mosaic']\n",
    "            if mosaic_tensor.shape[0] >= 3:\n",
    "                rgb_indices = []\n",
    "                required = [\"B04\", \"B03\", \"B02\"]\n",
    "                missing = []\n",
    "                for band in required:\n",
    "                     try: rgb_indices.append(mosaic_bands.index(band))\n",
    "                     except ValueError: missing.append(band)\n",
    "\n",
    "                if not missing:\n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    rgb = mosaic_tensor[rgb_indices, :, :]\n",
    "                    rgb = np.transpose(rgb, (1, 2, 0))\n",
    "                    min_val, max_val = rgb.min(), rgb.max()\n",
    "                    if max_val > min_val: rgb = (rgb - min_val) / (max_val - min_val)\n",
    "                    else: rgb = np.zeros_like(rgb)\n",
    "                    plt.imshow(rgb)\n",
    "                    plt.title(f\"Cloudless Mosaic RGB ({city_name}, {mosaic_year})\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(f\"\\nCannot display RGB composite: Missing bands {missing} in mosaic_bands {mosaic_bands}\")\n",
    "\n",
    "            # Plot the target UHI grid for the first sample\n",
    "            target_uhi = first_sample['target']\n",
    "            uhi_mask = first_sample['mask']\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            # Handle cases where target might be all NaN after masking/grouping\n",
    "            valid_uhi = target_uhi[uhi_mask > 0.5] # Use mask to select valid points\n",
    "            vmin = np.nanmin(valid_uhi) if valid_uhi.size > 0 else 0\n",
    "            vmax = np.nanmax(valid_uhi) if valid_uhi.size > 0 else 1\n",
    "            plt.imshow(target_uhi, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            plt.colorbar(label='UHI Index')\n",
    "            plt.title(f\"Target UHI Grid (Sample 0) - Masked areas are NaN/White\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError initializing or inspecting dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nCannot initialize dataset: Not all required files were found. Please run previous steps.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
