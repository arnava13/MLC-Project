{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd353f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded bounds from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "\n",
      "Branched Model Configuration dictionary created:\n",
      "{\n",
      "  \"model_type\": \"BranchedUHIModel_CommonRes\",\n",
      "  \"project_root\": \"/home/jupyter/MLC-Project\",\n",
      "  \"run_dir\": \"/home/jupyter/MLC-Project/training_runs/train_20250506_054808/NYC_BranchedUHI\",\n",
      "  \"city_name\": \"NYC\",\n",
      "  \"wandb_project_name\": \"MLC_UHI_Proj\",\n",
      "  \"wander_run_name_prefix\": \"NYC_BranchedUHI\",\n",
      "  \"feature_resolution_m\": 30,\n",
      "  \"uhi_grid_resolution_m\": 10,\n",
      "  \"weather_seq_length\": 60,\n",
      "  \"uhi_csv\": \"data/NYC/uhi.csv\",\n",
      "  \"bronx_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/bronx_weather.csv\",\n",
      "  \"manhattan_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/manhattan_weather.csv\",\n",
      "  \"bounds\": [\n",
      "    -73.99445667,\n",
      "    40.75879167,\n",
      "    -73.87945833,\n",
      "    40.85949667\n",
      "  ],\n",
      "  \"feature_flags\": {\n",
      "    \"use_dem\": true,\n",
      "    \"use_dsm\": true,\n",
      "    \"use_clay\": true,\n",
      "    \"use_sentinel_composite\": false,\n",
      "    \"use_lst\": false,\n",
      "    \"use_ndvi\": false,\n",
      "    \"use_ndbi\": false,\n",
      "    \"use_ndwi\": false\n",
      "  },\n",
      "  \"sentinel_bands_to_load\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\",\n",
      "    \"swir16\",\n",
      "    \"swir22\"\n",
      "  ],\n",
      "  \"dem_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_10m_pc.tif\",\n",
      "  \"dsm_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_10m_pc.tif\",\n",
      "  \"elevation_nodata\": -9999.0,\n",
      "  \"cloudless_mosaic_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\",\n",
      "  \"single_lst_median_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/lst_NYC_median_20210601_to_20210901.npy\",\n",
      "  \"lst_nodata\": 0.0,\n",
      "  \"weather_input_channels\": 6,\n",
      "  \"convlstm_hidden_dims\": [\n",
      "    16,\n",
      "    8\n",
      "  ],\n",
      "  \"convlstm_kernel_sizes\": [\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"convlstm_num_layers\": 2,\n",
      "  \"proj_static_ch\": 8,\n",
      "  \"proj_temporal_ch\": 8,\n",
      "  \"unet_base_channels\": 16,\n",
      "  \"unet_depth\": 2,\n",
      "  \"clay_model_size\": \"large\",\n",
      "  \"clay_bands\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\"\n",
      "  ],\n",
      "  \"clay_platform\": \"sentinel-2-l2a\",\n",
      "  \"clay_gsd\": 10,\n",
      "  \"freeze_backbone\": true,\n",
      "  \"clay_checkpoint_path\": \"/home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\",\n",
      "  \"clay_metadata_path\": \"/home/jupyter/MLC-Project/src/Clay/configs/metadata.yaml\",\n",
      "  \"n_train_batches\": 47,\n",
      "  \"num_workers\": 1,\n",
      "  \"epochs\": 500,\n",
      "  \"lr\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"loss_type\": \"mse\",\n",
      "  \"patience\": 50,\n",
      "  \"device\": \"cuda\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI\" # Modified prefix\n",
    "run_dir = output_dir / wander_run_name_prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "# NEW: Define the common resolution for spatial features entering the model\n",
    "feature_resolution_m = 30 # Start with 10m (matches Clay/UHI grid)\n",
    "\n",
    "# Define UHI grid resolution separately (used for final target matching)\n",
    "# This assumes UHI data corresponds to 10m grid\n",
    "uhi_grid_resolution_m = 10\n",
    "\n",
    "weather_seq_length = 60\n",
    "\n",
    "# Input Data Paths (relative)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\"\n",
    "relative_dem_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dem_10m_pc.tif\"\n",
    "relative_dsm_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dsm_10m_pc.tif\"\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0\n",
    "lst_nodata = 0.0 # Or appropriate value for LST median file\n",
    "\n",
    "# --- Feature Selection Flags ---\n",
    "# IMPORTANT: Only enable the features you want to use\n",
    "# The Model will expect exactly these features\n",
    "feature_flags = {\n",
    "    \"use_dem\": True,              # Digital Elevation Model\n",
    "    \"use_dsm\": True,              # Digital Surface Model\n",
    "    \"use_clay\": True,             # Clay feature extractor\n",
    "    \"use_sentinel_composite\": False, # Raw Sentinel-2 bands\n",
    "    \"use_lst\": False,             # Land Surface Temperature\n",
    "    \"use_ndvi\": False,            # Normalized Difference Vegetation Index\n",
    "    \"use_ndbi\": False,            # Normalized Difference Built-up Index\n",
    "    \"use_ndwi\": False,            # Normalized Difference Water Index\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM, No separate Elev branches) ---\n",
    "\n",
    "# Clay Backbone (if feature_flags[\"use_clay\"] is True)\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "relative_clay_checkpoint_path = \"notebooks/clay-v1.5.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "weather_input_channels = 6\n",
    "convlstm_hidden_dims = [16, 8]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# --- REMOVED High-Res Elevation Branch Config ---\n",
    "\n",
    "# U-Net Head\n",
    "unet_base_channels = 16\n",
    "unet_depth = 2 # <<< REDUCED from 3\n",
    "\n",
    "# Projection Layer Channels\n",
    "proj_static_ch = 8 # For projecting ALL static feats (Clay, LST, DEM, DSM, Indices)\n",
    "proj_temporal_ch = 8 # For projecting ConvLSTM output\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 1\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "\n",
    "# V100 Suggestion: Target batch size 2 => n_train_batches = 47 / 1 = 47\n",
    "n_train_batches = 47\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Resolve Paths using check_path from train_utils ---\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, project_root, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, project_root, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "# Always check paths needed by dataloader, flags control usage inside\n",
    "absolute_dem_path = check_path(relative_dem_path, project_root, \"DEM TIF\")\n",
    "absolute_dsm_path = check_path(relative_dsm_path, project_root, \"DSM TIF\")\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "absolute_single_lst_median_path = check_path(relative_single_lst_median_path, project_root, \"Single LST Median\", should_exist=feature_flags[\"use_lst\"]) # Check only if flag is true\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel_CommonRes\",\n",
    "    \"project_root\": project_root_str,\n",
    "    \"run_dir\": run_dir,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m, # NEW\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m, # For reference\n",
    "    \"weather_seq_length\": weather_seq_length,\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(absolute_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(absolute_manhattan_weather_csv),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(absolute_dem_path) if absolute_dem_path else None,\n",
    "    \"dsm_path\": str(absolute_dsm_path) if absolute_dsm_path else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "    \"single_lst_median_path\": str(absolute_single_lst_median_path) if absolute_single_lst_median_path else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config\n",
    "    \"weather_input_channels\": weather_input_channels,\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    \"unet_base_channels\": unet_base_channels,\n",
    "    \"unet_depth\": unet_depth, # Updated\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(absolute_clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(absolute_clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nBranched Model Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92b31ff2-f0b1-4823-93c1-b62dfb77abc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 05:57:08,724 - INFO - Target FEATURE grid size (H, W): (373, 323) @ 30m, CRS: EPSG:4326\n",
      "2025-05-06 05:57:08,725 - INFO - Target UHI grid size (H, W): (1118, 969) @ 10m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BranchedCityDataSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 1141.13it/s]\n",
      "2025-05-06 05:57:08,798 - INFO - Loading DEM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_10m_pc.tif\n",
      "2025-05-06 05:57:08,848 - INFO - DEM loaded raw shape: (1, 1120, 1279)\n",
      "2025-05-06 05:57:08,909 - INFO - Clipping DEM to bounds: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "2025-05-06 05:57:08,911 - INFO - Opened DEM (lazy load). Native shape (approx): (1, 1120, 1279)\n",
      "2025-05-06 05:57:08,911 - INFO - Loading DSM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_10m_pc.tif\n",
      "2025-05-06 05:57:08,971 - INFO - DSM loaded raw shape: (1, 1120, 1279)\n",
      "2025-05-06 05:57:09,044 - INFO - Clipping DSM to bounds: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "2025-05-06 05:57:09,045 - INFO - Opened DSM (lazy load). Native shape (approx): (1, 1120, 1279)\n",
      "2025-05-06 05:57:09,046 - INFO - Calculating global DEM min/max...\n",
      "2025-05-06 05:57:09,049 - INFO - Global DEM Min: -0.6499999761581421, Max: 86.5199966430664\n",
      "2025-05-06 05:57:09,049 - INFO - Calculating global DSM min/max...\n",
      "2025-05-06 05:57:09,052 - INFO - Global DSM Min: -0.7897066473960876, Max: 206.5931854248047\n",
      "2025-05-06 05:57:09,053 - INFO - Loading cloudless mosaic from /home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy with memory mapping\n",
      "2025-05-06 05:57:09,054 - INFO - Loaded mosaic shape (native res): (4, 1119, 1278)\n",
      "2025-05-06 05:57:09,062 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-05-06 05:57:09,063 - INFO - Loaded Manhattan weather data: 169 records\n",
      "2025-05-06 05:57:09,065 - INFO - Computed grid cell center coordinates.\n",
      "2025-05-06 05:57:09,065 - INFO - Dataset initialized for NYC with 59 unique timestamps.\n",
      "2025-05-06 05:57:09,066 - INFO - Weather sequence length T = 60\n",
      "2025-05-06 05:57:09,066 - INFO - Enabled features (flags): {\"use_dem\": true, \"use_dsm\": true, \"use_clay\": true, \"use_sentinel_composite\": false, \"use_lst\": false, \"use_ndvi\": false, \"use_ndbi\": false, \"use_ndwi\": false}\n",
      "2025-05-06 05:57:09,068 - INFO - DEM loaded: True\n",
      "2025-05-06 05:57:09,068 - INFO - DSM loaded: True\n",
      "2025-05-06 05:57:09,069 - INFO - LST loaded: False\n",
      "2025-05-06 05:57:09,070 - INFO - Mosaic loaded: True\n",
      "2025-05-06 05:57:09,071 - INFO - Calculating UHI statistics from training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 47 training (indices 0-46), 12 validation (indices 47-58) samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stats: 100%|██████████| 47/47 [00:00<00:00, 789.31it/s]\n",
      "2025-05-06 05:57:09,134 - INFO - Training UHI Mean: 1.0006, Std Dev: 0.0168\n",
      "2025-05-06 05:57:09,135 - INFO - Creating dataloaders...\n",
      "2025-05-06 05:57:09,136 - INFO - Using Train Batch Size: 1\n",
      "2025-05-06 05:57:09,137 - INFO - Using Validation Batch Size: 1\n",
      "2025-05-06 05:57:09,137 - INFO - Data loading setup complete.\n"
     ]
    }
   ],
   "source": [
    "# %% Data Loading and Preprocessing (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # Removed split_data\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = CityDataSetBranched(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"], # Corrected param name\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"], # Corrected param name\n",
    "        uhi_csv=absolute_uhi_csv, # Use absolute path resolved earlier\n",
    "        bronx_weather_csv=absolute_bronx_weather_csv,\n",
    "        manhattan_weather_csv=absolute_manhattan_weather_csv,\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"], # Corrected param name\n",
    "        dsm_path=config[\"dsm_path\"], # Corrected param name\n",
    "        elevation_nodata=config[\"elevation_nodata\"], # Corrected param name\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"], # Added missing param\n",
    "        weather_seq_length=config[\"weather_seq_length\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split --- #\n",
    "val_percent = 0.20 # Keep the percentage definition\n",
    "num_samples = len(dataset)\n",
    "if num_samples < 2: # Need at least one for train and one for val\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd1b761f-1d71-4581-bf50-c785d8e88a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/MLC-Project/training_runs/train_20250506_054808/NYC_BranchedUHI/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save config to run directory\u001b[39;00m\n\u001b[1;32m     17\u001b[0m config_path \u001b[38;5;241m=\u001b[39m Path(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(config, f)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize wandb\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/MLC-Project/training_runs/train_20250506_054808/NYC_BranchedUHI/config.json'"
     ]
    }
   ],
   "source": [
    "# --- Helper functions ---\n",
    "# Get the warmup epochs from config or default to 5\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "# Initialize metrics tracking\n",
    "best_val_loss = float('inf')\n",
    "best_val_rmse = float('inf')\n",
    "best_val_r2 = -float('inf')\n",
    "patience_counter = 0\n",
    "training_log = []\n",
    "\n",
    "# Create run directory\n",
    "model_save_dir = Path(config['run_dir']) / \"checkpoints\"\n",
    "\n",
    "# Save config to run directory\n",
    "config_path = Path(config['run_dir']) / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Initialize wandb\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "    print(\"Weights & Biases (wandb) available for logging.\")\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "    wandb = None\n",
    "    print(\"Weights & Biases (wandb) not available. Skipping wandb logging.\")\n",
    "\n",
    "if wandb_available:\n",
    "    # Configure wandb\n",
    "    wandb.init(\n",
    "        project=config['wandb_project_name'],\n",
    "        name=f\"{config['run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optional: watch model parameters\n",
    "    wandb.watch(model)\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, uhi_mean, uhi_std,\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)  # Use max_grad_norm parameter\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(log_metrics)\n",
    "        training_log.append(log_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, uhi_mean, uhi_std\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            log_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after warmup_epochs\n",
    "            if epoch >= warmup_epochs:\n",
    "                # Check for improvement (using validation loss now)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    model_save_path = model_save_dir / f\"best_model_epoch{epoch+1}.pt\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'loss': val_loss,\n",
    "                        'val_rmse': val_rmse,\n",
    "                        'val_r2': val_r2,\n",
    "                        'config': config\n",
    "                    }, model_save_path)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_loss {val_loss:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}, RMSE: {best_val_rmse:.4f}, R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
