{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd353f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/jupyter/MLC-Project/src', '/home/jupyter/MLC-Project']\n",
      "Project Root: /home/jupyter/MLC-Project\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# %% Imports and Setup\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray # For geospatial data handling with xarray\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset # Added Subset\n",
    "\n",
    "\n",
    "# --- Visualization & Progress ---\n",
    "# Optional: Import matplotlib or other plotting libs if needed for checks\n",
    "from tqdm.notebook import tqdm # Use notebook version if running interactively\n",
    "import wandb\n",
    "\n",
    "# --- Custom Modules ---\n",
    "# Project root is the parent directory of the current working directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "src = project_root / \"src\"\n",
    "\n",
    "# Add src directory to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(sys.path)\n",
    "\n",
    "# Import custom modules\n",
    "from src.ingest.dataloader_branched import CityDataSetBranched\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the utility module\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Optional: Configure logging level\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optional: Ignore specific warnings if needed\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, message='.*TypedStorage is deprecated.*')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI\" # Modified prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "# NEW: Define the common resolution for spatial features entering the model\n",
    "feature_resolution_m = 30 # Start with 10m (matches Clay/UHI grid)\n",
    "\n",
    "# Define UHI grid resolution separately (used for final target matching)\n",
    "# This assumes UHI data corresponds to 10m grid\n",
    "uhi_grid_resolution_m = 10\n",
    "\n",
    "weather_seq_length = 60\n",
    "\n",
    "# Input Data Paths (relative)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\"\n",
    "relative_dem_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dem_10m_pc.tif\"\n",
    "relative_dsm_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dsm_10m_pc.tif\"\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0\n",
    "lst_nodata = 0.0 # Or appropriate value for LST median file\n",
    "\n",
    "# --- Feature Selection Flags ---\n",
    "feature_flags = {\n",
    "    \"use_dem\": True,              # Digital Elevation Model\n",
    "    \"use_dsm\": True,              # Digital Surface Model\n",
    "    \"use_clay\": True,             # Clay feature extractor\n",
    "    \"use_sentinel_composite\": False, # Raw Sentinel-2 bands (use sentinel_bands_to_load to specify)\n",
    "    \"use_lst\": False,             # Land Surface Temperature\n",
    "    \"use_ndvi\": False,            # Normalized Difference Vegetation Index\n",
    "    \"use_ndbi\": False,            # Normalized Difference Built-up Index\n",
    "    \"use_ndwi\": False,            # Normalized Difference Water Index\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) --- #\n",
    "# If use_sentinel_composite is True and this list is empty, no sentinel bands will be added directly.\n",
    "sentinel_bands_to_load = [] # Example: [\"blue\", \"green\", \"red\"] \n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM, No separate Elev branches) ---\n",
    "# Clay Backbone (if feature_flags[\"use_clay\"] is True)\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "relative_clay_checkpoint_path = \"notebooks/clay-v1.5.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "weather_input_channels = 6\n",
    "convlstm_hidden_dims = [32, 16] # MODIFIED from [16, 8]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# Projection Layer Channels\n",
    "proj_static_ch = 8 # For projecting ALL static feats (Clay, LST, DEM, DSM, Indices, direct Sentinel)\n",
    "proj_temporal_ch = 8 # For projecting ConvLSTM output\n",
    "\n",
    "# --- Head Configuration (NEW) --- #\n",
    "head_type = \"unet\"  # Options: \"unet\", \"simple_cnn\"\n",
    "\n",
    "# U-Net Head (if head_type == \"unet\")\n",
    "unet_base_channels = 16\n",
    "unet_depth = 2 # <<< REDUCED from 3\n",
    "\n",
    "# Simple CNN Head (if head_type == \"simple_cnn\")\n",
    "simple_cnn_head_channels = [32, 16] # Example, tune as needed\n",
    "simple_cnn_head_kernels = [3, 3]      # Example, tune as needed\n",
    "simple_cnn_dropout_rate = 0.1       # Example dropout for simple_cnn head conv blocks\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 1\n",
    "epochs = 500\n",
    "lr = 1e-4 \n",
    "weight_decay = 1e-4 \n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "max_grad_norm = 1.0 \n",
    "warmup_epochs = 5 \n",
    "\n",
    "# V100 Suggestion: Target batch size 2 => n_train_batches = 47 / 1 = 47\n",
    "n_train_batches = 47\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Resolve Paths using check_path from train_utils ---\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, project_root, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, project_root, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "# Always check paths needed by dataloader, flags control usage inside\n",
    "absolute_dem_path = check_path(relative_dem_path, project_root, \"DEM TIF\")\n",
    "absolute_dsm_path = check_path(relative_dsm_path, project_root, \"DSM TIF\")\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "absolute_single_lst_median_path = check_path(relative_single_lst_median_path, project_root, \"Single LST Median\", should_exist=feature_flags[\"use_lst\"]) # Check only if flag is true\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Create Run Directory --- #\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{wander_run_name_prefix}_{run_timestamp}\"\n",
    "run_dir = output_dir_base / run_name\n",
    "run_dir.mkdir(parents=True, exist_ok=True) \n",
    "print(f\"Created run directory: {run_dir}\")\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel_CommonRes\",\n",
    "    \"project_root\": project_root_str,\n",
    "    \"run_dir\": str(run_dir), \n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m, \n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m, \n",
    "    \"weather_seq_length\": weather_seq_length,\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(absolute_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(absolute_manhattan_weather_csv),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(absolute_dem_path) if absolute_dem_path else None,\n",
    "    \"dsm_path\": str(absolute_dsm_path) if absolute_dsm_path else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "    \"single_lst_median_path\": str(absolute_single_lst_median_path) if absolute_single_lst_median_path else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config\n",
    "    \"weather_input_channels\": weather_input_channels,\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    # Head Config (NEW) \n",
    "    \"head_type\": head_type,\n",
    "    \"unet_base_channels\": unet_base_channels if head_type == \"unet\" else None,\n",
    "    \"unet_depth\": unet_depth if head_type == \"unet\" else None,\n",
    "    \"simple_cnn_head_channels\": simple_cnn_head_channels if head_type == \"simple_cnn\" else None,\n",
    "    \"simple_cnn_head_kernels\": simple_cnn_head_kernels if head_type == \"simple_cnn\" else None,\n",
    "    \"simple_cnn_dropout_rate\": simple_cnn_dropout_rate if head_type == \"simple_cnn\" else None,\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(absolute_clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(absolute_clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"max_grad_norm\": max_grad_norm, \n",
    "    \"warmup_epochs\": warmup_epochs, \n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nBranched Model Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b31ff2-f0b1-4823-93c1-b62dfb77abc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Data Loading and Preprocessing (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # Removed split_data\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = CityDataSetBranched(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"], # Corrected param name\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"], # Corrected param name\n",
    "        uhi_csv=absolute_uhi_csv, # Use absolute path resolved earlier\n",
    "        bronx_weather_csv=absolute_bronx_weather_csv,\n",
    "        manhattan_weather_csv=absolute_manhattan_weather_csv,\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"], # Corrected param name\n",
    "        dsm_path=config[\"dsm_path\"], # Corrected param name\n",
    "        elevation_nodata=config[\"elevation_nodata\"], # Corrected param name\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"], # Added missing param\n",
    "        weather_seq_length=config[\"weather_seq_length\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split --- #\n",
    "val_percent = 0.20 # Keep the percentage definition\n",
    "num_samples = len(dataset)\n",
    "if num_samples < 2: # Need at least one for train and one for val\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e90e15-1b5c-46f7-aedf-b3fe60662f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Model Initialization (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import necessary components ---\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging # Ensure logging is imported if not already\n",
    "\n",
    "# Instantiate the BranchedUHIModel\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "try:\n",
    "    model = BranchedUHIModel(\n",
    "        # --- Weather Branch Config --- #\n",
    "        weather_input_channels=config[\"weather_input_channels\"],\n",
    "        convlstm_hidden_dims=config[\"convlstm_hidden_dims\"],\n",
    "        convlstm_kernel_sizes=config[\"convlstm_kernel_sizes\"],\n",
    "        convlstm_num_layers=config[\"convlstm_num_layers\"],\n",
    "        weather_seq_length=config[\"weather_seq_length\"], # ADDED\n",
    "        # --- Static Feature Config --- #\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"), # Pass if needed\n",
    "        # Clay Specific\n",
    "        clay_model_size=config.get(\"clay_model_size\"),\n",
    "        clay_bands=config.get(\"clay_bands\"),\n",
    "        clay_platform=config.get(\"clay_platform\"),\n",
    "        clay_gsd=config.get(\"clay_gsd\"),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "        clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "        # --- Head Config --- #\n",
    "        proj_static_ch=config[\"proj_static_ch\"],\n",
    "        proj_temporal_ch=config[\"proj_temporal_ch\"],\n",
    "        # Head Type and Specific Params (NEW)\n",
    "        head_type=config[\"head_type\"],\n",
    "        unet_base_channels=config.get(\"unet_base_channels\"), # get handles if None\n",
    "        unet_depth=config.get(\"unet_depth\"),                 # get handles if None\n",
    "        simple_cnn_head_channels=config.get(\"simple_cnn_head_channels\"),\n",
    "        simple_cnn_head_kernels=config.get(\"simple_cnn_head_kernels\"),\n",
    "        simple_cnn_dropout_rate=config.get(\"simple_cnn_dropout_rate\", 0.1), # Default if not in config\n",
    "        # --- Target Grid Info --- #\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        bounds=config[\"bounds\"]\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"{config['model_type']} initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing BranchedUHIModel: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception after logging\n",
    "\n",
    "# --- Optimizer --- #\n",
    "try:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    print(\"Optimizer (AdamW) initialized.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing optimizer: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "    print(\"Loss function set to masked_mse_loss.\")\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "    print(\"Loss function set to masked_mae_loss.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "try:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "    print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing scheduler: {e}\", exc_info=True)\n",
    "    scheduler = None # Allow training without scheduler if init fails\n",
    "    print(\"Proceeding without LR scheduler due to initialization error.\")\n",
    "\n",
    "\n",
    "print(\"\\nModel, optimizer, loss function, and scheduler setup complete.\")\n",
    "# print(model) # Optional: Print model summary for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b761f-1d71-4581-bf50-c785d8e88a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Helper functions ---\n",
    "# Get the warmup epochs from config or default to 5\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "# Initialize metrics tracking\n",
    "best_val_loss = float('inf')\n",
    "best_val_rmse = float('inf')\n",
    "best_val_r2 = -float('inf')\n",
    "patience_counter = 0\n",
    "training_log = []\n",
    "\n",
    "# Create run directory\n",
    "model_save_dir = Path(config['run_dir']) / \"checkpoints\"\n",
    "\n",
    "# Save config to run directory\n",
    "config_path = Path(config['run_dir']) / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Initialize wandb\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "    print(\"Weights & Biases (wandb) available for logging.\")\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "    wandb = None\n",
    "    print(\"Weights & Biases (wandb) not available. Skipping wandb logging.\")\n",
    "\n",
    "if wandb_available:\n",
    "    # Configure wandb\n",
    "    wandb.init(\n",
    "        project=config['wandb_project_name'],\n",
    "        name=f\"{config['wander_run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optional: watch model parameters\n",
    "    wandb.watch(model)\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, uhi_mean, uhi_std,\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)  # Use max_grad_norm parameter\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(log_metrics)\n",
    "        training_log.append(log_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, uhi_mean, uhi_std\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            log_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after warmup_epochs\n",
    "            if epoch >= warmup_epochs:\n",
    "                # Check for improvement (using validation loss now)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    model_save_path = model_save_dir / f\"best_model_epoch{epoch+1}.pt\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'loss': val_loss,\n",
    "                        'val_rmse': val_rmse,\n",
    "                        'val_r2': val_r2,\n",
    "                        'config': config\n",
    "                    }, model_save_path)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_loss {val_loss:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}, RMSE: {best_val_rmse:.4f}, R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bc3f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
