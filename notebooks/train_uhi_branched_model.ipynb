{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd353f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/jupyter/MLC-Project\n",
      "PyTorch Version: 2.7.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# %% Imports and Setup\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray # For geospatial data handling with xarray\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset # Added Subset\n",
    "\n",
    "\n",
    "# --- Visualization & Progress ---\n",
    "# Optional: Import matplotlib or other plotting libs if needed for checks\n",
    "from tqdm.notebook import tqdm # Use notebook version if running interactively\n",
    "import wandb\n",
    "\n",
    "# --- Custom Modules ---\n",
    "# Project root is the parent directory of the current working directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "src = project_root / \"src\"\n",
    "\n",
    "# Add src directory to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(sys.path)\n",
    "\n",
    "# Import custom modules\n",
    "from src.ingest.dataloader_branched import CityDataSetBranched\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the utility module\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Optional: Configure logging level\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optional: Ignore specific warnings if needed\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, message='.*TypedStorage is deprecated.*')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded bounds from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "Run directory: /home/jupyter/MLC-Project/training_runs/BranchedUHIModel_NYC_unet_20250507_153649\n",
      "\\nBranched Model Configuration dictionary created:\n",
      "{\n",
      "  \"model_type\": \"BranchedUHIModel\",\n",
      "  \"project_root\": \"/home/jupyter/MLC-Project\",\n",
      "  \"city_name\": \"NYC\",\n",
      "  \"wandb_project_name\": \"MLC_UHI_Proj\",\n",
      "  \"wander_run_name_prefix\": \"NYC_BranchedUHI\",\n",
      "  \"feature_resolution_m\": 30,\n",
      "  \"uhi_grid_resolution_m\": 10,\n",
      "  \"temporal_seq_len\": 60,\n",
      "  \"clay_proj_channels\": 16,\n",
      "  \"enabled_weather_features\": [\n",
      "    \"air_temp\",\n",
      "    \"rel_humidity\",\n",
      "    \"avg_windspeed\",\n",
      "    \"wind_direction\",\n",
      "    \"solar_flux\"\n",
      "  ],\n",
      "  \"uhi_csv\": \"/home/jupyter/MLC-Project/data/NYC/uhi.csv\",\n",
      "  \"bronx_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/bronx_weather.csv\",\n",
      "  \"manhattan_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/manhattan_weather.csv\",\n",
      "  \"bounds\": [\n",
      "    -73.99445667,\n",
      "    40.75879167,\n",
      "    -73.87945833,\n",
      "    40.85949667\n",
      "  ],\n",
      "  \"feature_flags\": {\n",
      "    \"use_dem\": false,\n",
      "    \"use_dsm\": true,\n",
      "    \"use_clay\": true,\n",
      "    \"use_sentinel_composite\": false,\n",
      "    \"use_lst\": false,\n",
      "    \"use_ndvi\": false,\n",
      "    \"use_ndbi\": false,\n",
      "    \"use_ndwi\": false\n",
      "  },\n",
      "  \"sentinel_bands_to_load\": [],\n",
      "  \"dem_path\": null,\n",
      "  \"dsm_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_cop-dem-glo-30_native-resolution_pc.tif\",\n",
      "  \"elevation_nodata\": -9999.0,\n",
      "  \"cloudless_mosaic_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\",\n",
      "  \"single_lst_median_path\": null,\n",
      "  \"lst_nodata\": 0.0,\n",
      "  \"convlstm_hidden_dims\": [\n",
      "    32,\n",
      "    16\n",
      "  ],\n",
      "  \"convlstm_kernel_sizes\": [\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"convlstm_num_layers\": 2,\n",
      "  \"proj_static_ch\": 1,\n",
      "  \"proj_temporal_ch\": 16,\n",
      "  \"head_type\": \"unet\",\n",
      "  \"unet_base_channels\": 32,\n",
      "  \"unet_depth\": 2,\n",
      "  \"unet_dropout_rate\": 0.1,\n",
      "  \"simple_cnn_hidden_dims\": null,\n",
      "  \"simple_cnn_kernel_size\": null,\n",
      "  \"simple_cnn_dropout_rate\": null,\n",
      "  \"clay_model_size\": \"large\",\n",
      "  \"clay_bands\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\"\n",
      "  ],\n",
      "  \"clay_platform\": \"sentinel-2-l2a\",\n",
      "  \"clay_gsd\": 10,\n",
      "  \"freeze_backbone\": true,\n",
      "  \"clay_checkpoint_path\": \"/home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\",\n",
      "  \"clay_metadata_path\": \"/home/jupyter/MLC-Project/src/Clay/configs/metadata.yaml\",\n",
      "  \"n_train_batches\": 23,\n",
      "  \"num_workers\": 1,\n",
      "  \"epochs\": 500,\n",
      "  \"lr\": 1e-05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"loss_type\": \"mse\",\n",
      "  \"patience\": 50,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"warmup_epochs\": 5,\n",
      "  \"device\": \"cuda\",\n",
      "  \"run_dir\": \"/home/jupyter/MLC-Project/training_runs/BranchedUHIModel_NYC_unet_20250507_153649\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path # For path validation\n",
    "from src.ingest.data_utils import calculate_actual_weather_channels # For dynamic weather channels\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "# project_root is defined in the first cell\n",
    "project_root_str = str(project_root)\n",
    "data_dir = project_root / \"data\"\n",
    "city_name = \"NYC\" # Should be defined or loaded\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI\"\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "feature_resolution_m = 30\n",
    "uhi_grid_resolution_m = 10 # UHI target grid\n",
    "temporal_seq_len = 60 \n",
    "\n",
    "# --- Weather Feature Selection\n",
    "enabled_weather_features = [\n",
    "    \"air_temp\", \n",
    "    \"rel_humidity\", \n",
    "    \"avg_windspeed\", \n",
    "    \"wind_direction\",      \n",
    "    \"solar_flux\"\n",
    "]\n",
    "# Calculate the number of actual weather channels that will be produced by the dataloader\n",
    "actual_dataloader_weather_channels = calculate_actual_weather_channels(enabled_weather_features)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Define Absolute Input Data Paths Directly ---\n",
    "uhi_csv_path = data_dir / city_name / \"uhi.csv\"\n",
    "bronx_weather_csv_path = data_dir / city_name / \"bronx_weather.csv\"\n",
    "manhattan_weather_csv_path = data_dir / city_name / \"manhattan_weather.csv\"\n",
    "\n",
    "dem_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dem_nasadem_native-resolution_pc.tif\"\n",
    "dsm_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dsm_cop-dem-glo-30_native-resolution_pc.tif\"\n",
    "cloudless_mosaic_path = data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\" # Added .npy\n",
    "# Assuming the LST filename structure from download_data.ipynb if it's used\n",
    "lst_time_window_str_for_filename = \"20210601_to_20210901\" # Match download_data\n",
    "single_lst_median_path = data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_{lst_time_window_str_for_filename}.npy\" # Corrected and added .npy\n",
    "\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0 # Or np.nan\n",
    "lst_nodata = 0.0 # Or np.nan\n",
    "\n",
    "# --- Feature Selection Flags ---\n",
    "feature_flags = {\n",
    "    \"use_dem\": False,\n",
    "    \"use_dsm\": True,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False, # Set to True if LST is intended to be used\n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) ---\n",
    "sentinel_bands_to_load = []\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM, No separate Elev branches) ---\n",
    "# Clay Backbone\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True # Keep Clay backbone frozen for BranchedUHIModel typically\n",
    "clay_checkpoint_path = project_root / \"notebooks\" / \"clay-v1.5.ckpt\"\n",
    "clay_metadata_path = project_root / \"src\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "convlstm_hidden_dims = [32, 16]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# Projection Layer Channels\n",
    "clay_proj_channels = 16\n",
    "proj_static_ch = 1\n",
    "proj_temporal_ch = 16\n",
    "\n",
    "# --- Head Configuration ---\n",
    "head_type = \"unet\" # Options: \"unet\" or \"simple_cnn\"\n",
    "# U-Net specific\n",
    "unet_base_channels = 32\n",
    "unet_depth = 2\n",
    "unet_dropout_rate = 0.1 # Dropout for U-Net blocks\n",
    "# SimpleCNN specific\n",
    "simple_cnn_hidden_dims = [32, 16] # Example, adjust as needed\n",
    "simple_cnn_kernel_size = 3\n",
    "simple_cnn_dropout_rate = 0.1 # Dropout for SimpleCNN Head\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 1\n",
    "epochs = 500\n",
    "lr = 1e-5\n",
    "weight_decay = 1e-4\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "max_grad_norm = 1.0\n",
    "n_train_batches = 47\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Validate Paths (using check_path for files that *must* exist) ---\n",
    "uhi_csv_path = check_path(uhi_csv_path, project_root, \"UHI CSV\")\n",
    "bronx_weather_csv_path = check_path(bronx_weather_csv_path, project_root, \"Bronx Weather CSV\")\n",
    "manhattan_weather_csv_path = check_path(manhattan_weather_csv_path, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "if feature_flags[\"use_dem\"]:\n",
    "    dem_path = check_path(dem_path, project_root, \"DEM TIF\")\n",
    "if feature_flags[\"use_dsm\"]:\n",
    "    dsm_path = check_path(dsm_path, project_root, \"DSM TIF\")\n",
    "if feature_flags[\"use_clay\"]:\n",
    "    clay_checkpoint_path = check_path(clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "    clay_metadata_path = check_path(clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "if feature_flags[\"use_clay\"] or feature_flags[\"use_sentinel_composite\"]:\n",
    "    cloudless_mosaic_path = check_path(cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "if feature_flags[\"use_lst\"]:\n",
    "    single_lst_median_path = check_path(single_lst_median_path, project_root, \"Single LST Median\", should_exist=True)\n",
    "\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(uhi_csv_path) \n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {uhi_csv_path.name}: {bounds}\") \n",
    "\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel\", \n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m,\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m,\n",
    "    \"temporal_seq_len\": temporal_seq_len, # RENAMED\n",
    "    \"clay_proj_channels\": clay_proj_channels, # Added\n",
    "    \"enabled_weather_features\": enabled_weather_features, \n",
    "    \"uhi_csv\": str(uhi_csv_path),\n",
    "    \"bronx_weather_csv\": str(bronx_weather_csv_path),\n",
    "    \"manhattan_weather_csv\": str(manhattan_weather_csv_path),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(dem_path) if feature_flags[\"use_dem\"] else None,\n",
    "    \"dsm_path\": str(dsm_path) if feature_flags[\"use_dsm\"] else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(cloudless_mosaic_path) if feature_flags[\"use_clay\"] or feature_flags[\"use_sentinel_composite\"] else None,\n",
    "    \"single_lst_median_path\": str(single_lst_median_path) if feature_flags[\"use_lst\"] else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    \"head_type\": head_type,\n",
    "    \"unet_base_channels\": unet_base_channels if head_type == \"unet\" else None,\n",
    "    \"unet_depth\": unet_depth if head_type == \"unet\" else None,\n",
    "    \"unet_dropout_rate\": unet_dropout_rate if head_type == \"unet\" else None, # Added\n",
    "    \"simple_cnn_hidden_dims\": simple_cnn_hidden_dims if head_type == \"simple_cnn\" else None, # Changed from simple_cnn_head_channels\n",
    "    \"simple_cnn_kernel_size\": simple_cnn_kernel_size if head_type == \"simple_cnn\" else None, # Changed from simple_cnn_head_kernels\n",
    "    \"simple_cnn_dropout_rate\": simple_cnn_dropout_rate if head_type == \"simple_cnn\" else None,\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"max_grad_norm\": max_grad_norm,\n",
    "    \"warmup_epochs\": warmup_epochs,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "# --- Create Run Directory & Update Config ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name_suffix = f\"{config['model_type']}_{city_name}_{config['head_type']}_{run_timestamp}\"\n",
    "run_dir = output_dir_base / run_name_suffix\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"run_dir\"] = str(run_dir) \n",
    "\n",
    "print(f\"Run directory: {run_dir}\")\n",
    "print(\"\\\\nBranched Model Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b31ff2-f0b1-4823-93c1-b62dfb77abc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 15:36:50,111 - INFO - Dataloader configured to include previous UHI grid for autoregression (always active).\n",
      "2025-05-07 15:36:50,112 - INFO - Dataloader will produce 6 weather channels based on enabled features: ['air_temp', 'rel_humidity', 'avg_windspeed', 'wind_direction', 'solar_flux']\n",
      "2025-05-07 15:36:50,113 - INFO - Target FEATURE grid size (H, W): (373, 323) @ 30m, CRS: EPSG:4326\n",
      "2025-05-07 15:36:50,113 - INFO - Target UHI grid size (H, W): (1118, 969) @ 10m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BranchedCityDataSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 1024.84it/s]\n",
      "2025-05-07 15:36:50,192 - INFO - Loading DSM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_cop-dem-glo-30_native-resolution_pc.tif\n",
      "2025-05-07 15:36:50,199 - INFO - DSM loaded raw shape: (1, 364, 415)\n",
      "2025-05-07 15:36:50,208 - INFO - Clipping DSM to bounds: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "2025-05-07 15:36:50,209 - INFO - Opened DSM (lazy load). Native shape (approx): (1, 364, 415)\n",
      "2025-05-07 15:36:50,210 - INFO - Calculating global DSM 2nd/98th percentiles...\n",
      "2025-05-07 15:36:50,214 - INFO - Global DSM p2: 0.00, p98: 94.01\n",
      "2025-05-07 15:36:50,215 - INFO - Loading cloudless mosaic from /home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy with memory mapping\n",
      "2025-05-07 15:36:50,216 - INFO - Loaded mosaic shape (native res): (5, 1119, 1278)\n",
      "2025-05-07 15:36:50,224 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-05-07 15:36:50,224 - INFO - Loaded Manhattan weather data: 169 records\n",
      "2025-05-07 15:36:50,226 - INFO - Computed grid cell center coordinates for CRS: EPSG:4326.\n",
      "2025-05-07 15:36:50,227 - INFO - Computed grid cell center coordinates for potential weather grid building at feature resolution.\n",
      "2025-05-07 15:36:50,228 - INFO - Dataset initialized for NYC with 59 unique timestamps.\n",
      "2025-05-07 15:36:50,228 - INFO - Temporal sequence length T = 60\n",
      "2025-05-07 15:36:50,229 - INFO - Enabled features (flags): {\"use_dem\": false, \"use_dsm\": true, \"use_clay\": true, \"use_sentinel_composite\": false, \"use_lst\": false, \"use_ndvi\": false, \"use_ndbi\": false, \"use_ndwi\": false}\n",
      "2025-05-07 15:36:50,230 - INFO - DEM loaded: False\n",
      "2025-05-07 15:36:50,230 - INFO - DSM loaded: True\n",
      "2025-05-07 15:36:50,231 - INFO - LST loaded: False\n",
      "2025-05-07 15:36:50,232 - INFO - Mosaic loaded: True\n",
      "2025-05-07 15:36:50,233 - INFO - Calculating UHI statistics from training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential dataset split: 47 training (indices 0-46), 12 validation (indices 47-58) samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stats: 100%|██████████| 47/47 [00:00<00:00, 2184.92it/s]\n",
      "2025-05-07 15:36:50,260 - INFO - Training UHI Mean: 1.0006, Std Dev: 0.0168\n",
      "2025-05-07 15:36:50,261 - INFO - Creating dataloaders...\n",
      "2025-05-07 15:36:50,262 - INFO - Using Train Batch Size: 2\n",
      "2025-05-07 15:36:50,263 - INFO - Using Validation Batch Size: 1\n",
      "2025-05-07 15:36:50,263 - INFO - Data loading setup complete.\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# %% Data Loading and Preprocessing (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # Removed split_data\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = CityDataSetBranched(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"], # Corrected param name\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"], # Corrected param name\n",
    "        uhi_csv=config[\"uhi_csv\"], # Use paths from config\n",
    "        bronx_weather_csv=config[\"bronx_weather_csv\"],\n",
    "        manhattan_weather_csv=config[\"manhattan_weather_csv\"],\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], # NEW: Pass from config\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"], # Corrected param name\n",
    "        dsm_path=config[\"dsm_path\"], # Corrected param name\n",
    "        elevation_nodata=config[\"elevation_nodata\"], # Corrected param name\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"], # Added missing param\n",
    "        temporal_seq_len=config[\"temporal_seq_len\"], # RENAMED from weather_seq_length\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "        # use_autoregressive_uhi is removed here as it's always active in the dataloader now\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split --- #\n",
    "val_percent = 0.20 # Keep the percentage definition\n",
    "num_samples = len(dataset)\n",
    "if num_samples < 2: # Need at least one for train and one for val\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e90e15-1b5c-46f7-aedf-b3fe60662f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 15:37:57,014 - INFO - BranchedModel configured for target output UHI grid: (1118, 969)\n",
      "2025-05-07 15:37:57,021 - INFO - ConvLSTM input dimension set to 7 (Weather: 6 + Prev UHI: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BranchedUHIModel...\n",
      "Manually loading checkpoint: /home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\n",
      "Instantiating ClayMAEModule manually...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 15:38:02,843 - INFO - Loading pretrained weights from Hugging Face hub (timm/vit_large_patch14_reg4_dinov2.lvd142m)\n",
      "2025-05-07 15:38:03,175 - INFO - [timm/vit_large_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state_dict manually into self.model.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 15:38:06,239 - INFO - Identified final encoder layer as self.model.model.proj\n",
      "2025-05-07 15:38:06,240 - INFO - Keeping Clay backbone frozen.\n",
      "2025-05-07 15:38:06,245 - INFO - ClayFeatureExtractor output channels set to: 1024\n",
      "2025-05-07 15:38:06,249 - INFO - Added BatchNorm2d before Clay projection for 1024 channels\n",
      "2025-05-07 15:38:06,250 - INFO - Added Clay projection Conv1x1: 1024 -> 16 channels\n",
      "2025-05-07 15:38:06,251 - INFO - Total input channels for STATIC projection: 1\n",
      "2025-05-07 15:38:06,251 - INFO - ConvLSTM input channels (actual_weather_input_channels): 6\n",
      "2025-05-07 15:38:06,253 - INFO - Static projection: 1 -> 1 channels\n",
      "2025-05-07 15:38:06,254 - INFO - Temporal projection: 16 -> 16 channels\n",
      "2025-05-07 15:38:06,255 - INFO - Adding clay_output_channels (16) to head input channels\n",
      "2025-05-07 15:38:06,262 - INFO - Initialized UNetDecoder. In channels: 33, Base channels: 32, Depth: 2\n",
      "2025-05-07 15:38:06,262 - INFO - BranchedModel using UNetDecoder head. Output channels: 32\n",
      "2025-05-07 15:38:06,264 - INFO - Initialized FinalUpsamplerAndProjection: Bicubic upsampling. InCh=32, Target=(1118,969).\n",
      "2025-05-07 15:38:06,264 - INFO - BranchedUHIModel initialized with unet head and FinalUpsamplerAndProjection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Overriding patch size from hparams. Using fixed patch_size = 16\n",
      "Clay model properties: model_size=large, embed_dim=1024, patch_size=16 (patch_size OVERRIDDEN)\n",
      "Normalization prepared for bands: ['blue', 'green', 'red', 'nir']\n",
      "BranchedUHIModel initialized successfully.\n",
      "Optimizer (AdamW) initialized.\n",
      "Loss function set to masked_mse_loss.\n",
      "Initialized ReduceLROnPlateau scheduler.\n",
      "\\nModel, optimizer, loss function, and scheduler setup complete.\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# %% Model Initialization (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import necessary components ---\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging # Ensure logging is imported if not already\n",
    "\n",
    "# Instantiate the BranchedUHIModel\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "try:\n",
    "    model = BranchedUHIModel(\n",
    "        # --- Weather Branch Config --- #\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], \n",
    "        convlstm_hidden_dims=config[\"convlstm_hidden_dims\"],\n",
    "        convlstm_kernel_sizes=config[\"convlstm_kernel_sizes\"],\n",
    "        convlstm_num_layers=config[\"convlstm_num_layers\"],\n",
    "        temporal_seq_len=config[\"temporal_seq_len\"], # RENAMED from weather_seq_length\n",
    "        # --- Static Feature Config --- #\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        clay_proj_channels=config[\"clay_proj_channels\"], # Added\n",
    "        sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"), \n",
    "        # Clay Specific\n",
    "        clay_model_size=config.get(\"clay_model_size\"),\n",
    "        clay_bands=config.get(\"clay_bands\"),\n",
    "        clay_platform=config.get(\"clay_platform\"),\n",
    "        clay_gsd=config.get(\"clay_gsd\"),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "        clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "        # --- Head Config --- #\n",
    "        proj_static_ch=config[\"proj_static_ch\"],\n",
    "        proj_temporal_ch=config[\"proj_temporal_ch\"],\n",
    "        head_type=config[\"head_type\"],\n",
    "        unet_base_channels=config.get(\"unet_base_channels\"), \n",
    "        unet_depth=config.get(\"unet_depth\"),\n",
    "        unet_dropout_rate=config.get(\"unet_dropout_rate\"), # Added\n",
    "        simple_cnn_hidden_dims=config.get(\"simple_cnn_hidden_dims\"), # Changed from simple_cnn_head_channels\n",
    "        simple_cnn_kernel_size=config.get(\"simple_cnn_kernel_size\"), # Changed from simple_cnn_head_kernels\n",
    "        simple_cnn_dropout_rate=config.get(\"simple_cnn_dropout_rate\", 0.1), \n",
    "        # --- Target Grid Info --- #\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        bounds=config[\"bounds\"]\n",
    "        # Autoregressive flags are not needed here; the model implicitly handles the +1 input dim now\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"{config['model_type']} initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing BranchedUHIModel: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception after logging\n",
    "\n",
    "# --- Optimizer --- #\n",
    "try:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    print(\"Optimizer (AdamW) initialized.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing optimizer: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "    print(\"Loss function set to masked_mse_loss.\")\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "    print(\"Loss function set to masked_mae_loss.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "try:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=50, factor=0.5)\n",
    "    print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing scheduler: {e}\", exc_info=True)\n",
    "    scheduler = None \n",
    "    print(\"Proceeding without LR scheduler due to initialization error.\")\n",
    "\n",
    "\n",
    "print(\"\\\\nModel, optimizer, loss function, and scheduler setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1b761f-1d71-4581-bf50-c785d8e88a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biases (wandb) available for logging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marnava1304\u001b[0m (\u001b[33marnava1304-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/MLC-Project/notebooks/wandb/run-20250507_153821-1a931bue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/1a931bue' target=\"_blank\">NYC_BranchedUHI_20250507_1538</a></strong> to <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/1a931bue' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/1a931bue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 500 epochs with patience 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/24 [00:00<?, ?it/s]2025-05-07 15:38:26,685 - ERROR - Model forward pass failed (Other Error) on batch 0. Input Shapes - Temporal: torch.Size([2, 60, 7, 373, 323]), Others: {'static_features': torch.Size([2, 1, 373, 323]), 'clay_mosaic': torch.Size([2, 4, 373, 323]), 'norm_latlon': torch.Size([2, 4]), 'norm_timestamp': torch.Size([2, 4])}. Error: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 44.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 20.98 GiB is allocated by PyTorch, and 723.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 44.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.00 GiB is allocated by PyTorch, and 704.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NYC_BranchedUHI_20250507_1538</strong> at: <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/1a931bue' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj/runs/1a931bue</a><br> View project at: <a href='https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj' target=\"_blank\">https://wandb.ai/arnava1304-columbia-university/MLC_UHI_Proj</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250507_153821-1a931bue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 44.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.00 GiB is allocated by PyTorch, and 704.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# --- Train --- #\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_loader:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Use generic train function from train_utils\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     train_loss, train_rmse, train_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch_generic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43muhi_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muhi_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43muhi_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muhi_std\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_flags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_grad_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train R2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_r2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(train_loss):\n",
      "File \u001b[0;32m~/MLC-Project/src/train/train_utils.py:351\u001b[0m, in \u001b[0;36mtrain_epoch_generic\u001b[0;34m(model, dataloader, optimizer, loss_fn, device, uhi_mean, uhi_std, feature_flags, max_grad_norm, desc)\u001b[0m\n\u001b[1;32m    349\u001b[0m model_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_temporal_seq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m      model_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_temporal_seq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43minput_temporal_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m      weather_input \u001b[38;5;241m=\u001b[39m model_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_temporal_seq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# Use this as primary\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weather_seq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m      \u001b[38;5;66;03m# Fallback for non-autoregressive branched?\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 44.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.00 GiB is allocated by PyTorch, and 704.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# %% Training Loop (Branched Model)\n",
    "\n",
    "# --- Helper functions ---\n",
    "# Get the warmup epochs from config or default to 5\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "# Initialize metrics tracking\n",
    "best_val_loss = float('inf')\n",
    "best_val_r2 = -float('inf')\n",
    "patience_counter = 0\n",
    "training_log = []\n",
    "\n",
    "# Create run directory\n",
    "model_save_dir = Path(config['run_dir']) / \"checkpoints\"\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config to run directory\n",
    "config_path = Path(config['run_dir']) / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x)\n",
    "\n",
    "# Initialize wandb\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "    print(\"Weights & Biases (wandb) available for logging.\")\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "    wandb = None\n",
    "    print(\"Weights & Biases (wandb) not available. Skipping wandb logging.\")\n",
    "\n",
    "if wandb_available:\n",
    "    # Configure wandb\n",
    "    wandb.init(\n",
    "        project=config['wandb_project_name'],\n",
    "        name=f\"{config['wander_run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optional: watch model parameters\n",
    "    wandb.watch(model)\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, \n",
    "                uhi_mean=config['uhi_mean'], \n",
    "                uhi_std=config['uhi_std'],\n",
    "                feature_flags=config['feature_flags'],\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(log_metrics)\n",
    "        training_log.append(log_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, \n",
    "                uhi_mean=config['uhi_mean'], \n",
    "                uhi_std=config['uhi_std'],\n",
    "                feature_flags=config['feature_flags']\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            log_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after warmup_epochs\n",
    "            if epoch >= warmup_epochs:\n",
    "                # Check for improvement (using validation loss now)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    train_utils.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "                        'loss': val_loss,\n",
    "                        'val_rmse': val_rmse,\n",
    "                        'val_r2': val_r2,\n",
    "                        'config': config\n",
    "                    }, is_best=True, output_dir=model_save_dir)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_loss {val_loss:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Warmup epoch {epoch+1}/{warmup_epochs}. Skipping checkpointing and early stopping.\")\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}, Best R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bc3f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
