{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900e5ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Setup & Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import shutil # For checkpoint saving\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"Train_UHI_batched\" \n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(os.getcwd()).parent # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- Import Model Components ---\n",
    "from src.branched_uhi_model import BranchedUHIModel # Import the branched model\n",
    "from src.ingest.dataloader_branched import CityDataSetBranched # Import the corresponding dataloader\n",
    "\n",
    "# --- Import Training Utilities & Loss ---\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the new utility module\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optionally import wandb if needed\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Import necessary metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310531aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded bounds from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "\n",
      "Branched Model (Common Res) Configuration dictionary created:\n",
      "{\n",
      "  \"model_type\": \"BranchedUHIModel_CommonRes\",\n",
      "  \"project_root\": \"/home/jupyter/MLC-Project\",\n",
      "  \"city_name\": \"NYC\",\n",
      "  \"wandb_project_name\": \"MLC_UHI_Proj\",\n",
      "  \"wander_run_name_prefix\": \"NYC_BranchedUHI_CommonRes\",\n",
      "  \"feature_resolution_m\": 30,\n",
      "  \"uhi_grid_resolution_m\": 10,\n",
      "  \"weather_seq_length\": 60,\n",
      "  \"uhi_csv\": \"data/NYC/uhi.csv\",\n",
      "  \"bronx_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/bronx_weather.csv\",\n",
      "  \"manhattan_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/manhattan_weather.csv\",\n",
      "  \"bounds\": [\n",
      "    -73.99445667,\n",
      "    40.75879167,\n",
      "    -73.87945833,\n",
      "    40.85949667\n",
      "  ],\n",
      "  \"feature_flags\": {\n",
      "    \"use_dem\": true,\n",
      "    \"use_dsm\": true,\n",
      "    \"use_clay\": true,\n",
      "    \"use_sentinel_composite\": false,\n",
      "    \"use_lst\": false,\n",
      "    \"use_ndvi\": false,\n",
      "    \"use_ndbi\": false,\n",
      "    \"use_ndwi\": false\n",
      "  },\n",
      "  \"sentinel_bands_to_load\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\",\n",
      "    \"swir16\",\n",
      "    \"swir22\"\n",
      "  ],\n",
      "  \"dem_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_1m_pc.tif\",\n",
      "  \"dsm_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_1m_pc.tif\",\n",
      "  \"elevation_nodata\": -9999.0,\n",
      "  \"cloudless_mosaic_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\",\n",
      "  \"single_lst_median_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/lst_NYC_median_20210601_to_20210901.npy\",\n",
      "  \"lst_nodata\": 0.0,\n",
      "  \"weather_input_channels\": 6,\n",
      "  \"convlstm_hidden_dims\": [\n",
      "    16,\n",
      "    8\n",
      "  ],\n",
      "  \"convlstm_kernel_sizes\": [\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"convlstm_num_layers\": 2,\n",
      "  \"proj_static_ch\": 8,\n",
      "  \"proj_temporal_ch\": 8,\n",
      "  \"unet_base_channels\": 16,\n",
      "  \"unet_depth\": 3,\n",
      "  \"clay_model_size\": \"large\",\n",
      "  \"clay_bands\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\"\n",
      "  ],\n",
      "  \"clay_platform\": \"sentinel-2-l2a\",\n",
      "  \"clay_gsd\": 10,\n",
      "  \"freeze_backbone\": true,\n",
      "  \"clay_checkpoint_path\": \"/home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\",\n",
      "  \"clay_metadata_path\": \"/home/jupyter/MLC-Project/src/Clay/configs/metadata.yaml\",\n",
      "  \"n_train_batches\": 47,\n",
      "  \"num_workers\": 1,\n",
      "  \"epochs\": 500,\n",
      "  \"lr\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"loss_type\": \"mse\",\n",
      "  \"patience\": 50,\n",
      "  \"device\": \"cuda\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI_CommonRes\" # Modified prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "# NEW: Define the common resolution for spatial features entering the model\n",
    "feature_resolution_m = 30 # Start with 10m (matches Clay/UHI grid)\n",
    "\n",
    "# Define UHI grid resolution separately (used for final target matching)\n",
    "# This assumes UHI data corresponds to 10m grid\n",
    "uhi_grid_resolution_m = 10\n",
    "\n",
    "weather_seq_length = 60\n",
    "\n",
    "# Input Data Paths (relative)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\"\n",
    "relative_dem_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dem_1m_pc.tif\"\n",
    "relative_dsm_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dsm_1m_pc.tif\"\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0\n",
    "lst_nodata = 0.0 # Or appropriate value for LST median file\n",
    "\n",
    "# --- Feature Selection Flags ---\n",
    "# IMPORTANT: Only enable the features you want to use\n",
    "# The Model will expect exactly these features\n",
    "feature_flags = {\n",
    "    \"use_dem\": True,              # Digital Elevation Model\n",
    "    \"use_dsm\": True,              # Digital Surface Model\n",
    "    \"use_clay\": True,             # Clay feature extractor\n",
    "    \"use_sentinel_composite\": False, # Raw Sentinel-2 bands\n",
    "    \"use_lst\": False,             # Land Surface Temperature\n",
    "    \"use_ndvi\": False,            # Normalized Difference Vegetation Index\n",
    "    \"use_ndbi\": False,            # Normalized Difference Built-up Index\n",
    "    \"use_ndwi\": False,            # Normalized Difference Water Index\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM, No separate Elev branches) ---\n",
    "\n",
    "# Clay Backbone (if feature_flags[\"use_clay\"] is True)\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "relative_clay_checkpoint_path = \"notebooks/clay-v1.5.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "weather_input_channels = 6\n",
    "convlstm_hidden_dims = [16, 8]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# --- REMOVED High-Res Elevation Branch Config ---\n",
    "\n",
    "# U-Net Head\n",
    "unet_base_channels = 16\n",
    "unet_depth = 2 # <<< REDUCED from 3\n",
    "\n",
    "# Projection Layer Channels\n",
    "proj_static_ch = 8 # For projecting ALL static feats (Clay, LST, DEM, DSM, Indices)\n",
    "proj_temporal_ch = 8 # For projecting ConvLSTM output\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 1\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "\n",
    "# V100 Suggestion: Target batch size 2 => n_train_batches = 47 / 1 = 47\n",
    "n_train_batches = 47\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Resolve Paths using check_path from train_utils ---\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, project_root, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, project_root, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "# Always check paths needed by dataloader, flags control usage inside\n",
    "absolute_dem_path = check_path(relative_dem_path, project_root, \"DEM TIF\")\n",
    "absolute_dsm_path = check_path(relative_dsm_path, project_root, \"DSM TIF\")\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "absolute_single_lst_median_path = check_path(relative_single_lst_median_path, project_root, \"Single LST Median\", should_exist=feature_flags[\"use_lst\"]) # Check only if flag is true\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel_CommonRes\",\n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m, # NEW\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m, # For reference\n",
    "    \"weather_seq_length\": weather_seq_length,\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(absolute_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(absolute_manhattan_weather_csv),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(absolute_dem_path) if absolute_dem_path else None,\n",
    "    \"dsm_path\": str(absolute_dsm_path) if absolute_dsm_path else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "    \"single_lst_median_path\": str(absolute_single_lst_median_path) if absolute_single_lst_median_path else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config\n",
    "    \"weather_input_channels\": weather_input_channels,\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    \"unet_base_channels\": unet_base_channels,\n",
    "    \"unet_depth\": unet_depth, # Updated\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(absolute_clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(absolute_clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nBranched Model (Common Res) Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 01:27:59,987 - INFO - Target FEATURE grid size (H, W): (373, 323) @ 30m, CRS: EPSG:4326\n",
      "2025-05-06 01:27:59,988 - INFO - Target UHI grid size (H, W): (1118, 969) @ 10m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BranchedCityDataSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 369.89it/s]\n",
      "2025-05-06 01:28:00,189 - INFO - Loading DEM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_1m_pc.tif\n",
      "2025-05-06 01:28:03,616 - INFO - Clipping DEM to bounds: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "2025-05-06 01:28:03,617 - INFO - Opened DEM (lazy load). Native shape (approx): (1, 10071, 11501)\n",
      "2025-05-06 01:28:03,618 - INFO - Loading DSM from: /home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dsm_1m_pc.tif\n"
     ]
    }
   ],
   "source": [
    "# %% Data Loading and Preprocessing (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # Removed split_data\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = CityDataSetBranched(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"], # Corrected param name\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"], # Corrected param name\n",
    "        uhi_csv=absolute_uhi_csv, # Use absolute path resolved earlier\n",
    "        bronx_weather_csv=absolute_bronx_weather_csv,\n",
    "        manhattan_weather_csv=absolute_manhattan_weather_csv,\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"], # Corrected param name\n",
    "        dsm_path=config[\"dsm_path\"], # Corrected param name\n",
    "        elevation_nodata=config[\"elevation_nodata\"], # Corrected param name\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"], # Added missing param\n",
    "        weather_seq_length=config[\"weather_seq_length\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split --- #\n",
    "val_percent = 0.20 # Keep the percentage definition\n",
    "num_samples = len(dataset)\n",
    "if num_samples < 2: # Need at least one for train and one for val\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e34b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Model Initialization (Branched Model + Common Resampling)\n",
    "\n",
    "# Instantiate the BranchedUHIModel\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "model = BranchedUHIModel(\n",
    "    # --- Weather Branch Config --- #\n",
    "    weather_input_channels=config[\"weather_input_channels\"],\n",
    "    convlstm_hidden_dims=config[\"convlstm_hidden_dims\"],\n",
    "    convlstm_kernel_sizes=config[\"convlstm_kernel_sizes\"],\n",
    "    convlstm_num_layers=config[\"convlstm_num_layers\"],\n",
    "    # --- Static Feature Config --- #\n",
    "    feature_flags=config[\"feature_flags\"],\n",
    "    sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"), # Pass if needed\n",
    "    # Clay Specific\n",
    "    clay_model_size=config.get(\"clay_model_size\"),\n",
    "    clay_bands=config.get(\"clay_bands\"),\n",
    "    clay_platform=config.get(\"clay_platform\"),\n",
    "    clay_gsd=config.get(\"clay_gsd\"),\n",
    "    freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "    clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "    clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "    # --- Head Config --- #\n",
    "    proj_static_ch=config[\"proj_static_ch\"],\n",
    "    proj_temporal_ch=config[\"proj_temporal_ch\"],\n",
    "    unet_base_channels=config[\"unet_base_channels\"],\n",
    "    unet_depth=config[\"unet_depth\"],\n",
    "    # --- Target Grid Info (NEW) --- #\n",
    "    uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "    bounds=config[\"bounds\"]\n",
    ")\n",
    "\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "# Add ReduceLROnPlateau scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "\n",
    "print(\"Model, optimizer, loss function, and scheduler initialized.\")\n",
    "# print(model) # Optional: Print model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba5ca8",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17da99",
   "metadata": {},
   "source": [
    "## Setup DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bc3f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b173ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Training Loop (Generic - Use for both CNN and Branched)\n",
    "\n",
    "# --- Imports ---\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "# from torch.cuda.amp import GradScaler # Removed GradScaler import\n",
    "import src.train.train_utils as train_utils # Import the full module\n",
    "import numpy as np # Added for isnan check\n",
    "import pandas as pd # For saving log\n",
    "\n",
    "# --- Setup ---\n",
    "print(f\"Model {config['model_type']} initialized on {device}\")\n",
    "\n",
    "# --- Optimizer and Loss (should be initialized in model setup cell) ---\n",
    "# Ensure optimizer and loss_fn are accessible from the previous cell's scope\n",
    "if 'optimizer' not in locals() or 'loss_fn' not in locals():\n",
    "    raise NameError(\"Optimizer or loss_fn not defined. Run the model initialization cell.\")\n",
    "# Ensure scheduler is accessible\n",
    "if 'scheduler' not in locals():\n",
    "    raise NameError(\"Scheduler not defined. Run the model initialization cell.\")\n",
    "\n",
    "# --- AMP GradScaler --- #\n",
    "# scaler = GradScaler() # Removed scaler initialization\n",
    "\n",
    "# --- Tracking Variables --- #\n",
    "best_val_loss = float('inf') # Using validation loss for checkpointing\n",
    "epochs_no_improve = 0\n",
    "last_saved_epoch = -1 # Track last saved epoch\n",
    "\n",
    "# --- Output Directory & Run Name (should be set in config cell) --- #\n",
    "# Ensure output_dir is a Path object if loaded from config str\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{config.get('wander_run_name_prefix', 'train')}_{run_timestamp}\"\n",
    "output_dir_base = Path(config.get('project_root', '.')) / \"training_runs\"\n",
    "output_dir = output_dir_base / run_name\n",
    "output_dir = Path(output_dir)\n",
    "config[\"output_dir\"] = output_dir\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# --- Retrieve UHI Stats from Config --- #\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean/uhi_std not in config. Run data loading cell.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f}\")\n",
    "\n",
    "# --- WANDB Init --- #\n",
    "if wandb:\n",
    "    try:\n",
    "        if wandb.run is not None: wandb.finish() # Finish previous run if any\n",
    "        wandb.init(\n",
    "            project=config[\"wandb_project_name\"],\n",
    "            name=run_name,\n",
    "            config=config\n",
    "        )\n",
    "        print(f\"Wandb initialized for run: {run_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization failed: {e}\")\n",
    "        wandb = None\n",
    "else:\n",
    "    print(\"Wandb not available, skipping logging.\")\n",
    "\n",
    "# --- Training Loop --- # \n",
    "print(f\"Starting {config['model_type']} training...\")\n",
    "training_start_time = time.time()\n",
    "training_log = [] # Local log\n",
    "epoch = -1 # Initialize epoch counter before the loop\n",
    "start_checkpointing_epoch = 50 # <<< NEW: Epoch to start saving best model and early stopping\n",
    "\n",
    "try:\n",
    "    # Use epoch range from config\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "\n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils (without scaler)\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, uhi_mean, uhi_std # Removed scaler\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(log_metrics)\n",
    "        training_log.append(log_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, uhi_mean, uhi_std\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            log_metrics.update(val_metrics) # Add val metrics for local log\n",
    "            if wandb:\n",
    "                wandb.log({\"epoch\": epoch + 1, **val_metrics}) # Log validation metrics too\n",
    "\n",
    "            # --- Step the scheduler --- #\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # --- Checkpointing & Early Stopping (after start_checkpointing_epoch) --- #\n",
    "            if epoch + 1 > start_checkpointing_epoch:\n",
    "                is_best = val_loss < best_val_loss\n",
    "                if is_best:\n",
    "                    print(f\"Validation Loss improved from {best_val_loss:.4f} to {val_loss:.4f}\")\n",
    "                    best_val_loss = val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    last_saved_epoch = epoch + 1\n",
    "                    # Use save_checkpoint from train_utils\n",
    "                    train_utils.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'best_val_loss': best_val_loss, # Save best loss\n",
    "                        'config': config # Save full config dict\n",
    "                    }, is_best=True, output_dir=output_dir)\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"Validation Loss did not improve ({val_loss:.4f}). Best: {best_val_loss:.4f}. No improvement for {epochs_no_improve} epochs.\")\n",
    "\n",
    "                if epochs_no_improve >= config['patience']:\n",
    "                    print(f\"Early stopping triggered after {epochs_no_improve} epochs without validation loss improvement.\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} <= {start_checkpointing_epoch}, skipping best model check/save.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping validation/checkpointing: val_loader is None.\")\n",
    "            # Always save last checkpoint if no validation\n",
    "            train_utils.save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss, # Save current best loss seen\n",
    "                'config': config\n",
    "            }, is_best=False, output_dir=output_dir, filename='checkpoint_last.pth.tar')\n",
    "            last_saved_epoch = epoch + 1 # Update tracker\n",
    "\n",
    "        # --- Epoch Timing --- #\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1} duration: {epoch_duration:.2f} seconds\")\n",
    "        if wandb:\n",
    "            wandb.log({\"epoch\": epoch + 1, \"epoch_duration_sec\": epoch_duration})\n",
    "\n",
    "finally:\n",
    "    # --- End Training Actions (Executed even if loop breaks early) --- #\n",
    "    training_duration = time.time() - training_start_time\n",
    "    print(f\"\\nTotal training time: {training_duration / 60:.2f} minutes\")\n",
    "\n",
    "    # --- Save Final Checkpoint --- #\n",
    "    # Use the state from the last *completed* epoch (before potential break)\n",
    "    final_epoch_num = epoch + 1 # This will be correct whether the loop finished or broke\n",
    "    print(f\"Saving final model state from epoch {final_epoch_num}...\")\n",
    "    try:\n",
    "        train_utils.save_checkpoint({\n",
    "            'epoch': final_epoch_num,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': config\n",
    "        }, is_best=False, output_dir=output_dir, filename='checkpoint_final.pth.tar')\n",
    "        print(f\"Final checkpoint saved to {output_dir / 'checkpoint_final.pth.tar'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final checkpoint: {e}\")\n",
    "\n",
    "    # --- Save Local Training Log --- #\n",
    "    if training_log:\n",
    "        try:\n",
    "            log_df = pd.DataFrame(training_log)\n",
    "            log_df.to_csv(output_dir / 'training_log.csv', index=False)\n",
    "            print(f\"Saved local training log to {output_dir / 'training_log.csv'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save local training log: {e}\")\n",
    "    else:\n",
    "        print(\"No training log data to save.\")\n",
    "\n",
    "\n",
    "    if wandb and wandb.run: # Check if wandb run exists before logging/finishing\n",
    "        wandb.log({\"total_training_time_min\": training_duration / 60})\n",
    "        wandb.finish()\n",
    "        print(\"W&B run finished.\")\n",
    "\n",
    "    print(\"Training loop finished.\")\n",
    "    if val_loader:\n",
    "        print(f\"Best validation Loss recorded: {best_val_loss:.4f} (achieved at epoch {last_saved_epoch if last_saved_epoch > 0 else 'N/A'})\")\n",
    "    if last_saved_epoch > 0:\n",
    "         print(f\"Best model checkpoint saved in: {output_dir / 'model_best.pth.tar'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120f7ff-644a-4eed-a6d8-d802e39a8896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
