{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "import math\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# --- WANDB --- #\n",
    "import wandb\n",
    "# ------------ #\n",
    "\n",
    "# --- Metrics --- \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# ------------ #\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- UPDATED IMPORTS for Branched Model --- #\n",
    "from src.branched_uhi_model import BranchedUHIModel # NEW MODEL\n",
    "from src.ingest.dataloader_branched import BranchedCityDataSet # NEW DATALOADER\n",
    "# ------------------------------------------ #\n",
    "\n",
    "from src.train.loss import masked_mae_loss, masked_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT ON FIRST RUN\n",
    "#!wget -q https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310531aa",
   "metadata": {},
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM)\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI_ConvLSTM\" # Modified prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "resolution_m = 10\n",
    "weather_seq_length = 60 # <<< Explicitly set to 60\n",
    "\n",
    "# Input Data Paths (relative to project root for portability in config)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\"\n",
    "\n",
    "# --- CORRECTED DEM/DSM Paths (Relative) --- #\n",
    "relative_dem_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dem_1ft_2017.tif\"\n",
    "relative_dsm_path = relative_data_dir / city_name / \"sat_files\" / \"nyc_dsm_1ft_2017.tif\"\n",
    "\n",
    "# --- Cloudless Mosaic / LST Paths (Example) ---\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "\n",
    "# --- Feature Selection Flags --- #\n",
    "feature_flags = {\n",
    "    \"use_dsm\": True,\n",
    "    \"use_dem\": True,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False,\n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM) ---\n",
    "\n",
    "# Clay Backbone (if feature_flags[\"use_clay\"] is True)\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "relative_clay_checkpoint_path = \"notebooks/clay-v1.5.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM) - NEW PARAMS\n",
    "weather_input_channels = 6 # From dataloader (temp, humidity, wind_speed, wind_sin, wind_cos, solar)\n",
    "convlstm_hidden_dims = [64, 32]  # Example: 2 layers with 64 then 32 hidden channels\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)] # Kernel size for each layer (must match len hidden_dims)\n",
    "convlstm_num_layers = len(convlstm_hidden_dims) # Number of layers\n",
    "\n",
    "# U-Net Head (No changes here)\n",
    "unet_base_channels = 64\n",
    "unet_depth = 4\n",
    "\n",
    "# Projection Layer Channels (for fusion)\n",
    "proj_static_ch = 32\n",
    "proj_temporal_ch = 32 # For projecting ConvLSTM output\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 4\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "n_train_batches = 8\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Sanity Checks and Absolute Paths ---\n",
    "# (Check path function is unchanged)\n",
    "def check_path(relative_path, description, should_exist=True):\n",
    "    abs_path = project_root / relative_path\n",
    "    if should_exist and not abs_path.exists():\n",
    "        raise FileNotFoundError(f\"{description} not found at {abs_path}. Ensure download_data.ipynb was run.\")\n",
    "    return abs_path\n",
    "\n",
    "# (Path checks are unchanged)\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, \"Manhattan Weather CSV\")\n",
    "absolute_dem_path = check_path(relative_dem_path, \"DEM TIF\") if feature_flags[\"use_dem\"] else None\n",
    "absolute_dsm_path = check_path(relative_dsm_path, \"DSM TIF\") if feature_flags[\"use_dsm\"] else None\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, \"Clay Checkpoint\") if feature_flags[\"use_clay\"] else None\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, \"Clay Metadata\", should_exist=feature_flags[\"use_clay\"]) if feature_flags[\"use_clay\"] else None\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, \"Cloudless Mosaic\") if feature_flags[\"use_sentinel_composite\"] or feature_flags[\"use_clay\"] or feature_flags[\"use_ndvi\"] or feature_flags[\"use_ndbi\"] or feature_flags[\"use_ndwi\"] else None\n",
    "absolute_single_lst_median_path = check_path(relative_single_lst_median_path, \"Single LST Median\") if feature_flags[\"use_lst\"] else None\n",
    "\n",
    "# --- Calculate Bounds (Unchanged) ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Central Config Dictionary (Updated for ConvLSTM) --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel_ConvLSTM\",\n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"resolution_m\": resolution_m,\n",
    "    \"weather_seq_length\": weather_seq_length, # Added\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(absolute_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(absolute_manhattan_weather_csv),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(absolute_dem_path) if absolute_dem_path else None,\n",
    "    \"dsm_path\": str(absolute_dsm_path) if absolute_dsm_path else None,\n",
    "    \"cloudless_mosaic_path\": str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "    \"single_lst_median_path\": str(absolute_single_lst_median_path) if absolute_single_lst_median_path else None,\n",
    "    # \"elevation_nodata\": None, # Removed, handled by data_utils\n",
    "    # Model Config (ConvLSTM)\n",
    "    \"weather_input_channels\": weather_input_channels,\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims, # Added\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes, # Added\n",
    "    \"convlstm_num_layers\": convlstm_num_layers, # Added\n",
    "    \"proj_static_ch\": proj_static_ch, # Added\n",
    "    \"proj_temporal_ch\": proj_temporal_ch, # Added\n",
    "    \"unet_base_channels\": unet_base_channels, # Added\n",
    "    \"unet_depth\": unet_depth, # Added\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(absolute_clay_checkpoint_path) if absolute_clay_checkpoint_path else None,\n",
    "    \"clay_metadata_path\": str(absolute_clay_metadata_path) if absolute_clay_metadata_path else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nBranched Model (ConvLSTM) Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, Path) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Setup DataLoader using BranchedCityDataSet\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    # Use BranchedCityDataSet\n",
    "    dataset = BranchedCityDataSet(\n",
    "        bounds=config['bounds'],\n",
    "        resolution_m=config['resolution_m'],\n",
    "        uhi_csv=absolute_uhi_csv,\n",
    "        bronx_weather_csv=str(absolute_bronx_weather_csv),\n",
    "        manhattan_weather_csv=str(absolute_manhattan_weather_csv),\n",
    "        data_dir=data_dir_base,\n",
    "        city_name=config['city_name'],\n",
    "        # Pass feature flags and related paths from config\n",
    "        feature_flags=config['feature_flags'],\n",
    "        sentinel_bands_to_load=config['sentinel_bands_to_load'],\n",
    "        dem_path=str(absolute_dem_path) if config['feature_flags']['use_dem'] else None,\n",
    "        dsm_path=str(absolute_dsm_path) if config['feature_flags']['use_dsm'] else None,\n",
    "        cloudless_mosaic_path=str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "        single_lst_median_path=str(absolute_single_lst_median_path) if config['feature_flags']['use_lst'] else None,\n",
    "        # --- ADDED weather_seq_length --- #\n",
    "        weather_seq_length=config['weather_seq_length'],\n",
    "        # -------------------------------- #\n",
    "        # target_crs can be added if needed, using default from dataloader\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split (Random) ---\n",
    "# (Split logic unchanged)\n",
    "val_percent = 0.40\n",
    "n_samples = len(dataset)\n",
    "if n_samples < 10:\n",
    "    print(f\"Warning: Dataset size ({n_samples}) is very small. Using all data for training.\")\n",
    "    n_val = 0\n",
    "    n_train = n_samples\n",
    "    val_ds = None\n",
    "else:\n",
    "    n_val = int(n_samples * val_percent)\n",
    "    n_train = n_samples - n_val\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "print(f\"Random dataset split: {len(train_ds)} training, {len(val_ds) if val_ds else 0} validation samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY ---\n",
    "# (Stat calculation unchanged)\n",
    "print(\"Calculating UHI statistics from training data...\")\n",
    "all_train_targets = []\n",
    "calc_batch_size = min(64, len(train_ds)) if len(train_ds) > 0 else 1\n",
    "if len(train_ds) > 0:\n",
    "    temp_loader = DataLoader(train_ds, batch_size=calc_batch_size, shuffle=False, num_workers=0)\n",
    "    for batch in tqdm(temp_loader, desc=\"Calculating stats\"):\n",
    "        target_tensor = batch.get('target')\n",
    "        mask_tensor = batch.get('mask')\n",
    "        if target_tensor is None or mask_tensor is None:\n",
    "            logging.warning(\"Skipping batch in stats calculation due to missing target or mask.\")\n",
    "            continue\n",
    "        valid_targets = target_tensor[mask_tensor.bool()]\n",
    "        if valid_targets.numel() > 0:\n",
    "            all_train_targets.append(valid_targets.cpu())\n",
    "else:\n",
    "    print(\"Training dataset is empty, skipping UHI statistics calculation.\")\n",
    "if not all_train_targets:\n",
    "     if len(train_ds) > 0:\n",
    "         raise ValueError(\"No valid training targets found to calculate UHI statistics.\")\n",
    "     else:\n",
    "         print(\"Warning: No training data, setting UHI stats to 0 and 1.\")\n",
    "         uhi_mean = 0.0\n",
    "         uhi_std = 1.0\n",
    "else:\n",
    "    all_train_targets_tensor = torch.cat(all_train_targets)\n",
    "    uhi_mean = all_train_targets_tensor.mean().item()\n",
    "    uhi_std = all_train_targets_tensor.std().item()\n",
    "    uhi_std = uhi_std if uhi_std > 1e-6 else 1.0\n",
    "print(f\"Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f}\")\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "# (Dataloader creation unchanged)\n",
    "print(\"Creating dataloaders...\")\n",
    "train_batch_size = max(1, len(train_ds) // config['n_train_batches']) if len(train_ds) > 0 else 1\n",
    "val_batch_size = len(val_ds) if val_ds and len(val_ds) > 0 else 1\n",
    "print(f\"Using Train Batch Size: {train_batch_size}\")\n",
    "train_loader = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, num_workers=config['num_workers'], pin_memory=True, drop_last=True) if len(train_ds) > 0 else None\n",
    "val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=config['num_workers'], pin_memory=True) if val_ds and len(val_ds) > 0 else None\n",
    "print(\"Data loading setup complete.\")\n",
    "\n",
    "# --- Verify Dataloader Output --- #\n",
    "# Now check for the new keys from dataloader\n",
    "if train_loader:\n",
    "    try:\n",
    "        first_batch = next(iter(train_loader))\n",
    "        print(\"\\nFirst training batch keys:\", list(first_batch.keys()))\n",
    "        # Print shapes to verify\n",
    "        for k, v in first_batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                print(f\"  {k}: {v.shape}, dtype={v.dtype}\")\n",
    "            else:\n",
    "                 print(f\"  {k}: type={type(v)}\")\n",
    "        # Check weather seq shape\n",
    "        expected_weather_shape = (train_batch_size, config['weather_seq_length'], config['weather_input_channels'], dataset.sat_H, dataset.sat_W)\n",
    "        actual_weather_shape = first_batch['weather_seq'].shape\n",
    "        if actual_weather_shape != expected_weather_shape:\n",
    "             logging.warning(f\"Weather sequence shape mismatch! Expected {expected_weather_shape}, got {actual_weather_shape}\")\n",
    "        # Check static features shape (if present)\n",
    "        if 'static_features' in first_batch:\n",
    "            print(f\"  Static features channels: {first_batch['static_features'].shape[1]}\")\n",
    "            expected_static_channels = dataset.combined_static_features.shape[0]\n",
    "            if first_batch['static_features'].shape[1] != expected_static_channels:\n",
    "                 logging.warning(f\"Static features channel mismatch! Dataloader has {expected_static_channels}, batch has {first_batch['static_features'].shape[1]}\")\n",
    "        # Check clay mosaic shape (if present)\n",
    "        if 'cloudless_mosaic' in first_batch:\n",
    "             expected_clay_channels = len(config['clay_bands']) # Should be 4\n",
    "             if first_batch['cloudless_mosaic'].shape[1] != expected_clay_channels:\n",
    "                 logging.warning(f\"Cloudless mosaic channel mismatch! Expected {expected_clay_channels} for Clay, got {first_batch['cloudless_mosaic'].shape[1]}\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nCould not get first batch, training loader might be empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError inspecting first batch: {e}\")\n",
    "else:\n",
    "    print(\"\\nTraining loader is not available (likely no training data).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model Initialization (BranchedUHIModel with ConvLSTM)\n",
    "\n",
    "print(\"Initializing BranchedUHIModel with ConvLSTM...\")\n",
    "\n",
    "# --- Determine Static Input Channels for Model --- \n",
    "# This depends on which *non-Clay* static features are enabled in the dataloader\n",
    "static_channels_model = 0\n",
    "if config['feature_flags']['use_dsm']: static_channels_model += 1\n",
    "if config['feature_flags']['use_dem']: static_channels_model += 1\n",
    "if config['feature_flags']['use_sentinel_composite']: static_channels_model += len(config['sentinel_bands_to_load'])\n",
    "if config['feature_flags']['use_lst']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndvi']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndbi']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndwi']: static_channels_model += 1\n",
    "print(f\"Calculated non-Clay static input channels for model: {static_channels_model}\")\n",
    "\n",
    "# --- Instantiate the BranchedUHIModel with ConvLSTM --- #\n",
    "model = BranchedUHIModel(\n",
    "    # Non-default args first\n",
    "    weather_input_channels=config['weather_input_channels'],\n",
    "    convlstm_hidden_dims=config['convlstm_hidden_dims'],\n",
    "    convlstm_kernel_sizes=config['convlstm_kernel_sizes'],\n",
    "    convlstm_num_layers=config['convlstm_num_layers'],\n",
    "    static_channels=static_channels_model, # Pass calculated non-clay static channels\n",
    "    unet_base_channels=config['unet_base_channels'],\n",
    "    unet_depth=config['unet_depth'],\n",
    "    # Default args next\n",
    "    include_clay_features=config['feature_flags']['use_clay'],\n",
    "    clay_checkpoint_path=str(absolute_clay_checkpoint_path) if config['feature_flags']['use_clay'] else None,\n",
    "    clay_metadata_path=str(absolute_clay_metadata_path) if config['feature_flags']['use_clay'] else None,\n",
    "    freeze_clay_backbone=config['freeze_backbone'] if config['feature_flags']['use_clay'] else False,\n",
    "    clay_embed_dim=1024, # Assuming ViT-Large from Clay\n",
    "    proj_static_ch=config['proj_static_ch'],\n",
    "    proj_temporal_ch=config['proj_temporal_ch'],\n",
    "    # Clay kwargs (match constructor)\n",
    "    model_size=config['clay_model_size'] if config['feature_flags']['use_clay'] else None,\n",
    "    bands=config['clay_bands'] if config['feature_flags']['use_clay'] else None,\n",
    "    platform=config['clay_platform'] if config['feature_flags']['use_clay'] else None,\n",
    "    gsd=config['clay_gsd'] if config['feature_flags']['use_clay'] else None\n",
    ").to(device)\n",
    "\n",
    "print(\"BranchedUHIModel (ConvLSTM) initialized.\")\n",
    "# Optional: Print model summary\n",
    "# try:\n",
    "#     from torchinfo import summary\n",
    "#     # Determine dummy input shapes (use H, W from dataloader if available)\n",
    "#     H, W = (dataset.sat_H, dataset.sat_W) if 'dataset' in locals() else (224, 224) # Default guess\n",
    "#     T = config['weather_seq_length']\n",
    "#     B = 2 # Dummy batch size\n",
    "#     dummy_weather = torch.randn(B, T, config['weather_input_channels'], H, W)\n",
    "#     dummy_static = torch.randn(B, static_channels_model, H, W) if static_channels_model > 0 else None\n",
    "#     dummy_clay_mosaic = torch.randn(B, len(config['clay_bands']), H, W) if config['feature_flags']['use_clay'] else None\n",
    "#     dummy_norm_time = torch.randn(B, 4) if config['feature_flags']['use_clay'] else None\n",
    "#     dummy_norm_latlon = torch.randn(B, 4) if config['feature_flags']['use_clay'] else None\n",
    "#     summary(model, input_data=[dummy_weather, dummy_static, dummy_clay_mosaic, dummy_norm_time, dummy_norm_latlon], device=str(device))\n",
    "# except ImportError:\n",
    "#     print(\"Install torchinfo (`pip install torchinfo`) for model summary.\")\n",
    "# except Exception as e:\n",
    "#      print(f\"Could not print model summary: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e34b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper Functions (Train/Validate Epochs for ConvLSTM Model)\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device, uhi_mean, uhi_std, feature_flags):\n",
    "    \"\"\"Trains the BranchedUHIModel (ConvLSTM) for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # --- Unpack batch based on BranchedCityDataSet output (ConvLSTM version) ---\n",
    "        # Mandatory items\n",
    "        weather_seq = batch.get('weather_seq').to(device)      # (B, T, C_weather, H, W)\n",
    "        target_unnorm = batch.get('target').to(device)          # (B, 1, H, W)\n",
    "        mask = batch.get('mask').to(device)                  # (B, 1, H, W)\n",
    "\n",
    "        # Optional items - get them if they exist in the batch\n",
    "        static_features = batch.get('static_features') # Non-clay static features\n",
    "        if static_features is not None: static_features = static_features.to(device)\n",
    "\n",
    "        cloudless_mosaic = batch.get('cloudless_mosaic') # Input for Clay\n",
    "        if cloudless_mosaic is not None: cloudless_mosaic = cloudless_mosaic.to(device)\n",
    "\n",
    "        norm_latlon = batch.get('norm_latlon')           # Clay metadata\n",
    "        if norm_latlon is not None: norm_latlon = norm_latlon.to(device)\n",
    "\n",
    "        norm_time = batch.get('norm_timestamp')      # Clay metadata (NOTE: Key changed in dataloader update)\n",
    "        if norm_time is not None: norm_time = norm_time.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            # --- Forward Pass - Use ConvLSTM model signature ---\n",
    "            # Determine target H, W from target tensor\n",
    "            target_h, target_w = target_unnorm.shape[2], target_unnorm.shape[3]\n",
    "            target_h_w_tuple = (target_h, target_w)\n",
    "            \n",
    "            prediction_norm = model(\n",
    "                weather_seq=weather_seq,\n",
    "                static_features=static_features,    # Pass non-clay static features separately\n",
    "                cloudless_mosaic=cloudless_mosaic,  # Pass mosaic needed for Clay\n",
    "                norm_latlon_tensor=norm_latlon,\n",
    "                norm_time_tensor=norm_time,\n",
    "                target_h_w=target_h_w_tuple       # Pass target size for final resize\n",
    "            ) # Shape (B, 1, H_target, W_target)\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            prediction_norm_final = prediction_norm.squeeze(1) # Shape (B, H_target, W_target)\n",
    "            target_norm = (target_unnorm.squeeze(1) - uhi_mean) / uhi_std # Normalize target (B, H, W)\n",
    "            loss = loss_fn(prediction_norm_final, target_norm, mask.squeeze(1)) # Use mask (B, H, W)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Accumulate for Metrics (using unnormalized values) ---\n",
    "            prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "            mask_bool = mask.squeeze(1).bool() # Get boolean mask (B, H, W)\n",
    "            all_preds_unnorm.append(prediction_unnorm[mask_bool].detach().cpu())\n",
    "            all_targets_unnorm.append(target_unnorm.squeeze(1)[mask_bool].detach().cpu())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            logging.error(f\"Runtime error during training: {e}\")\n",
    "            if \"out of memory\" in str(e):\n",
    "                logging.error(\"CUDA out of memory. Try reducing batch size (or n_train_batches).\")\n",
    "            continue # Skip batch\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "            continue # Skip batch\n",
    "\n",
    "    # --- Epoch Metrics Calculation (Unchanged) ---\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    if all_targets_unnorm:\n",
    "        all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "        all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "        valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "        if np.sum(valid_idx) > 0:\n",
    "            rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "            r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "        else:\n",
    "             logging.warning(\"No valid pixels found for calculating metrics in training epoch.\")\n",
    "             rmse = float('nan'); r2 = float('nan')\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in training epoch.\")\n",
    "        rmse = float('nan'); r2 = float('nan')\n",
    "    return avg_loss, rmse, r2\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device, uhi_mean, uhi_std, feature_flags):\n",
    "    \"\"\"Evaluates the BranchedUHIModel (ConvLSTM) on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # --- Unpack batch (same as train_epoch) ---\n",
    "            weather_seq = batch.get('weather_seq').to(device)\n",
    "            target_unnorm = batch.get('target').to(device)\n",
    "            mask = batch.get('mask').to(device)\n",
    "            static_features = batch.get('static_features')\n",
    "            if static_features is not None: static_features = static_features.to(device)\n",
    "            cloudless_mosaic = batch.get('cloudless_mosaic')\n",
    "            if cloudless_mosaic is not None: cloudless_mosaic = cloudless_mosaic.to(device)\n",
    "            norm_latlon = batch.get('norm_latlon')\n",
    "            if norm_latlon is not None: norm_latlon = norm_latlon.to(device)\n",
    "            norm_time = batch.get('norm_timestamp') # Key changed\n",
    "            if norm_time is not None: norm_time = norm_time.to(device)\n",
    "\n",
    "            try:\n",
    "                # --- Forward Pass ---\n",
    "                target_h, target_w = target_unnorm.shape[2], target_unnorm.shape[3]\n",
    "                target_h_w_tuple = (target_h, target_w)\n",
    "                \n",
    "                prediction_norm = model(\n",
    "                    weather_seq=weather_seq,\n",
    "                    static_features=static_features,\n",
    "                    cloudless_mosaic=cloudless_mosaic,\n",
    "                    norm_latlon_tensor=norm_latlon,\n",
    "                    norm_time_tensor=norm_time,\n",
    "                    target_h_w=target_h_w_tuple\n",
    "                )\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                prediction_norm_final = prediction_norm.squeeze(1)\n",
    "                target_norm = (target_unnorm.squeeze(1) - uhi_mean) / uhi_std\n",
    "                loss = loss_fn(prediction_norm_final, target_norm, mask.squeeze(1))\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Accumulate for Metrics ---\n",
    "                prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "                mask_bool = mask.squeeze(1).bool()\n",
    "                all_preds_unnorm.append(prediction_unnorm[mask_bool].detach().cpu())\n",
    "                all_targets_unnorm.append(target_unnorm.squeeze(1)[mask_bool].detach().cpu())\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on errors\n",
    "\n",
    "    # --- Epoch Metrics Calculation (Unchanged) ---\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    if all_targets_unnorm:\n",
    "        all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "        all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "        valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "        if np.sum(valid_idx) > 0:\n",
    "             rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "             r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "        else:\n",
    "             logging.warning(\"No valid pixels found for calculating metrics in validation epoch.\")\n",
    "             rmse = float('nan'); r2 = float('nan')\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in validation epoch.\")\n",
    "        rmse = float('nan'); r2 = float('nan')\n",
    "    return avg_loss, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f57963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper Functions (Train/Validate Epochs)\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_filename='model_best.pth.tar'):\n",
    "    \"\"\"Saves model checkpoint.\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "        print(f\"Saved new best model to {best_filename}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device, uhi_mean, uhi_std, feature_flags):\n",
    "    \"\"\"Trains the BranchedUHIModel for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # --- Unpack batch based on BranchedCityDataSet output ---\n",
    "        # Mandatory items\n",
    "        weather_seq = batch.get('weather_seq').to(device)\n",
    "        target_unnorm = batch.get('target').to(device)\n",
    "        mask = batch.get('mask').to(device)\n",
    "        \n",
    "        # Optional items - get them if present in batch (based on feature_flags used in dataloader)\n",
    "        # The BranchedUHIModel forward pass will expect these if the corresponding feature_flag was True during init\n",
    "        dsm = batch.get('dsm').to(device) if feature_flags['use_dsm'] and 'dsm' in batch else None\n",
    "        dem = batch.get('dem').to(device) if feature_flags['use_dem'] and 'dem' in batch else None\n",
    "        clay_input = batch.get('clay_patch').to(device) if feature_flags['use_clay'] and 'clay_patch' in batch else None # Assuming dataloader provides 'clay_patch'\n",
    "        sentinel_composite = batch.get('sentinel_composite').to(device) if feature_flags['use_sentinel_composite'] and 'sentinel_composite' in batch else None\n",
    "        lst = batch.get('lst').to(device) if feature_flags['use_lst'] and 'lst' in batch else None\n",
    "        ndvi = batch.get('ndvi').to(device) if feature_flags['use_ndvi'] and 'ndvi' in batch else None\n",
    "        ndbi = batch.get('ndbi').to(device) if feature_flags['use_ndbi'] and 'ndbi' in batch else None\n",
    "        ndwi = batch.get('ndwi').to(device) if feature_flags['use_ndwi'] and 'ndwi' in batch else None\n",
    "        # Clay specific position/time info if needed by Clay part of the model\n",
    "        norm_latlon = batch.get('norm_latlon').to(device) if feature_flags['use_clay'] and 'norm_latlon' in batch else None \n",
    "        norm_time = batch.get('norm_time').to(device) if feature_flags['use_clay'] and 'norm_time' in batch else None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            # --- Forward Pass - Use BranchedUHIModel signature ---\n",
    "            # Pass all potential inputs; the model internally uses what it needs based on its config\n",
    "            prediction_norm = model(\n",
    "                weather_seq=weather_seq,\n",
    "                dsm=dsm,\n",
    "                dem=dem,\n",
    "                clay_patch=clay_input, # Pass the patch data\n",
    "                norm_latlon=norm_latlon, # Pass positional info for Clay\n",
    "                norm_time=norm_time,     # Pass time info for Clay\n",
    "                sentinel_composite=sentinel_composite,\n",
    "                lst=lst,\n",
    "                ndvi=ndvi,\n",
    "                ndbi=ndbi,\n",
    "                ndwi=ndwi\n",
    "                # Add other features as model arguments if needed\n",
    "            ) # Shape (B, 1, H_target, W_target)\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            prediction_norm_final = prediction_norm.squeeze(1) # Shape (B, H_target, W_target)\n",
    "\n",
    "            # Normalize Target for Loss Calculation\n",
    "            target_norm = (target_unnorm - uhi_mean) / uhi_std\n",
    "\n",
    "            # Calculate Loss (using normalized values)\n",
    "            loss = loss_fn(prediction_norm_final, target_norm, mask)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            # Optional: Gradient clipping\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Accumulate for Metrics (using unnormalized values) ---\n",
    "            prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "            all_preds_unnorm.append(prediction_unnorm[mask.bool()].detach().cpu())\n",
    "            all_targets_unnorm.append(target_unnorm[mask.bool()].detach().cpu())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            logging.error(f\"Runtime error during training: {e}\")\n",
    "            if \"out of memory\" in str(e):\n",
    "                logging.error(\"CUDA out of memory. Try reducing n_train_batches (increases batch size).\")\n",
    "            continue # Skip batch\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "            continue # Skip batch\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    # Calculate metrics on unnormalized, masked data for the whole epoch\n",
    "    if all_targets_unnorm:\n",
    "        all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "        all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "        valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "        if np.sum(valid_idx) > 0: # Check if there are any valid pixels\n",
    "            rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "            r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "        else:\n",
    "             logging.warning(\"No valid pixels found for calculating metrics in training epoch.\")\n",
    "             rmse = float('nan')\n",
    "             r2 = float('nan')\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in training epoch.\")\n",
    "        rmse = float('nan')\n",
    "        r2 = float('nan')\n",
    "\n",
    "    return avg_loss, rmse, r2\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device, uhi_mean, uhi_std, feature_flags):\n",
    "    \"\"\"Evaluates the BranchedUHIModel on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # --- Unpack batch (same as train_epoch) ---\n",
    "            weather_seq = batch.get('weather_seq').to(device)\n",
    "            target_unnorm = batch.get('target').to(device)\n",
    "            mask = batch.get('mask').to(device)\n",
    "            dsm = batch.get('dsm').to(device) if feature_flags['use_dsm'] and 'dsm' in batch else None\n",
    "            dem = batch.get('dem').to(device) if feature_flags['use_dem'] and 'dem' in batch else None\n",
    "            clay_input = batch.get('clay_patch').to(device) if feature_flags['use_clay'] and 'clay_patch' in batch else None\n",
    "            sentinel_composite = batch.get('sentinel_composite').to(device) if feature_flags['use_sentinel_composite'] and 'sentinel_composite' in batch else None\n",
    "            lst = batch.get('lst').to(device) if feature_flags['use_lst'] and 'lst' in batch else None\n",
    "            ndvi = batch.get('ndvi').to(device) if feature_flags['use_ndvi'] and 'ndvi' in batch else None\n",
    "            ndbi = batch.get('ndbi').to(device) if feature_flags['use_ndbi'] and 'ndbi' in batch else None\n",
    "            ndwi = batch.get('ndwi').to(device) if feature_flags['use_ndwi'] and 'ndwi' in batch else None\n",
    "            norm_latlon = batch.get('norm_latlon').to(device) if feature_flags['use_clay'] and 'norm_latlon' in batch else None \n",
    "            norm_time = batch.get('norm_time').to(device) if feature_flags['use_clay'] and 'norm_time' in batch else None\n",
    "\n",
    "            try:\n",
    "                # --- Forward Pass ---\n",
    "                prediction_norm = model(\n",
    "                    weather_seq=weather_seq,\n",
    "                    dsm=dsm,\n",
    "                    dem=dem,\n",
    "                    clay_patch=clay_input,\n",
    "                    norm_latlon=norm_latlon,\n",
    "                    norm_time=norm_time,\n",
    "                    sentinel_composite=sentinel_composite,\n",
    "                    lst=lst,\n",
    "                    ndvi=ndvi,\n",
    "                    ndbi=ndbi,\n",
    "                    ndwi=ndwi\n",
    "                )\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                prediction_norm_final = prediction_norm.squeeze(1)\n",
    "                target_norm = (target_unnorm - uhi_mean) / uhi_std\n",
    "                loss = loss_fn(prediction_norm_final, target_norm, mask)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Accumulate for Metrics ---\n",
    "                prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "                all_preds_unnorm.append(prediction_unnorm[mask.bool()].detach().cpu())\n",
    "                all_targets_unnorm.append(target_unnorm[mask.bool()].detach().cpu())\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on errors\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    # Calculate metrics on unnormalized, masked data\n",
    "    if all_targets_unnorm:\n",
    "        all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "        all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "        valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "        if np.sum(valid_idx) > 0:\n",
    "             rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "             r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "        else:\n",
    "             logging.warning(\"No valid pixels found for calculating metrics in validation epoch.\")\n",
    "             rmse = float('nan')\n",
    "             r2 = float('nan')\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in validation epoch.\")\n",
    "        rmse = float('nan')\n",
    "        r2 = float('nan')\n",
    "\n",
    "    return avg_loss, rmse, r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf1b23",
   "metadata": {},
   "source": [
    "# %% Training Loop Execution (ConvLSTM Model)\n",
    "\n",
    "# Ensure model and dataloaders are initialized from previous cells\n",
    "if 'model' not in locals() or model is None:\n",
    "    raise NameError(\"Model is not initialized. Run the model initialization cell first.\")\n",
    "if 'train_loader' not in locals():\n",
    "    raise NameError(\"Train loader is not initialized. Run the DataLoader setup cell first.\")\n",
    "if 'val_loader' not in locals():\n",
    "    val_loader = None\n",
    "    print(\"Validation loader not found, proceeding without validation.\")\n",
    "\n",
    "print(f\"Model {config['model_type']} initialized on {device}\") # Updated model type in log\n",
    "\n",
    "# --- Optimizer and Loss (Unchanged) ---\n",
    "best_val_r2 = -float('inf')\n",
    "epochs_no_improve = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "loss_fn = masked_mae_loss if config[\"loss_type\"] == \"mae\" else masked_mse_loss\n",
    "\n",
    "# --- Output Directory & Run Name (Unchanged) ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{config['wander_run_name_prefix']}_{run_timestamp}\" # Prefix updated in config cell\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"output_dir\"] = str(output_dir)\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# --- Retrieve UHI Stats (Unchanged) ---\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean and uhi_std not found in config. Ensure they were calculated.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f} for normalization.\")\n",
    "\n",
    "# --- Feature Flags for Training/Validation functions (Unchanged) ---\n",
    "feature_flags_from_config = config['feature_flags']\n",
    "\n",
    "# --- Initialize WANDB (Unchanged) ---\n",
    "if 'wandb' in sys.modules:\n",
    "    try:\n",
    "        if wandb.run is not None: print(\"Finishing previous W&B run...\"); wandb.finish()\n",
    "        wandb.init( project=config[\"wandb_project_name\"], name=run_name, config=config )\n",
    "        print(f\"Wandb initialized for run: {run_name}\")\n",
    "    except Exception as e: print(f\"Wandb initialization failed: {e}\"); wandb = None\n",
    "else: print(\"Wandb not imported, skipping W&B logging.\"); wandb = None\n",
    "\n",
    "# Save configuration locally (Unchanged)\n",
    "try:\n",
    "    config_serializable = {k: str(v) if isinstance(v, Path) else v for k, v in config.items()}\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_serializable, f, indent=2, default=lambda x: str(x) if isinstance(x, Path) else x)\n",
    "    print(\"Saved local configuration to config.json\")\n",
    "except Exception as e: print(f\"Warning: Failed to save local configuration: {e}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "print(f\"Starting {config['model_type']} training...\") # Updated model type in log\n",
    "training_log = []\n",
    "last_saved_epoch = -1\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "\n",
    "    # --- Train Epoch (Call is unchanged, function handles logic) ---\n",
    "    if train_loader:\n",
    "        train_loss, train_rmse, train_r2 = train_epoch(model, train_loader, optimizer, loss_fn, device,\n",
    "                                                       uhi_mean, uhi_std, feature_flags_from_config)\n",
    "    else:\n",
    "        print(\"Skipping training epoch as train_loader is not available.\")\n",
    "        train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    log_metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_r2\": train_r2\n",
    "    }\n",
    "\n",
    "    # --- Validation and Checkpointing (Logic unchanged) ---\n",
    "    is_best = False\n",
    "    if val_loader:\n",
    "        val_loss, val_rmse, val_r2 = validate_epoch(model, val_loader, loss_fn, device,\n",
    "                                                    uhi_mean, uhi_std, feature_flags_from_config)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} | Val Loss={val_loss:.4f} RMSE={val_rmse:.4f} R2={val_r2:.4f}\")\n",
    "        log_metrics.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"val_r2\": val_r2\n",
    "        })\n",
    "\n",
    "        if np.isnan(val_r2):\n",
    "             print(\"Warning: Validation R^2 is NaN. Stopping training.\")\n",
    "             break\n",
    "\n",
    "        is_best = val_r2 > best_val_r2\n",
    "        if is_best:\n",
    "            best_val_r2 = val_r2\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"New best validation R^2: {best_val_r2:.4f}\")\n",
    "            if wandb and wandb.run: wandb.run.summary[\"best_val_r2\"] = best_val_r2\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in validation R^2 for {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= config[\"patience\"]:\n",
    "            print(f\"Early stopping triggered after {config['patience']} epochs with no improvement.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} (No validation)\")\n",
    "        if np.isnan(train_loss):\n",
    "             print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "             break\n",
    "\n",
    "    # --- Log Metrics (Unchanged) ---\n",
    "    if wandb: wandb.log(log_metrics)\n",
    "    training_log.append(log_metrics)\n",
    "\n",
    "    # --- Save Checkpoint (Unchanged) ---\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_r2': best_val_r2,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config_serializable\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "    if is_best: last_saved_epoch = epoch + 1\n",
    "\n",
    "# --- Final Steps (Unchanged) ---\n",
    "try:\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_df.to_csv(output_dir / 'training_log.csv', index=False)\n",
    "    print(f\"Saved local training log to {output_dir / 'training_log.csv'}\")\n",
    "except Exception as e: print(f\"Warning: Failed to save local training log: {e}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "if val_loader: print(f\"Best validation R^2 recorded: {best_val_r2:.4f} (epoch {last_saved_epoch if last_saved_epoch > 0 else 'N/A'})\" if not np.isinf(best_val_r2) else \"Best validation R^2: N/A\")\n",
    "print(f\"Final checkpoint saved in: {output_dir / 'checkpoint_last.pth.tar'}\")\n",
    "if last_saved_epoch > 0: print(f\"Best model saved in: {output_dir / 'model_best.pth.tar'}\")\n",
    "if wandb and wandb.run: wandb.finish(); print(\"Wandb run finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6584bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Training Loop Execution\n",
    "\n",
    "# Ensure model and dataloaders are initialized from previous cells\n",
    "if 'model' not in locals() or model is None:\n",
    "    raise NameError(\"Model is not initialized. Run the model initialization cell first.\")\n",
    "if 'train_loader' not in locals():\n",
    "    raise NameError(\"Train loader is not initialized. Run the DataLoader setup cell first.\")\n",
    "# val_loader can be None if val_percent was 0 or dataset too small\n",
    "if 'val_loader' not in locals():\n",
    "    val_loader = None \n",
    "    print(\"Validation loader not found, proceeding without validation.\")\n",
    "\n",
    "print(f\"Model {config['model_type']} initialized on {device}\")\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "best_val_r2 = -float('inf') # Initialize best R^2\n",
    "epochs_no_improve = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "loss_fn = masked_mae_loss if config[\"loss_type\"] == \"mae\" else masked_mse_loss\n",
    "\n",
    "# --- Output Directory & Run Name ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{config['wander_run_name_prefix']}_{run_timestamp}\"\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"output_dir\"] = str(output_dir) # Update config with actual output dir\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# --- Retrieve UHI Stats ---\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean and uhi_std not found in config. Ensure they were calculated.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f} for normalization.\")\n",
    "\n",
    "# --- Feature Flags for Training/Validation functions ---\n",
    "feature_flags_from_config = config['feature_flags']\n",
    "\n",
    "# --- Initialize WANDB ---\n",
    "if 'wandb' in sys.modules:\n",
    "    try:\n",
    "        if wandb.run is not None:\n",
    "            print(\"Finishing previous W&B run...\")\n",
    "            wandb.finish()\n",
    "        wandb.init(\n",
    "            project=config[\"wandb_project_name\"],\n",
    "            name=run_name,\n",
    "            config=config # Log the entire config dictionary\n",
    "        )\n",
    "        print(f\"Wandb initialized for run: {run_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization failed: {e}\")\n",
    "        wandb = None\n",
    "else:\n",
    "    print(\"Wandb not imported, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Save configuration used for this run locally\n",
    "try:\n",
    "    # Use the same serialization helper as before\n",
    "    config_serializable = {k: str(v) if isinstance(v, Path) else v for k, v in config.items()}\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_serializable, f, indent=2, default=lambda x: str(x) if isinstance(x, Path) else x)\n",
    "    print(\"Saved local configuration to config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local configuration: {e}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"Starting {config['model_type']} training...\")\n",
    "training_log = []\n",
    "last_saved_epoch = -1\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "\n",
    "    if train_loader:\n",
    "        train_loss, train_rmse, train_r2 = train_epoch(model, train_loader, optimizer, loss_fn, device,\n",
    "                                                       uhi_mean, uhi_std, feature_flags_from_config)\n",
    "    else:\n",
    "        print(\"Skipping training epoch as train_loader is not available.\")\n",
    "        train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    log_metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_r2\": train_r2\n",
    "    }\n",
    "\n",
    "    is_best = False\n",
    "    if val_loader:\n",
    "        val_loss, val_rmse, val_r2 = validate_epoch(model, val_loader, loss_fn, device,\n",
    "                                                    uhi_mean, uhi_std, feature_flags_from_config)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} | Val Loss={val_loss:.4f} RMSE={val_rmse:.4f} R2={val_r2:.4f}\")\n",
    "        log_metrics.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"val_r2\": val_r2\n",
    "        })\n",
    "\n",
    "        if np.isnan(val_r2):\n",
    "             print(\"Warning: Validation R^2 is NaN. Cannot determine improvement. Stopping training.\")\n",
    "             break\n",
    "\n",
    "        is_best = val_r2 > best_val_r2\n",
    "        if is_best:\n",
    "            best_val_r2 = val_r2\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"New best validation R^2: {best_val_r2:.4f}\")\n",
    "            if wandb and wandb.run:\n",
    "                wandb.run.summary[\"best_val_r2\"] = best_val_r2\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in validation R^2 for {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= config[\"patience\"]:\n",
    "            print(f\"Early stopping triggered after {config['patience']} epochs with no improvement.\")\n",
    "            break\n",
    "    else: # No validation\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} (No validation)\")\n",
    "        if np.isnan(train_loss):\n",
    "             print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "             break\n",
    "\n",
    "    if wandb:\n",
    "        wandb.log(log_metrics)\n",
    "    training_log.append(log_metrics)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_r2': best_val_r2,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config_serializable\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "    if is_best:\n",
    "        last_saved_epoch = epoch + 1\n",
    "\n",
    "# --- Final Steps ---\n",
    "try:\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_df.to_csv(output_dir / 'training_log.csv', index=False)\n",
    "    print(f\"Saved local training log to {output_dir / 'training_log.csv'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local training log: {e}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "if val_loader:\n",
    "    print(f\"Best validation R^2 recorded: {best_val_r2:.4f} (achieved at epoch {last_saved_epoch if last_saved_epoch > 0 else 'N/A'})\" if not np.isinf(best_val_r2) else \"Best validation R^2: N/A\")\n",
    "print(f\"Final checkpoint saved in: {output_dir / 'checkpoint_last.pth.tar'}\")\n",
    "if last_saved_epoch > 0:\n",
    "     print(f\"Best model saved in: {output_dir / 'model_best.pth.tar'}\")\n",
    "\n",
    "if wandb and wandb.run:\n",
    "    wandb.finish()\n",
    "    print(\"Wandb run finished.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
