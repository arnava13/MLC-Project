{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd353f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/jupyter/MLC-Project/src', '/home/jupyter/MLC-Project']\n",
      "Project Root: /home/jupyter/MLC-Project\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# %% Imports and Setup\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray # For geospatial data handling with xarray\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset # Added Subset\n",
    "\n",
    "\n",
    "# --- Visualization & Progress ---\n",
    "# Optional: Import matplotlib or other plotting libs if needed for checks\n",
    "from tqdm.notebook import tqdm # Use notebook version if running interactively\n",
    "import wandb\n",
    "\n",
    "# --- Custom Modules ---\n",
    "# Project root is the parent directory of the current working directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "src = project_root / \"src\"\n",
    "\n",
    "# Add src directory to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(sys.path)\n",
    "\n",
    "# Import custom modules\n",
    "from src.ingest.dataloader_branched import CityDataSetBranched\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the utility module\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Optional: Configure logging level\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optional: Ignore specific warnings if needed\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, message='.*TypedStorage is deprecated.*')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import check_path # For path validation\n",
    "from src.ingest.data_utils import calculate_actual_weather_channels # For dynamic weather channels\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "# project_root is defined in the first cell\n",
    "project_root_str = str(project_root)\n",
    "data_dir = project_root / \"data\"\n",
    "city_name = \"NYC\" # Should be defined or loaded\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI\"\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "feature_resolution_m = 30\n",
    "uhi_grid_resolution_m = 10 # UHI target grid\n",
    "weather_seq_length = 60\n",
    "\n",
    "# --- Weather Feature Selection --- NEW ---\n",
    "enabled_weather_features = [\n",
    "    \"air_temp\", \n",
    "    \"rel_humidity\", \n",
    "    \"avg_windspeed\", \n",
    "    \"wind_dir\",       # This will be converted to sin/cos components by data_utils\n",
    "    \"solar_flux\"\n",
    "]\n",
    "# Calculate the number of actual weather channels that will be produced by the dataloader\n",
    "actual_dataloader_weather_channels = calculate_actual_weather_channels(enabled_weather_features)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Define Absolute Input Data Paths Directly ---\n",
    "uhi_csv_path = data_dir / city_name / \"uhi.csv\"\n",
    "bronx_weather_csv_path = data_dir / city_name / \"bronx_weather.csv\"\n",
    "manhattan_weather_csv_path = data_dir / city_name / \"manhattan_weather.csv\"\n",
    "\n",
    "dem_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dem_nasadem_native-resolution_pc.tif\"\n",
    "dsm_path = data_dir / city_name / \"sat_files\" / f\"{city_name.lower()}_dsm_cop-dem-glo-30_native-resolution_pc.tif\"\n",
    "cloudless_mosaic_path = data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\" # Added .npy\n",
    "# Assuming the LST filename structure from download_data.ipynb if it's used\n",
    "lst_time_window_str_for_filename = \"20210601_to_20210901\" # Match download_data\n",
    "single_lst_median_path = data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_{lst_time_window_str_for_filename}.npy\" # Corrected and added .npy\n",
    "\n",
    "\n",
    "# Nodata values\n",
    "elevation_nodata = -9999.0 # Or np.nan\n",
    "lst_nodata = 0.0 # Or np.nan\n",
    "\n",
    "# --- Feature Selection Flags ---\n",
    "feature_flags = {\n",
    "    \"use_dem\": True,\n",
    "    \"use_dsm\": True,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False, # Set to True if LST is intended to be used\n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) ---\n",
    "sentinel_bands_to_load = []\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM, No separate Elev branches) ---\n",
    "# Clay Backbone\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "clay_checkpoint_path = project_root / \"notebooks\" / \"clay-v1.5.ckpt\"\n",
    "clay_metadata_path = project_root / \"src\" / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "# weather_input_channels = 6 # REMOVED - will use actual_dataloader_weather_channels or model will derive\n",
    "convlstm_hidden_dims = [32, 16]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# Projection Layer Channels\n",
    "proj_static_ch = 8\n",
    "proj_temporal_ch = 8\n",
    "\n",
    "# --- Head Configuration ---\n",
    "head_type = \"unet\"\n",
    "unet_base_channels = 16\n",
    "unet_depth = 2\n",
    "simple_cnn_head_channels = [32, 16]\n",
    "simple_cnn_head_kernels = [3, 3]\n",
    "simple_cnn_dropout_rate = 0.1\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 1\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "max_grad_norm = 1.0\n",
    "warmup_epochs = 5\n",
    "n_train_batches = 47\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Validate Paths (using check_path for files that *must* exist) ---\n",
    "# Fixed: Removed is_absolute parameter to match updated function signature\n",
    "uhi_csv_path = check_path(uhi_csv_path, project_root, \"UHI CSV\")\n",
    "bronx_weather_csv_path = check_path(bronx_weather_csv_path, project_root, \"Bronx Weather CSV\")\n",
    "manhattan_weather_csv_path = check_path(manhattan_weather_csv_path, project_root, \"Manhattan Weather CSV\")\n",
    "\n",
    "if feature_flags[\"use_dem\"]:\n",
    "    dem_path = check_path(dem_path, project_root, \"DEM TIF\")\n",
    "if feature_flags[\"use_dsm\"]:\n",
    "    dsm_path = check_path(dsm_path, project_root, \"DSM TIF\")\n",
    "if feature_flags[\"use_clay\"]:\n",
    "    clay_checkpoint_path = check_path(clay_checkpoint_path, project_root, \"Clay Checkpoint\")\n",
    "    clay_metadata_path = check_path(clay_metadata_path, project_root, \"Clay Metadata\")\n",
    "# Check cloudless mosaic if Clay or direct Sentinel composite is used\n",
    "if feature_flags[\"use_clay\"] or feature_flags[\"use_sentinel_composite\"]:\n",
    "    cloudless_mosaic_path = check_path(cloudless_mosaic_path, project_root, \"Cloudless Mosaic\")\n",
    "if feature_flags[\"use_lst\"]:\n",
    "    single_lst_median_path = check_path(single_lst_median_path, project_root, \"Single LST Median\", should_exist=True)\n",
    "\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(uhi_csv_path) # Use the validated path\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {uhi_csv_path.name}: {bounds}\") # Use .name for just the filename\n",
    "\n",
    "\n",
    "# --- Central Config Dictionary --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel\", # More specific model type\n",
    "    \"project_root\": project_root_str,\n",
    "    # \"run_dir\" will be added below\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"feature_resolution_m\": feature_resolution_m,\n",
    "    \"uhi_grid_resolution_m\": uhi_grid_resolution_m,\n",
    "    \"weather_seq_length\": weather_seq_length,\n",
    "    \"enabled_weather_features\": enabled_weather_features, # NEW: Add to config\n",
    "    \"uhi_csv\": str(uhi_csv_path),\n",
    "    \"bronx_weather_csv\": str(bronx_weather_csv_path),\n",
    "    \"manhattan_weather_csv\": str(manhattan_weather_csv_path),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags,\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path\": str(dem_path) if feature_flags[\"use_dem\"] else None,\n",
    "    \"dsm_path\": str(dsm_path) if feature_flags[\"use_dsm\"] else None,\n",
    "    \"elevation_nodata\": elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(cloudless_mosaic_path) if feature_flags[\"use_clay\"] or feature_flags[\"use_sentinel_composite\"] else None,\n",
    "    \"single_lst_median_path\": str(single_lst_median_path) if feature_flags[\"use_lst\"] else None,\n",
    "    \"lst_nodata\": lst_nodata,\n",
    "    # Model Config\n",
    "    # \"weather_input_channels\": actual_dataloader_weather_channels, # REMOVED - Model will use its enabled_weather_features list\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    \"head_type\": head_type,\n",
    "    \"unet_base_channels\": unet_base_channels if head_type == \"unet\" else None,\n",
    "    \"unet_depth\": unet_depth if head_type == \"unet\" else None,\n",
    "    \"simple_cnn_head_channels\": simple_cnn_head_channels if head_type == \"simple_cnn\" else None,\n",
    "    \"simple_cnn_head_kernels\": simple_cnn_head_kernels if head_type == \"simple_cnn\" else None,\n",
    "    \"simple_cnn_dropout_rate\": simple_cnn_dropout_rate if head_type == \"simple_cnn\" else None,\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(clay_checkpoint_path) if feature_flags[\"use_clay\"] else None,\n",
    "    \"clay_metadata_path\": str(clay_metadata_path) if feature_flags[\"use_clay\"] else None,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"max_grad_norm\": max_grad_norm,\n",
    "    \"warmup_epochs\": warmup_epochs,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "# --- Create Run Directory & Update Config ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name_suffix = f\"{config['model_type']}_{city_name}_{run_timestamp}\"\n",
    "run_dir = output_dir_base / run_name_suffix\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"run_dir\"] = str(run_dir) # Add to config for later use\n",
    "\n",
    "print(f\"Run directory: {run_dir}\")\n",
    "print(\"\\nBranched Model Configuration dictionary created:\")\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b31ff2-f0b1-4823-93c1-b62dfb77abc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Data Loading and Preprocessing (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    calculate_uhi_stats, # Removed split_data\n",
    "    create_dataloaders\n",
    ")\n",
    "from torch.utils.data import Subset # Import Subset\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = CityDataSetBranched(\n",
    "        bounds=config[\"bounds\"],\n",
    "        feature_resolution_m=config[\"feature_resolution_m\"], # Corrected param name\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"], # Corrected param name\n",
    "        uhi_csv=config[\"uhi_csv\"], # Use paths from config\n",
    "        bronx_weather_csv=config[\"bronx_weather_csv\"],\n",
    "        manhattan_weather_csv=config[\"manhattan_weather_csv\"],\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], # NEW: Pass from config\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path=config[\"dem_path\"], # Corrected param name\n",
    "        dsm_path=config[\"dsm_path\"], # Corrected param name\n",
    "        elevation_nodata=config[\"elevation_nodata\"], # Corrected param name\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        lst_nodata=config[\"lst_nodata\"], # Added missing param\n",
    "        weather_seq_length=config[\"weather_seq_length\"],\n",
    "        target_crs_str=config.get(\"target_crs_str\", \"EPSG:4326\") # Added optional param\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Sequential Train/Val Split --- #\n",
    "val_percent = 0.20 # Keep the percentage definition\n",
    "num_samples = len(dataset)\n",
    "if num_samples < 2: # Need at least one for train and one for val\n",
    "    raise ValueError(f\"Dataset has only {num_samples} samples, cannot perform train/val split.\")\n",
    "\n",
    "n_train = int(num_samples * (1 - val_percent))\n",
    "n_val = num_samples - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    raise ValueError(f\"Split resulted in zero samples for train ({n_train}) or validation ({n_val}). Adjust val_percent or check dataset size.\")\n",
    "\n",
    "train_indices = list(range(n_train))\n",
    "val_indices = list(range(n_train, num_samples))\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "val_ds = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Sequential dataset split: {len(train_ds)} training (indices 0-{n_train-1}), {len(val_ds)} validation (indices {n_train}-{num_samples-1}) samples.\")\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e90e15-1b5c-46f7-aedf-b3fe60662f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Model Initialization (Branched Model + Common Resampling)\n",
    "\n",
    "# --- Import necessary components ---\n",
    "from src.branched_uhi_model import BranchedUHIModel\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging # Ensure logging is imported if not already\n",
    "\n",
    "# Instantiate the BranchedUHIModel\n",
    "print(f\"Initializing {config['model_type']}...\")\n",
    "try:\n",
    "    model = BranchedUHIModel(\n",
    "        # --- Weather Branch Config --- #\n",
    "        # weather_input_channels=config[\"weather_input_channels\"], # REMOVED: Model uses enabled_weather_features\n",
    "        enabled_weather_features=config[\"enabled_weather_features\"], # NEW: Pass from config\n",
    "        convlstm_hidden_dims=config[\"convlstm_hidden_dims\"],\n",
    "        convlstm_kernel_sizes=config[\"convlstm_kernel_sizes\"],\n",
    "        convlstm_num_layers=config[\"convlstm_num_layers\"],\n",
    "        weather_seq_length=config[\"weather_seq_length\"], \n",
    "        # --- Static Feature Config --- #\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config.get(\"sentinel_bands_to_load\"), \n",
    "        # Clay Specific\n",
    "        clay_model_size=config.get(\"clay_model_size\"),\n",
    "        clay_bands=config.get(\"clay_bands\"),\n",
    "        clay_platform=config.get(\"clay_platform\"),\n",
    "        clay_gsd=config.get(\"clay_gsd\"),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        clay_checkpoint_path=config.get(\"clay_checkpoint_path\"),\n",
    "        clay_metadata_path=config.get(\"clay_metadata_path\"),\n",
    "        # --- Head Config --- #\n",
    "        proj_static_ch=config[\"proj_static_ch\"],\n",
    "        proj_temporal_ch=config[\"proj_temporal_ch\"],\n",
    "        head_type=config[\"head_type\"],\n",
    "        unet_base_channels=config.get(\"unet_base_channels\"), \n",
    "        unet_depth=config.get(\"unet_depth\"),                 \n",
    "        simple_cnn_head_channels=config.get(\"simple_cnn_head_channels\"),\n",
    "        simple_cnn_head_kernels=config.get(\"simple_cnn_head_kernels\"),\n",
    "        simple_cnn_dropout_rate=config.get(\"simple_cnn_dropout_rate\", 0.1), \n",
    "        # --- Target Grid Info --- #\n",
    "        uhi_grid_resolution_m=config[\"uhi_grid_resolution_m\"],\n",
    "        bounds=config[\"bounds\"]\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"{config['model_type']} initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing BranchedUHIModel: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception after logging\n",
    "\n",
    "# --- Optimizer --- #\n",
    "try:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    print(\"Optimizer (AdamW) initialized.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing optimizer: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# --- Loss Function --- #\n",
    "if config[\"loss_type\"] == 'mse':\n",
    "    loss_fn = masked_mse_loss\n",
    "    print(\"Loss function set to masked_mse_loss.\")\n",
    "elif config[\"loss_type\"] == 'mae':\n",
    "    loss_fn = masked_mae_loss\n",
    "    print(\"Loss function set to masked_mae_loss.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {config['loss_type']}\")\n",
    "\n",
    "# --- LR Scheduler --- #\n",
    "try:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "    print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing scheduler: {e}\", exc_info=True)\n",
    "    scheduler = None # Allow training without scheduler if init fails\n",
    "    print(\"Proceeding without LR scheduler due to initialization error.\")\n",
    "\n",
    "\n",
    "print(\"\\nModel, optimizer, loss function, and scheduler setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b761f-1d71-4581-bf50-c785d8e88a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Training Loop (Branched Model)\n",
    "\n",
    "# --- Helper functions ---\n",
    "# Get the warmup epochs from config or default to 5\n",
    "warmup_epochs = config.get(\"warmup_epochs\", 5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "# Initialize metrics tracking\n",
    "best_val_loss = float('inf')\n",
    "best_val_r2 = -float('inf')\n",
    "patience_counter = 0\n",
    "training_log = []\n",
    "\n",
    "# Create run directory\n",
    "model_save_dir = Path(config['run_dir']) / \"checkpoints\"\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config to run directory\n",
    "config_path = Path(config['run_dir']) / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x)\n",
    "\n",
    "# Initialize wandb\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "    print(\"Weights & Biases (wandb) available for logging.\")\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "    wandb = None\n",
    "    print(\"Weights & Biases (wandb) not available. Skipping wandb logging.\")\n",
    "\n",
    "if wandb_available:\n",
    "    # Configure wandb\n",
    "    wandb.init(\n",
    "        project=config['wandb_project_name'],\n",
    "        name=f\"{config['wander_run_name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optional: watch model parameters\n",
    "    wandb.watch(model)\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs with patience {config['patience']}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # --- Train --- #\n",
    "        if train_loader:\n",
    "            # Use generic train function from train_utils\n",
    "            train_loss, train_rmse, train_r2 = train_utils.train_epoch_generic(\n",
    "                model, train_loader, optimizer, loss_fn, device, \n",
    "                uhi_mean=config['uhi_mean'], \n",
    "                uhi_std=config['uhi_std'],\n",
    "                feature_flags=config['feature_flags'],\n",
    "                max_grad_norm=config.get(\"max_grad_norm\", 1.0)\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        else:\n",
    "            print(\"Skipping training: train_loader is None.\")\n",
    "            train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "            log_metrics = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_rmse\": train_rmse, \"train_r2\": train_r2}\n",
    "        \n",
    "        # Log train metrics AFTER checking for NaN\n",
    "        if wandb:\n",
    "            wandb.log(log_metrics)\n",
    "        training_log.append(log_metrics) # Append to local log regardless of W&B\n",
    "\n",
    "\n",
    "        # --- Validate --- #\n",
    "        if val_loader:\n",
    "            # Use generic validate function from train_utils\n",
    "            val_loss, val_rmse, val_r2 = train_utils.validate_epoch_generic(\n",
    "                model, val_loader, loss_fn, device, \n",
    "                uhi_mean=config['uhi_mean'], \n",
    "                uhi_std=config['uhi_std'],\n",
    "                feature_flags=config['feature_flags']\n",
    "            )\n",
    "            print(f\"Val Loss:   {val_loss:.4f}, Val RMSE:   {val_rmse:.4f}, Val R2:   {val_r2:.4f}\")\n",
    "            if np.isnan(val_loss):\n",
    "                print(\"Warning: Validation Loss is NaN. Stopping training.\")\n",
    "                break\n",
    "            val_metrics = {\"val_loss\": val_loss, \"val_rmse\": val_rmse, \"val_r2\": val_r2}\n",
    "            log_metrics.update(val_metrics)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            if wandb:\n",
    "                wandb.log(val_metrics)\n",
    "            \n",
    "            # Warmup period: don't save or check early stopping until after warmup_epochs\n",
    "            if epoch >= warmup_epochs:\n",
    "                # Check for improvement (using validation loss now)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_r2 = val_r2\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    train_utils.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "                        'loss': val_loss,\n",
    "                        'val_rmse': val_rmse,\n",
    "                        'val_r2': val_r2,\n",
    "                        'config': config\n",
    "                    }, is_best=True, output_dir=model_save_dir)\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with val_loss {val_loss:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= config['patience']:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Warmup epoch {epoch+1}/{warmup_epochs}. Skipping checkpointing and early stopping.\")\n",
    "        else:\n",
    "            print(\"Skipping validation: val_loader is None.\")\n",
    "            \n",
    "        # Step the scheduler after validation (if it exists)\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if wandb:\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "                \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}, Best R2: {best_val_r2:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = model_save_dir / \"final_model.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save training log\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_path = Path(config['run_dir']) / \"training_log.csv\"\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Finish wandb run\n",
    "    if wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bc3f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
