{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488f02",
   "metadata": {},
   "source": [
    "# Branched UHI Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f6a63e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Added for interpolation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for loading bounds from csv\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm\n",
    "import math\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# --- WANDB --- #\n",
    "import wandb\n",
    "# ------------ #\n",
    "\n",
    "# --- Metrics --- \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# ------------ #\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "project_root = Path(os.getcwd()).parent  # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- UPDATED IMPORTS for Branched Model --- #\n",
    "from src.branched_uhi_model import BranchedUHIModel \n",
    "from src.ingest.dataloader_branched import CityDataSetBranched # NEW DATALOADER\n",
    "# ------------------------------------------ #\n",
    "\n",
    "from src.train.loss import masked_mae_loss, masked_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900e5ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Setup & Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import shutil # For checkpoint saving\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(os.getcwd()).parent # Assumes notebook is in 'notebooks' subdir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- Import Model Components ---\n",
    "from src.branched_uhi_model import BranchedUHIModel # Import the branched model\n",
    "from src.ingest.dataloader_branched import CityDataSetBranched # Import the corresponding dataloader\n",
    "\n",
    "# --- Import Training Utilities & Loss ---\n",
    "from src.train.loss import masked_mse_loss, masked_mae_loss # Import loss functions\n",
    "import src.train.train_utils as train_utils # Import the new utility module\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optionally import wandb if needed\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Import necessary metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310531aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded bounds from uhi.csv: [np.float64(-73.99445667), np.float64(40.75879167), np.float64(-73.87945833), np.float64(40.85949667)]\n",
      "\n",
      "Branched Model (HiRes Elev) Configuration dictionary created:\n",
      "{\n",
      "  \"model_type\": \"BranchedUHIModel_HiResElev\",\n",
      "  \"project_root\": \"/home/jupyter/MLC-Project\",\n",
      "  \"city_name\": \"NYC\",\n",
      "  \"wandb_project_name\": \"MLC_UHI_Proj\",\n",
      "  \"wander_run_name_prefix\": \"NYC_BranchedUHI_HiResElev\",\n",
      "  \"resolution_m\": 10,\n",
      "  \"weather_seq_length\": 60,\n",
      "  \"uhi_csv\": \"data/NYC/uhi.csv\",\n",
      "  \"bronx_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/bronx_weather.csv\",\n",
      "  \"manhattan_weather_csv\": \"/home/jupyter/MLC-Project/data/NYC/manhattan_weather.csv\",\n",
      "  \"bounds\": [\n",
      "    -73.99445667,\n",
      "    40.75879167,\n",
      "    -73.87945833,\n",
      "    40.85949667\n",
      "  ],\n",
      "  \"feature_flags\": {\n",
      "    \"use_dsm\": true,\n",
      "    \"use_dem\": true,\n",
      "    \"use_clay\": true,\n",
      "    \"use_sentinel_composite\": false,\n",
      "    \"use_lst\": false,\n",
      "    \"use_ndvi\": false,\n",
      "    \"use_ndbi\": false,\n",
      "    \"use_ndwi\": false\n",
      "  },\n",
      "  \"sentinel_bands_to_load\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\",\n",
      "    \"swir16\",\n",
      "    \"swir22\"\n",
      "  ],\n",
      "  \"dem_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_1ft_2017.tif\",\n",
      "  \"dsm_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/nyc_dem_1ft_2017.tif\",\n",
      "  \"cloudless_mosaic_path\": \"/home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\",\n",
      "  \"single_lst_median_path\": null,\n",
      "  \"weather_input_channels\": 6,\n",
      "  \"convlstm_hidden_dims\": [\n",
      "    64,\n",
      "    32\n",
      "  ],\n",
      "  \"convlstm_kernel_sizes\": [\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"convlstm_num_layers\": 2,\n",
      "  \"proj_static_ch\": 32,\n",
      "  \"proj_temporal_ch\": 32,\n",
      "  \"unet_base_channels\": 64,\n",
      "  \"unet_depth\": 4,\n",
      "  \"clay_model_size\": \"large\",\n",
      "  \"clay_bands\": [\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"red\",\n",
      "    \"nir\"\n",
      "  ],\n",
      "  \"clay_platform\": \"sentinel-2-l2a\",\n",
      "  \"clay_gsd\": 10,\n",
      "  \"freeze_backbone\": true,\n",
      "  \"clay_checkpoint_path\": \"/home/jupyter/MLC-Project/notebooks/clay-v1.5.ckpt\",\n",
      "  \"clay_metadata_path\": \"/home/jupyter/MLC-Project/src/Clay/configs/metadata.yaml\",\n",
      "  \"include_dem_branch\": true,\n",
      "  \"include_dsm_branch\": true,\n",
      "  \"elevation_branch_start_channels\": 16,\n",
      "  \"elevation_branch_out_channels\": 32,\n",
      "  \"elevation_branch_downsample_layers\": 4,\n",
      "  \"elevation_branch_kernel_size\": 3,\n",
      "  \"n_train_batches\": 8,\n",
      "  \"num_workers\": 4,\n",
      "  \"epochs\": 500,\n",
      "  \"lr\": 5e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"loss_type\": \"mse\",\n",
      "  \"patience\": 50,\n",
      "  \"device\": \"cuda\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %% Configuration / Hyperparameters for BranchedUHIModel (ConvLSTM + HighRes Elev)\n",
    "\n",
    "# --- Import utils ---\n",
    "# check_path moved to train_utils\n",
    "from src.train.train_utils import check_path\n",
    "# -------------------\n",
    "\n",
    "# --- Paths & Basic Info ---\n",
    "project_root_str = str(project_root) # Store as string for config\n",
    "data_dir_base = project_root / \"data\"\n",
    "city_name = \"NYC\"\n",
    "output_dir_base = project_root / \"training_runs\"\n",
    "\n",
    "# --- WANDB Config ---\n",
    "wandb_project_name = \"MLC_UHI_Proj\"\n",
    "wander_run_name_prefix = f\"{city_name}_BranchedUHI_HiResElev\" # Modified prefix\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "resolution_m = 10 # Target resolution for *low-res* features (UHI, Weather, LST, Clay)\n",
    "weather_seq_length = 60\n",
    "\n",
    "# Input Data Paths (relative to project root for portability in config)\n",
    "relative_data_dir = Path(\"data\")\n",
    "relative_uhi_csv = relative_data_dir / city_name / \"uhi.csv\"\n",
    "relative_bronx_weather_csv = relative_data_dir / city_name / \"bronx_weather.csv\"\n",
    "relative_manhattan_weather_csv = relative_data_dir / city_name / \"manhattan_weather.csv\"\n",
    "\n",
    "# --- HIGH-RESOLUTION DEM/DSM Paths (Relative) --- #\n",
    "# Use updated filenames from download_data\n",
    "relative_dem_path_high_res = relative_data_dir / city_name / \"sat_files\" / \"nyc_dem_1m_pc.tif\"\n",
    "relative_dsm_path_high_res = relative_data_dir / city_name / \"sat_files\" / \"nyc_dsm_1m_pc.tif\"\n",
    "high_res_nodata = -9999.0 # Match the nodata used in DSM/DEM download\n",
    "# Define low-res paths as None since we aren't using them\n",
    "relative_dem_path_low_res = None\n",
    "relative_dsm_path_low_res = None\n",
    "low_res_elevation_nodata = None\n",
    "\n",
    "# --- Cloudless Mosaic / LST Paths (Only needed if flags are True) ---\n",
    "relative_cloudless_mosaic_path = relative_data_dir / city_name / \"sat_files\" / f\"sentinel_{city_name}_20210601_to_20210901_cloudless_mosaic.npy\"\n",
    "relative_single_lst_median_path = relative_data_dir / city_name / \"sat_files\" / f\"lst_{city_name}_median_20210601_to_20210901.npy\"\n",
    "\n",
    "\n",
    "# --- Feature Selection Flags (UPDATED KEYS) --- #\n",
    "feature_flags = {\n",
    "    \"use_dem_high_res\": True,\n",
    "    \"use_dsm_high_res\": True,\n",
    "    \"use_dem_low_res\": False,\n",
    "    \"use_dsm_low_res\": False,\n",
    "    \"use_clay\": True,\n",
    "    \"use_sentinel_composite\": False,\n",
    "    \"use_lst\": False,\n",
    "    \"use_ndvi\": False,\n",
    "    \"use_ndbi\": False,\n",
    "    \"use_ndwi\": False,\n",
    "}\n",
    "\n",
    "# --- Bands for Sentinel Composite (if use_sentinel_composite is True) --- #\n",
    "sentinel_bands_to_load = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# --- Model Config (BranchedUHIModel with ConvLSTM & HighRes Elev) ---\n",
    "\n",
    "# Clay Backbone (if feature_flags[\"use_clay\"] is True)\n",
    "clay_model_size = \"large\"\n",
    "clay_bands = [\"blue\", \"green\", \"red\", \"nir\"]\n",
    "clay_platform = \"sentinel-2-l2a\"\n",
    "clay_gsd = 10\n",
    "freeze_backbone = True\n",
    "relative_clay_checkpoint_path = \"notebooks/clay-v1.5.ckpt\"\n",
    "relative_clay_metadata_path = Path(\"src\") / \"Clay\" / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Temporal Weather Processor (ConvLSTM)\n",
    "weather_input_channels = 6\n",
    "convlstm_hidden_dims = [64, 32]\n",
    "convlstm_kernel_sizes = [(3,3), (3,3)]\n",
    "convlstm_num_layers = len(convlstm_hidden_dims)\n",
    "\n",
    "# High-Resolution Elevation Branches (NEW)\n",
    "include_dem_branch = True # Enable DEM branch in the MODEL\n",
    "include_dsm_branch = True # Enable DSM branch in the MODEL\n",
    "elevation_branch_start_channels = 16\n",
    "elevation_branch_out_channels = 32 # Output channels PER branch\n",
    "elevation_branch_downsample_layers = 4 # Adjust based on resolution difference\n",
    "elevation_branch_kernel_size = 3\n",
    "\n",
    "# U-Net Head\n",
    "unet_base_channels = 64\n",
    "unet_depth = 4\n",
    "\n",
    "# Projection Layer Channels\n",
    "proj_static_ch = 32 # For projecting NON-ELEVATION static feats (Clay + Indices + LST + Sentinel)\n",
    "proj_temporal_ch = 32 # For projecting ConvLSTM output\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "num_workers = 4\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "loss_type = 'mse'\n",
    "patience = 50\n",
    "cpu = False\n",
    "n_train_batches = 8\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- REMOVED check_path definition (now in train_utils) ---\n",
    "\n",
    "# --- Resolve Paths using check_path from train_utils ---\n",
    "# --- Sanity Checks and Absolute Paths ---\n",
    "absolute_uhi_csv = check_path(relative_uhi_csv, project_root, \"UHI CSV\")\n",
    "absolute_bronx_weather_csv = check_path(relative_bronx_weather_csv, project_root, \"Bronx Weather CSV\")\n",
    "absolute_manhattan_weather_csv = check_path(relative_manhattan_weather_csv, project_root, \"Manhattan Weather CSV\")\n",
    "# Get paths based on feature flags (using correct flags)\n",
    "absolute_dem_path_high_res = check_path(relative_dem_path_high_res, project_root, \"High-Res DEM TIF\") if feature_flags[\"use_dem_high_res\"] else None\n",
    "absolute_dsm_path_high_res = check_path(relative_dsm_path_high_res, project_root, \"High-Res DSM TIF\") if feature_flags[\"use_dsm_high_res\"] else None\n",
    "absolute_dem_path_low_res = check_path(relative_dem_path_low_res, project_root, \"Low-Res DEM\", should_exist=False) # Allow None\n",
    "absolute_dsm_path_low_res = check_path(relative_dsm_path_low_res, project_root, \"Low-Res DSM\", should_exist=False) # Allow None\n",
    "absolute_clay_checkpoint_path = check_path(relative_clay_checkpoint_path, project_root, \"Clay Checkpoint\") if feature_flags[\"use_clay\"] else None\n",
    "absolute_clay_metadata_path = check_path(relative_clay_metadata_path, project_root, \"Clay Metadata\", should_exist=feature_flags[\"use_clay\"]) if feature_flags[\"use_clay\"] else None\n",
    "needs_mosaic = feature_flags[\"use_sentinel_composite\"] or feature_flags[\"use_clay\"] or feature_flags[\"use_ndvi\"] or feature_flags[\"use_ndbi\"] or feature_flags[\"use_ndwi\"]\n",
    "absolute_cloudless_mosaic_path = check_path(relative_cloudless_mosaic_path, project_root, \"Cloudless Mosaic\") if needs_mosaic else None\n",
    "absolute_single_lst_median_path = check_path(relative_single_lst_median_path, project_root, \"Single LST Median\") if feature_flags[\"use_lst\"] else None\n",
    "\n",
    "# --- Calculate Bounds ---\n",
    "uhi_df = pd.read_csv(absolute_uhi_csv)\n",
    "required_cols = ['Longitude', 'Latitude']\n",
    "if not all(col in uhi_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"UHI CSV must contain columns: {required_cols}\")\n",
    "bounds = [\n",
    "    uhi_df['Longitude'].min(),\n",
    "    uhi_df['Latitude'].min(),\n",
    "    uhi_df['Longitude'].max(),\n",
    "    uhi_df['Latitude'].max()\n",
    "]\n",
    "print(f\"Loaded bounds from {absolute_uhi_csv.name}: {bounds}\")\n",
    "\n",
    "# --- Central Config Dictionary (Updated for ConvLSTM + HighRes Elev) --- #\n",
    "config = {\n",
    "    # Paths & Info\n",
    "    \"model_type\": \"BranchedUHIModel_HiResElev\",\n",
    "    \"project_root\": project_root_str,\n",
    "    \"city_name\": city_name,\n",
    "    \"wandb_project_name\": wandb_project_name,\n",
    "    \"wander_run_name_prefix\": wander_run_name_prefix,\n",
    "    # Data Loading\n",
    "    \"resolution_m\": resolution_m,\n",
    "    \"weather_seq_length\": weather_seq_length,\n",
    "    \"uhi_csv\": str(relative_uhi_csv),\n",
    "    \"bronx_weather_csv\": str(absolute_bronx_weather_csv),\n",
    "    \"manhattan_weather_csv\": str(absolute_manhattan_weather_csv),\n",
    "    \"bounds\": bounds,\n",
    "    \"feature_flags\": feature_flags, # Pass updated flags\n",
    "    \"sentinel_bands_to_load\": sentinel_bands_to_load,\n",
    "    \"dem_path_high_res\": str(absolute_dem_path_high_res) if absolute_dem_path_high_res else None,\n",
    "    \"dsm_path_high_res\": str(absolute_dsm_path_high_res) if absolute_dsm_path_high_res else None,\n",
    "    \"high_res_nodata\": high_res_nodata, # Pass high-res nodata\n",
    "    \"dem_path_low_res\": str(absolute_dem_path_low_res) if absolute_dem_path_low_res else None,\n",
    "    \"dsm_path_low_res\": str(absolute_dsm_path_low_res) if absolute_dsm_path_low_res else None,\n",
    "    \"low_res_elevation_nodata\": low_res_elevation_nodata,\n",
    "    \"cloudless_mosaic_path\": str(absolute_cloudless_mosaic_path) if absolute_cloudless_mosaic_path else None,\n",
    "    \"single_lst_median_path\": str(absolute_single_lst_median_path) if absolute_single_lst_median_path else None,\n",
    "    # Model Config\n",
    "    \"weather_input_channels\": weather_input_channels,\n",
    "    \"convlstm_hidden_dims\": convlstm_hidden_dims,\n",
    "    \"convlstm_kernel_sizes\": convlstm_kernel_sizes,\n",
    "    \"convlstm_num_layers\": convlstm_num_layers,\n",
    "    \"proj_static_ch\": proj_static_ch,\n",
    "    \"proj_temporal_ch\": proj_temporal_ch,\n",
    "    \"unet_base_channels\": unet_base_channels,\n",
    "    \"unet_depth\": unet_depth,\n",
    "    # Clay specific\n",
    "    \"clay_model_size\": clay_model_size,\n",
    "    \"clay_bands\": clay_bands,\n",
    "    \"clay_platform\": clay_platform,\n",
    "    \"clay_gsd\": clay_gsd,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"clay_checkpoint_path\": str(absolute_clay_checkpoint_path) if absolute_clay_checkpoint_path else None,\n",
    "    \"clay_metadata_path\": str(absolute_clay_metadata_path) if absolute_clay_metadata_path else None,\n",
    "    # High-Res Elevation Branch specific (for MODEL init)\n",
    "    \"include_dem_branch\": include_dem_branch,\n",
    "    \"include_dsm_branch\": include_dsm_branch,\n",
    "    \"elevation_branch_start_channels\": elevation_branch_start_channels,\n",
    "    \"elevation_branch_out_channels\": elevation_branch_out_channels,\n",
    "    \"elevation_branch_downsample_layers\": elevation_branch_downsample_layers,\n",
    "    \"elevation_branch_kernel_size\": elevation_branch_kernel_size,\n",
    "    # Training Hyperparameters\n",
    "    \"n_train_batches\": n_train_batches,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_type\": loss_type,\n",
    "    \"patience\": patience,\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "print(\"\\nBranched Model (HiRes Elev) Configuration dictionary created:\")\n",
    "# Use a default function to handle non-serializable types like Path\n",
    "print(json.dumps(config, indent=2, default=lambda x: str(x) if isinstance(x, (Path, torch.device)) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212078d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 07:45:40,103 - INFO - Loading cloudless mosaic from /home/jupyter/MLC-Project/data/NYC/sat_files/sentinel_NYC_20210601_to_20210901_cloudless_mosaic.npy\n",
      "2025-05-05 07:45:40,189 - INFO - Resizing full mosaic from (1119, 1278) to (1118, 969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BranchedCityDataSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 07:45:40,504 - WARNING - No non-Clay static features were loaded or enabled.\n",
      "2025-05-05 07:45:40,517 - INFO - Loaded Bronx weather data: 169 records\n",
      "2025-05-05 07:45:40,517 - INFO - Loaded Manhattan weather data: 169 records\n",
      "2025-05-05 07:45:40,534 - INFO - Computed grid cell center coordinates.\n",
      "Precomputing UHI grids: 100%|██████████| 59/59 [00:00<00:00, 367.09it/s]\n",
      "2025-05-05 07:45:40,706 - INFO - Precomputing weather grids for all unique timestamps...\n",
      "Precomputing weather grids: 100%|██████████| 59/59 [00:06<00:00,  9.11it/s]\n",
      "2025-05-05 07:45:47,183 - INFO - Finished precomputing weather grids.\n",
      "2025-05-05 07:45:47,183 - INFO - Dataset initialized for NYC with 59 unique timestamps.\n",
      "2025-05-05 07:45:47,184 - INFO - Target grid size (H, W): (1118, 969), CRS: EPSG:4326\n",
      "2025-05-05 07:45:47,184 - INFO - Weather sequence length T = 60\n",
      "2025-05-05 07:45:47,185 - INFO - Enabled features (flags): {\"use_dsm\": true, \"use_dem\": true, \"use_clay\": true, \"use_sentinel_composite\": false, \"use_lst\": false, \"use_ndvi\": false, \"use_ndbi\": false, \"use_ndwi\": false}\n",
      "2025-05-05 07:45:47,186 - INFO - Total NON-CLAY static feature channels: 0\n",
      "2025-05-05 07:45:47,186 - INFO -  High-res DEM loaded: False (shape: N/A)\n",
      "2025-05-05 07:45:47,187 - INFO -  High-res DSM loaded: False (shape: N/A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dataset split: 36 training, 23 validation samples.\n",
      "Calculating UHI statistics from training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb32abaed6f4382982ee0fe166910c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating stats:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Data Loading and Preprocessing (Branched Model + HighRes Elev)\n",
    "\n",
    "# --- Import utils ---\n",
    "from src.train.train_utils import (\n",
    "    split_data,\n",
    "    calculate_uhi_stats,\n",
    "    create_dataloaders\n",
    ")\n",
    "# -------------------\n",
    "\n",
    "print(\"Initializing BranchedCityDataSet...\")\n",
    "try:\n",
    "    dataset = BranchedCityDataSet(\n",
    "        bounds=config[\"bounds\"],\n",
    "        resolution_m=config[\"resolution_m\"],\n",
    "        uhi_csv=absolute_uhi_csv, # Use absolute path resolved earlier\n",
    "        bronx_weather_csv=absolute_bronx_weather_csv,\n",
    "        manhattan_weather_csv=absolute_manhattan_weather_csv,\n",
    "        data_dir=project_root_str,\n",
    "        city_name=config[\"city_name\"],\n",
    "        feature_flags=config[\"feature_flags\"],\n",
    "        sentinel_bands_to_load=config[\"sentinel_bands_to_load\"],\n",
    "        dem_path_high_res=config[\"dem_path_high_res\"],\n",
    "        dsm_path_high_res=config[\"dsm_path_high_res\"],\n",
    "        high_res_nodata=config[\"high_res_nodata\"],\n",
    "        dem_path_low_res=config[\"dem_path_low_res\"],\n",
    "        dsm_path_low_res=config[\"dsm_path_low_res\"],\n",
    "        low_res_elevation_nodata=config[\"low_res_elevation_nodata\"],\n",
    "        cloudless_mosaic_path=config[\"cloudless_mosaic_path\"],\n",
    "        single_lst_median_path=config[\"single_lst_median_path\"],\n",
    "        weather_seq_length=config[\"weather_seq_length\"]\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset initialization failed: {e}\")\n",
    "    print(\"Ensure required data files (DEM, DSM, weather, UHI, potentially mosaic/LST) exist.\")\n",
    "    print(\"Run `notebooks/download_data.ipynb` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during dataset initialization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Train/Val Split --- #\n",
    "train_ds, val_ds = split_data(dataset, val_percent=0.40, seed=42)\n",
    "\n",
    "# --- Calculate UHI Mean and Std from Training Data ONLY --- #\n",
    "uhi_mean, uhi_std = calculate_uhi_stats(train_ds)\n",
    "config['uhi_mean'] = uhi_mean\n",
    "config['uhi_std'] = uhi_std\n",
    "\n",
    "# --- Create DataLoaders --- #\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    n_train_batches=config['n_train_batches'],\n",
    "    num_workers=config['num_workers'],\n",
    "    device=device # Pass device from config cell\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model Initialization (BranchedUHIModel with ConvLSTM + HighRes Elev)\n",
    "\n",
    "print(\"Initializing BranchedUHIModel with HighRes Elevation Branches...\")\n",
    "\n",
    "# --- Determine Static Input Channels for Model (NON-ELEVATION) --- #\n",
    "# This depends on which *non-Clay*, *non-Elevation* static features are enabled in the dataloader\n",
    "static_channels_model = 0\n",
    "# Check flags used by dataloader to add features to 'static_features' output\n",
    "if config['feature_flags']['use_sentinel_composite']: static_channels_model += len(config['sentinel_bands_to_load'])\n",
    "if config['feature_flags']['use_lst']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndvi']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndbi']: static_channels_model += 1\n",
    "if config['feature_flags']['use_ndwi']: static_channels_model += 1\n",
    "print(f\"Calculated non-Clay, non-Elevation static input channels for model: {static_channels_model}\")\n",
    "\n",
    "# --- Instantiate the BranchedUHIModel with ConvLSTM & HighRes Elev --- #\n",
    "model = BranchedUHIModel(\n",
    "    # Non-default args first\n",
    "    weather_input_channels=config['weather_input_channels'],\n",
    "    convlstm_hidden_dims=config['convlstm_hidden_dims'],\n",
    "    convlstm_kernel_sizes=config['convlstm_kernel_sizes'],\n",
    "    convlstm_num_layers=config['convlstm_num_layers'],\n",
    "    static_channels=static_channels_model, # Pass calculated NON-ELEVATION static channels\n",
    "    unet_base_channels=config['unet_base_channels'],\n",
    "    unet_depth=config['unet_depth'],\n",
    "    # Default args next\n",
    "    include_clay_features=config['feature_flags']['use_clay'],\n",
    "    clay_checkpoint_path=str(absolute_clay_checkpoint_path) if config['feature_flags']['use_clay'] else None,\n",
    "    clay_metadata_path=str(absolute_clay_metadata_path) if config['feature_flags']['use_clay'] else None,\n",
    "    freeze_clay_backbone=config['freeze_backbone'] if config['feature_flags']['use_clay'] else False,\n",
    "    clay_embed_dim=1024, # Assuming ViT-Large from Clay\n",
    "    proj_static_ch=config['proj_static_ch'],\n",
    "    proj_temporal_ch=config['proj_temporal_ch'],\n",
    "    # --- NEW: Pass High-Res Elevation Args --- #\n",
    "    include_dem_branch=config['include_dem_branch'],\n",
    "    include_dsm_branch=config['include_dsm_branch'],\n",
    "    elevation_branch_start_channels=config['elevation_branch_start_channels'],\n",
    "    elevation_branch_out_channels=config['elevation_branch_out_channels'],\n",
    "    elevation_branch_downsample_layers=config['elevation_branch_downsample_layers'],\n",
    "    elevation_branch_kernel_size=config['elevation_branch_kernel_size'],\n",
    "    # ----------------------------------------- #\n",
    "    # Clay kwargs (match constructor)\n",
    "    model_size=config['clay_model_size'] if config['feature_flags']['use_clay'] else None,\n",
    "    bands=config['clay_bands'] if config['feature_flags']['use_clay'] else None,\n",
    "    platform=config['clay_platform'] if config['feature_flags']['use_clay'] else None,\n",
    "    gsd=config['clay_gsd'] if config['feature_flags']['use_clay'] else None\n",
    ").to(device)\n",
    "\n",
    "print(\"BranchedUHIModel (HighRes Elev) initialized.\")\n",
    "# Optional: Print model summary (might need adjustment for new inputs)\n",
    "# try:\n",
    "#     from torchinfo import summary\n",
    "#     H, W = (dataset.sat_H, dataset.sat_W) if 'dataset' in locals() else (224, 224)\n",
    "#     T = config['weather_seq_length']\n",
    "#     B = 2\n",
    "#     dummy_weather = torch.randn(B, T, config['weather_input_channels'], H, W)\n",
    "#     dummy_static = torch.randn(B, static_channels_model, H, W) if static_channels_model > 0 else None\n",
    "#     dummy_clay_mosaic = torch.randn(B, len(config['clay_bands']), H, W) if config['feature_flags']['use_clay'] else None\n",
    "#     dummy_norm_time = torch.randn(B, 4) if config['feature_flags']['use_clay'] else None\n",
    "#     dummy_norm_latlon = torch.randn(B, 4) if config['feature_flags']['use_clay'] else None\n",
    "#     # Add dummy high-res inputs (need to guess shape or get from loaded data)\n",
    "#     dummy_high_res_h, dummy_high_res_w = (H * 10, W * 10) # Example guess for 1m vs 10m\n",
    "#     dummy_dem = torch.randn(B, 1, dummy_high_res_h, dummy_high_res_w) if config['include_dem_branch'] else None\n",
    "#     dummy_dsm = torch.randn(B, 1, dummy_high_res_h, dummy_high_res_w) if config['include_dsm_branch'] else None\n",
    "#     # Need to match forward signature order\n",
    "#     summary(model, input_data=[dummy_weather, dummy_static, dummy_clay_mosaic, dummy_norm_time, dummy_norm_latlon, dummy_dem, dummy_dsm], device=str(device))\n",
    "# except ImportError:\n",
    "#     print(\"Install torchinfo (`pip install torchinfo`) for model summary.\")\n",
    "# except Exception as e:\n",
    "#      print(f\"Could not print model summary: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e34b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper Functions (Train/Validate Epochs for ConvLSTM Model + HighRes Elev)\n",
    "\n",
    "# --- REMOVED save_checkpoint definition (now in train_utils) ---\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device, uhi_mean, uhi_std, config):\n",
    "    \"\"\"Trains the BranchedUHIModel (ConvLSTM + HighRes Elev) for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "\n",
    "    # Get feature flags for convenience\n",
    "    feature_flags = config.get('feature_flags', {}) # Flags used by dataloader\n",
    "    include_clay_flag = feature_flags.get('use_clay', False)\n",
    "    include_dem_branch_flag = config.get('include_dem_branch', False) # Flags used by model\n",
    "    include_dsm_branch_flag = config.get('include_dsm_branch', False) # Flags used by model\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # --- Unpack batch (BranchedCityDataSet + HighRes Elev) ---\n",
    "            weather_seq = batch.get('weather_seq').to(device)\n",
    "            target_unnorm = batch.get('target').to(device)\n",
    "            mask = batch.get('mask').to(device)\n",
    "\n",
    "            static_features = batch.get('static_features')\n",
    "            if static_features is not None: static_features = static_features.to(device)\n",
    "\n",
    "            cloudless_mosaic = batch.get('cloudless_mosaic')\n",
    "            if include_clay_flag and cloudless_mosaic is not None: cloudless_mosaic = cloudless_mosaic.to(device)\n",
    "            else: cloudless_mosaic = None\n",
    "\n",
    "            norm_latlon = batch.get('norm_latlon')\n",
    "            if include_clay_flag and norm_latlon is not None: norm_latlon = norm_latlon.to(device)\n",
    "            else: norm_latlon = None\n",
    "\n",
    "            norm_timestamp = batch.get('norm_timestamp')\n",
    "            if include_clay_flag and norm_timestamp is not None: norm_timestamp = norm_timestamp.to(device)\n",
    "            else: norm_timestamp = None\n",
    "\n",
    "            high_res_dem = batch.get('high_res_dem')\n",
    "            if include_dem_branch_flag and high_res_dem is not None: high_res_dem = high_res_dem.to(device)\n",
    "            else: high_res_dem = None\n",
    "\n",
    "            high_res_dsm = batch.get('high_res_dsm')\n",
    "            if include_dsm_branch_flag and high_res_dsm is not None: high_res_dsm = high_res_dsm.to(device)\n",
    "            else: high_res_dsm = None\n",
    "\n",
    "            # Get target H, W for the forward pass (low-res grid)\n",
    "            target_h, target_w = target_unnorm.shape[2], target_unnorm.shape[3]\n",
    "            target_h_w_tuple = (target_h, target_w)\n",
    "\n",
    "            # --- Forward Pass --- #\n",
    "            prediction_norm = model(\n",
    "                weather_seq=weather_seq,\n",
    "                static_features=static_features,\n",
    "                cloudless_mosaic=cloudless_mosaic,\n",
    "                norm_latlon=norm_latlon,\n",
    "                norm_timestamp=norm_timestamp,\n",
    "                high_res_dem=high_res_dem,\n",
    "                high_res_dsm=high_res_dsm,\n",
    "                target_h_w=target_h_w_tuple\n",
    "            )\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            prediction_norm_final = prediction_norm.squeeze(1)\n",
    "            target_norm = (target_unnorm.squeeze(1) - uhi_mean) / uhi_std\n",
    "            loss = loss_fn(prediction_norm_final, target_norm, mask.squeeze(1))\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(\"NaN loss detected, skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Accumulate for Metrics (using unnormalized values) ---\n",
    "            prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "            mask_bool = mask.squeeze(1).bool()\n",
    "            all_preds_unnorm.append(prediction_unnorm[mask_bool].detach().cpu())\n",
    "            all_targets_unnorm.append(target_unnorm.squeeze(1)[mask_bool].detach().cpu())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            logging.error(f\"Runtime error during training: {e}\")\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                logging.error(\"CUDA out of memory. Try reducing batch size (or n_train_batches), image size, or model complexity.\")\n",
    "                # Optionally break or raise here if OOM is critical\n",
    "            # Log batch details for debugging other runtime errors\n",
    "            logging.error(f\"Batch keys: {batch.keys()}\")\n",
    "            for k, v_ in batch.items():\n",
    "                 if isinstance(v_, torch.Tensor):\n",
    "                      logging.error(f\"  {k} shape: {v_.shape}, dtype: {v_.dtype}, device: {v_.device}\")\n",
    "                 else:\n",
    "                      logging.error(f\"  {k} type: {type(v_)}\")\n",
    "            continue # Continue to next batch\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during training step: {e}\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "    # --- Epoch Metrics Calculation ---\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    rmse, r2 = float('nan'), float('nan')\n",
    "    if all_targets_unnorm:\n",
    "        try:\n",
    "            all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "            all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "            valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "            if np.sum(valid_idx) > 0:\n",
    "                rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "                r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "            else:\n",
    "                 logging.warning(\"No valid overlapping pixels found for calculating metrics in training epoch.\")\n",
    "        except Exception as metric_e:\n",
    "             logging.error(f\"Error calculating epoch metrics: {metric_e}\")\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in training epoch.\")\n",
    "    return avg_loss, rmse, r2\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device, uhi_mean, uhi_std, config):\n",
    "    \"\"\"Evaluates the BranchedUHIModel (ConvLSTM + HighRes Elev) on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_targets_unnorm = []\n",
    "    all_preds_unnorm = []\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "\n",
    "    # Get feature flags for convenience\n",
    "    feature_flags = config.get('feature_flags', {})\n",
    "    include_clay_flag = feature_flags.get('use_clay', False)\n",
    "    include_dem_branch_flag = config.get('include_dem_branch', False)\n",
    "    include_dsm_branch_flag = config.get('include_dsm_branch', False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                # --- Unpack batch (same as train_epoch) ---\n",
    "                weather_seq = batch.get('weather_seq').to(device)\n",
    "                target_unnorm = batch.get('target').to(device)\n",
    "                mask = batch.get('mask').to(device)\n",
    "\n",
    "                static_features = batch.get('static_features')\n",
    "                if static_features is not None: static_features = static_features.to(device)\n",
    "\n",
    "                cloudless_mosaic = batch.get('cloudless_mosaic')\n",
    "                if include_clay_flag and cloudless_mosaic is not None: cloudless_mosaic = cloudless_mosaic.to(device)\n",
    "                else: cloudless_mosaic = None\n",
    "\n",
    "                norm_latlon = batch.get('norm_latlon')\n",
    "                if include_clay_flag and norm_latlon is not None: norm_latlon = norm_latlon.to(device)\n",
    "                else: norm_latlon = None\n",
    "\n",
    "                norm_timestamp = batch.get('norm_timestamp')\n",
    "                if include_clay_flag and norm_timestamp is not None: norm_timestamp = norm_timestamp.to(device)\n",
    "                else: norm_timestamp = None\n",
    "\n",
    "                high_res_dem = batch.get('high_res_dem')\n",
    "                if include_dem_branch_flag and high_res_dem is not None: high_res_dem = high_res_dem.to(device)\n",
    "                else: high_res_dem = None\n",
    "\n",
    "                high_res_dsm = batch.get('high_res_dsm')\n",
    "                if include_dsm_branch_flag and high_res_dsm is not None: high_res_dsm = high_res_dsm.to(device)\n",
    "                else: high_res_dsm = None\n",
    "\n",
    "                target_h, target_w = target_unnorm.shape[2], target_unnorm.shape[3]\n",
    "                target_h_w_tuple = (target_h, target_w)\n",
    "\n",
    "                # --- Forward Pass --- #\n",
    "                prediction_norm = model(\n",
    "                    weather_seq=weather_seq,\n",
    "                    static_features=static_features,\n",
    "                    cloudless_mosaic=cloudless_mosaic,\n",
    "                    norm_latlon=norm_latlon,\n",
    "                    norm_timestamp=norm_timestamp,\n",
    "                    high_res_dem=high_res_dem,\n",
    "                    high_res_dsm=high_res_dsm,\n",
    "                    target_h_w=target_h_w_tuple\n",
    "                )\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                prediction_norm_final = prediction_norm.squeeze(1)\n",
    "                target_norm = (target_unnorm.squeeze(1) - uhi_mean) / uhi_std\n",
    "                loss = loss_fn(prediction_norm_final, target_norm, mask.squeeze(1))\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    logging.warning(\"NaN validation loss detected, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Accumulate for Metrics ---\n",
    "                prediction_unnorm = prediction_norm_final * uhi_std + uhi_mean\n",
    "                mask_bool = mask.squeeze(1).bool()\n",
    "                all_preds_unnorm.append(prediction_unnorm[mask_bool].detach().cpu())\n",
    "                all_targets_unnorm.append(target_unnorm.squeeze(1)[mask_bool].detach().cpu())\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during validation step: {e}\", exc_info=True)\n",
    "                 # Log batch details for debugging\n",
    "                 logging.error(f\"Batch keys: {batch.keys()}\")\n",
    "                 for k, v_ in batch.items():\n",
    "                     if isinstance(v_, torch.Tensor):\n",
    "                          logging.error(f\"  {k} shape: {v_.shape}, dtype: {v_.dtype}, device: {v_.device}\")\n",
    "                     else:\n",
    "                          logging.error(f\"  {k} type: {type(v_)}\")\n",
    "                 continue # Continue to next batch\n",
    "\n",
    "    # --- Epoch Metrics Calculation ---\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    rmse, r2 = float('nan'), float('nan')\n",
    "    if all_targets_unnorm:\n",
    "        try:\n",
    "            all_targets_unnorm_flat = torch.cat(all_targets_unnorm).numpy()\n",
    "            all_preds_unnorm_flat = torch.cat(all_preds_unnorm).numpy()\n",
    "            valid_idx = ~np.isnan(all_targets_unnorm_flat) & ~np.isnan(all_preds_unnorm_flat)\n",
    "            if np.sum(valid_idx) > 0:\n",
    "                rmse = math.sqrt(mean_squared_error(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx]))\n",
    "                r2 = r2_score(all_targets_unnorm_flat[valid_idx], all_preds_unnorm_flat[valid_idx])\n",
    "            else:\n",
    "                logging.warning(\"No valid overlapping pixels found for calculating metrics in validation epoch.\")\n",
    "        except Exception as metric_e:\n",
    "            logging.error(f\"Error calculating epoch metrics: {metric_e}\")\n",
    "    else:\n",
    "        logging.warning(\"No targets accumulated for calculating metrics in validation epoch.\")\n",
    "    return avg_loss, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6584bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Training Loop Execution\n",
    "\n",
    "# Ensure model and dataloaders are initialized from previous cells\n",
    "if 'model' not in locals() or model is None:\n",
    "    raise NameError(\"Model is not initialized. Run the model initialization cell first.\")\n",
    "if 'train_loader' not in locals():\n",
    "    raise NameError(\"Train loader is not initialized. Run the DataLoader setup cell first.\")\n",
    "# val_loader can be None if val_percent was 0 or dataset too small\n",
    "if 'val_loader' not in locals():\n",
    "    val_loader = None \n",
    "    print(\"Validation loader not found, proceeding without validation.\")\n",
    "\n",
    "print(f\"Model {config['model_type']} initialized on {device}\")\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "best_val_r2 = -float('inf') # Initialize best R^2\n",
    "epochs_no_improve = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "loss_fn = masked_mae_loss if config[\"loss_type\"] == \"mae\" else masked_mse_loss\n",
    "\n",
    "# --- Output Directory & Run Name ---\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{config['wander_run_name_prefix']}_{run_timestamp}\"\n",
    "output_dir = Path(output_dir_base) / run_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "config[\"output_dir\"] = str(output_dir) # Update config with actual output dir\n",
    "print(f\"Checkpoints and logs will be saved to: {output_dir}\")\n",
    "\n",
    "# --- Retrieve UHI Stats ---\n",
    "uhi_mean = config.get('uhi_mean')\n",
    "uhi_std = config.get('uhi_std')\n",
    "if uhi_mean is None or uhi_std is None:\n",
    "    raise ValueError(\"uhi_mean and uhi_std not found in config. Ensure they were calculated.\")\n",
    "print(f\"Using Training UHI Mean: {uhi_mean:.4f}, Std Dev: {uhi_std:.4f} for normalization.\")\n",
    "\n",
    "# --- Feature Flags for Training/Validation functions ---\n",
    "feature_flags_from_config = config['feature_flags']\n",
    "\n",
    "# --- Initialize WANDB ---\n",
    "if 'wandb' in sys.modules:\n",
    "    try:\n",
    "        if wandb.run is not None:\n",
    "            print(\"Finishing previous W&B run...\")\n",
    "            wandb.finish()\n",
    "        wandb.init(\n",
    "            project=config[\"wandb_project_name\"],\n",
    "            name=run_name,\n",
    "            config=config # Log the entire config dictionary\n",
    "        )\n",
    "        print(f\"Wandb initialized for run: {run_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization failed: {e}\")\n",
    "        wandb = None\n",
    "else:\n",
    "    print(\"Wandb not imported, skipping W&B logging.\")\n",
    "    wandb = None\n",
    "\n",
    "# Save configuration used for this run locally\n",
    "try:\n",
    "    # Use the same serialization helper as before\n",
    "    config_serializable = {k: str(v) if isinstance(v, Path) else v for k, v in config.items()}\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_serializable, f, indent=2, default=lambda x: str(x) if isinstance(x, Path) else x)\n",
    "    print(\"Saved local configuration to config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local configuration: {e}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"Starting {config['model_type']} training...\")\n",
    "training_log = []\n",
    "last_saved_epoch = -1\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"--- Epoch {epoch+1}/{config['epochs']} ---\")\n",
    "\n",
    "    if train_loader:\n",
    "        train_loss, train_rmse, train_r2 = train_epoch(model, train_loader, optimizer, loss_fn, device,\n",
    "                                                       uhi_mean, uhi_std, feature_flags_from_config)\n",
    "    else:\n",
    "        print(\"Skipping training epoch as train_loader is not available.\")\n",
    "        train_loss, train_rmse, train_r2 = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    log_metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_r2\": train_r2\n",
    "    }\n",
    "\n",
    "    is_best = False\n",
    "    if val_loader:\n",
    "        val_loss, val_rmse, val_r2 = validate_epoch(model, val_loader, loss_fn, device,\n",
    "                                                    uhi_mean, uhi_std, feature_flags_from_config)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} | Val Loss={val_loss:.4f} RMSE={val_rmse:.4f} R2={val_r2:.4f}\")\n",
    "        log_metrics.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"val_r2\": val_r2\n",
    "        })\n",
    "\n",
    "        if np.isnan(val_r2):\n",
    "             print(\"Warning: Validation R^2 is NaN. Cannot determine improvement. Stopping training.\")\n",
    "             break\n",
    "\n",
    "        is_best = val_r2 > best_val_r2\n",
    "        if is_best:\n",
    "            best_val_r2 = val_r2\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"New best validation R^2: {best_val_r2:.4f}\")\n",
    "            if wandb and wandb.run:\n",
    "                wandb.run.summary[\"best_val_r2\"] = best_val_r2\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in validation R^2 for {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= config[\"patience\"]:\n",
    "            print(f\"Early stopping triggered after {config['patience']} epochs with no improvement.\")\n",
    "            break\n",
    "    else: # No validation\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} RMSE={train_rmse:.4f} R2={train_r2:.4f} (No validation)\")\n",
    "        if np.isnan(train_loss):\n",
    "             print(\"Warning: Training loss is NaN. Stopping training.\")\n",
    "             break\n",
    "\n",
    "    if wandb:\n",
    "        wandb.log(log_metrics)\n",
    "    training_log.append(log_metrics)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {'epoch': epoch + 1,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'best_val_r2': best_val_r2,\n",
    "         'optimizer' : optimizer.state_dict(),\n",
    "         'config': config_serializable\n",
    "         },\n",
    "        is_best,\n",
    "        filename=output_dir / 'checkpoint_last.pth.tar',\n",
    "        best_filename=output_dir / 'model_best.pth.tar'\n",
    "    )\n",
    "    if is_best:\n",
    "        last_saved_epoch = epoch + 1\n",
    "\n",
    "# --- Final Steps ---\n",
    "try:\n",
    "    log_df = pd.DataFrame(training_log)\n",
    "    log_df.to_csv(output_dir / 'training_log.csv', index=False)\n",
    "    print(f\"Saved local training log to {output_dir / 'training_log.csv'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Failed to save local training log: {e}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "if val_loader:\n",
    "    print(f\"Best validation R^2 recorded: {best_val_r2:.4f} (achieved at epoch {last_saved_epoch if last_saved_epoch > 0 else 'N/A'})\" if not np.isinf(best_val_r2) else \"Best validation R^2: N/A\")\n",
    "print(f\"Final checkpoint saved in: {output_dir / 'checkpoint_last.pth.tar'}\")\n",
    "if last_saved_epoch > 0:\n",
    "     print(f\"Best model saved in: {output_dir / 'model_best.pth.tar'}\")\n",
    "\n",
    "if wandb and wandb.run:\n",
    "    wandb.finish()\n",
    "    print(\"Wandb run finished.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
